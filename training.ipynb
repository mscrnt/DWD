{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kenneth/miniconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow as tf\n",
    "tf.disable_v2_behavior()\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  dresden_files/001.txt\n",
      "Downloading file:  dresden_files/002.txt\n",
      "Downloading file:  dresden_files/003.txt\n",
      "Downloading file:  dresden_files/004.txt\n",
      "Downloading file:  dresden_files/005.txt\n",
      "Downloading file:  dresden_files/006.txt\n",
      "Downloading file:  dresden_files/007.txt\n",
      "Downloading file:  dresden_files/008.txt\n",
      "Downloading file:  dresden_files/009.txt\n",
      "Downloading file:  dresden_files/010.txt\n",
      "Downloading file:  dresden_files/011.txt\n",
      "Downloading file:  dresden_files/012.txt\n",
      "Downloading file:  dresden_files/013.txt\n",
      "Downloading file:  dresden_files/014.txt\n",
      "Downloading file:  dresden_files/015.txt\n",
      "Downloading file:  dresden_files/016.txt\n",
      "Downloading file:  dresden_files/017.txt\n",
      "Downloading file:  dresden_files/018.txt\n",
      "Downloading file:  dresden_files/019.txt\n",
      "Downloading file:  dresden_files/020.txt\n",
      "Downloading file:  dresden_files/021.txt\n",
      "Downloading file:  dresden_files/022.txt\n",
      "Downloading file:  dresden_files/023.txt\n",
      "Downloading file:  dresden_files/024.txt\n",
      "Downloading file:  dresden_files/025.txt\n",
      "Downloading file:  dresden_files/026.txt\n",
      "Downloading file:  dresden_files/027.txt\n",
      "Downloading file:  dresden_files/028.txt\n",
      "Downloading file:  dresden_files/029.txt\n",
      "Downloading file:  dresden_files/030.txt\n",
      "Downloading file:  dresden_files/031.txt\n",
      "Downloading file:  dresden_files/032.txt\n",
      "Downloading file:  dresden_files/033.txt\n",
      "Downloading file:  dresden_files/034.txt\n",
      "Downloading file:  dresden_files/035.txt\n",
      "Downloading file:  dresden_files/036.txt\n",
      "Downloading file:  dresden_files/037.txt\n",
      "Downloading file:  dresden_files/038.txt\n",
      "Downloading file:  dresden_files/039.txt\n",
      "Downloading file:  dresden_files/040.txt\n",
      "Downloading file:  dresden_files/041.txt\n",
      "Downloading file:  dresden_files/042.txt\n",
      "Downloading file:  dresden_files/043.txt\n",
      "Downloading file:  dresden_files/044.txt\n",
      "Downloading file:  dresden_files/045.txt\n",
      "Downloading file:  dresden_files/046.txt\n",
      "Downloading file:  dresden_files/047.txt\n",
      "Downloading file:  dresden_files/048.txt\n",
      "Downloading file:  dresden_files/049.txt\n",
      "Downloading file:  dresden_files/050.txt\n",
      "Downloading file:  dresden_files/051.txt\n",
      "Downloading file:  dresden_files/052.txt\n",
      "Downloading file:  dresden_files/053.txt\n",
      "Downloading file:  dresden_files/054.txt\n",
      "Downloading file:  dresden_files/055.txt\n",
      "Downloading file:  dresden_files/056.txt\n",
      "Downloading file:  dresden_files/057.txt\n",
      "Downloading file:  dresden_files/058.txt\n",
      "Downloading file:  dresden_files/059.txt\n",
      "Downloading file:  dresden_files/060.txt\n",
      "Downloading file:  dresden_files/061.txt\n",
      "Downloading file:  dresden_files/062.txt\n",
      "Downloading file:  dresden_files/063.txt\n",
      "Downloading file:  dresden_files/064.txt\n",
      "Downloading file:  dresden_files/065.txt\n",
      "Downloading file:  dresden_files/066.txt\n",
      "Downloading file:  dresden_files/067.txt\n",
      "Downloading file:  dresden_files/068.txt\n",
      "Downloading file:  dresden_files/069.txt\n",
      "Downloading file:  dresden_files/070.txt\n",
      "Downloading file:  dresden_files/071.txt\n",
      "Downloading file:  dresden_files/072.txt\n",
      "Downloading file:  dresden_files/073.txt\n",
      "Downloading file:  dresden_files/074.txt\n",
      "Downloading file:  dresden_files/075.txt\n",
      "Downloading file:  dresden_files/076.txt\n",
      "Downloading file:  dresden_files/077.txt\n",
      "Downloading file:  dresden_files/078.txt\n",
      "Downloading file:  dresden_files/079.txt\n",
      "Downloading file:  dresden_files/080.txt\n",
      "Downloading file:  dresden_files/081.txt\n",
      "Downloading file:  dresden_files/082.txt\n",
      "Downloading file:  dresden_files/083.txt\n",
      "Downloading file:  dresden_files/084.txt\n",
      "Downloading file:  dresden_files/085.txt\n",
      "Downloading file:  dresden_files/086.txt\n",
      "Downloading file:  dresden_files/087.txt\n",
      "Downloading file:  dresden_files/088.txt\n",
      "Downloading file:  dresden_files/089.txt\n",
      "Downloading file:  dresden_files/090.txt\n",
      "Downloading file:  dresden_files/091.txt\n",
      "Downloading file:  dresden_files/092.txt\n",
      "Downloading file:  dresden_files/093.txt\n",
      "Downloading file:  dresden_files/094.txt\n",
      "Downloading file:  dresden_files/095.txt\n",
      "Downloading file:  dresden_files/096.txt\n",
      "Downloading file:  dresden_files/097.txt\n",
      "Downloading file:  dresden_files/098.txt\n",
      "Downloading file:  dresden_files/099.txt\n",
      "Downloading file:  dresden_files/100.txt\n",
      "Downloading file:  dresden_files/101.txt\n",
      "Downloading file:  dresden_files/102.txt\n",
      "Downloading file:  dresden_files/103.txt\n",
      "Downloading file:  dresden_files/104.txt\n",
      "Downloading file:  dresden_files/105.txt\n",
      "Downloading file:  dresden_files/106.txt\n",
      "Downloading file:  dresden_files/107.txt\n",
      "Downloading file:  dresden_files/108.txt\n",
      "Downloading file:  dresden_files/109.txt\n",
      "Downloading file:  dresden_files/110.txt\n",
      "Downloading file:  dresden_files/111.txt\n",
      "Downloading file:  dresden_files/112.txt\n",
      "Downloading file:  dresden_files/113.txt\n",
      "Downloading file:  dresden_files/114.txt\n",
      "Downloading file:  dresden_files/115.txt\n",
      "Downloading file:  dresden_files/116.txt\n",
      "Downloading file:  dresden_files/117.txt\n",
      "Downloading file:  dresden_files/118.txt\n",
      "Downloading file:  dresden_files/119.txt\n",
      "Downloading file:  dresden_files/120.txt\n",
      "Downloading file:  dresden_files/121.txt\n",
      "Downloading file:  dresden_files/122.txt\n",
      "Downloading file:  dresden_files/123.txt\n",
      "Downloading file:  dresden_files/124.txt\n",
      "Downloading file:  dresden_files/125.txt\n",
      "Downloading file:  dresden_files/126.txt\n",
      "Downloading file:  dresden_files/127.txt\n",
      "Downloading file:  dresden_files/128.txt\n",
      "Downloading file:  dresden_files/129.txt\n",
      "Downloading file:  dresden_files/130.txt\n",
      "Downloading file:  dresden_files/131.txt\n",
      "Downloading file:  dresden_files/132.txt\n",
      "Downloading file:  dresden_files/133.txt\n",
      "Downloading file:  dresden_files/134.txt\n",
      "Downloading file:  dresden_files/135.txt\n",
      "Downloading file:  dresden_files/136.txt\n",
      "Downloading file:  dresden_files/137.txt\n",
      "Downloading file:  dresden_files/138.txt\n",
      "Downloading file:  dresden_files/139.txt\n",
      "Downloading file:  dresden_files/140.txt\n",
      "Downloading file:  dresden_files/141.txt\n",
      "Downloading file:  dresden_files/142.txt\n",
      "Downloading file:  dresden_files/143.txt\n",
      "Downloading file:  dresden_files/144.txt\n",
      "Downloading file:  dresden_files/145.txt\n",
      "Downloading file:  dresden_files/146.txt\n",
      "Downloading file:  dresden_files/147.txt\n",
      "Downloading file:  dresden_files/148.txt\n",
      "Downloading file:  dresden_files/149.txt\n",
      "Downloading file:  dresden_files/150.txt\n",
      "Downloading file:  dresden_files/151.txt\n",
      "Downloading file:  dresden_files/152.txt\n",
      "Downloading file:  dresden_files/153.txt\n",
      "Downloading file:  dresden_files/154.txt\n",
      "Downloading file:  dresden_files/155.txt\n",
      "Downloading file:  dresden_files/156.txt\n",
      "Downloading file:  dresden_files/157.txt\n",
      "Downloading file:  dresden_files/158.txt\n",
      "Downloading file:  dresden_files/159.txt\n",
      "Downloading file:  dresden_files/160.txt\n",
      "Downloading file:  dresden_files/161.txt\n",
      "Downloading file:  dresden_files/162.txt\n",
      "Downloading file:  dresden_files/163.txt\n",
      "Downloading file:  dresden_files/164.txt\n",
      "Downloading file:  dresden_files/165.txt\n",
      "Downloading file:  dresden_files/166.txt\n",
      "Downloading file:  dresden_files/167.txt\n",
      "Downloading file:  dresden_files/168.txt\n",
      "Downloading file:  dresden_files/169.txt\n",
      "Downloading file:  dresden_files/170.txt\n",
      "Downloading file:  dresden_files/171.txt\n",
      "Downloading file:  dresden_files/172.txt\n",
      "Downloading file:  dresden_files/173.txt\n",
      "Downloading file:  dresden_files/174.txt\n",
      "Downloading file:  dresden_files/175.txt\n",
      "Downloading file:  dresden_files/176.txt\n",
      "Downloading file:  dresden_files/177.txt\n",
      "Downloading file:  dresden_files/178.txt\n",
      "Downloading file:  dresden_files/179.txt\n",
      "Downloading file:  dresden_files/180.txt\n",
      "Downloading file:  dresden_files/181.txt\n",
      "Downloading file:  dresden_files/182.txt\n",
      "Downloading file:  dresden_files/183.txt\n",
      "Downloading file:  dresden_files/184.txt\n",
      "Downloading file:  dresden_files/185.txt\n",
      "Downloading file:  dresden_files/186.txt\n",
      "Downloading file:  dresden_files/187.txt\n",
      "Downloading file:  dresden_files/188.txt\n",
      "Downloading file:  dresden_files/189.txt\n",
      "Downloading file:  dresden_files/190.txt\n",
      "Downloading file:  dresden_files/191.txt\n",
      "Downloading file:  dresden_files/192.txt\n",
      "Downloading file:  dresden_files/193.txt\n",
      "Downloading file:  dresden_files/194.txt\n",
      "Downloading file:  dresden_files/195.txt\n",
      "Downloading file:  dresden_files/196.txt\n",
      "Downloading file:  dresden_files/197.txt\n",
      "Downloading file:  dresden_files/198.txt\n",
      "Downloading file:  dresden_files/199.txt\n",
      "Downloading file:  dresden_files/200.txt\n",
      "Downloading file:  dresden_files/201.txt\n",
      "Downloading file:  dresden_files/202.txt\n",
      "Downloading file:  dresden_files/203.txt\n",
      "Downloading file:  dresden_files/204.txt\n",
      "Downloading file:  dresden_files/205.txt\n",
      "Downloading file:  dresden_files/206.txt\n",
      "Downloading file:  dresden_files/207.txt\n",
      "Downloading file:  dresden_files/208.txt\n",
      "Downloading file:  dresden_files/209.txt\n",
      "Downloading file:  dresden_files/210.txt\n",
      "Downloading file:  dresden_files/211.txt\n",
      "Downloading file:  dresden_files/212.txt\n",
      "Downloading file:  dresden_files/213.txt\n",
      "Downloading file:  dresden_files/214.txt\n",
      "Downloading file:  dresden_files/215.txt\n",
      "Downloading file:  dresden_files/216.txt\n",
      "Downloading file:  dresden_files/217.txt\n",
      "Downloading file:  dresden_files/218.txt\n",
      "Downloading file:  dresden_files/219.txt\n",
      "Downloading file:  dresden_files/220.txt\n",
      "Downloading file:  dresden_files/221.txt\n",
      "Downloading file:  dresden_files/222.txt\n",
      "Downloading file:  dresden_files/223.txt\n",
      "Downloading file:  dresden_files/224.txt\n",
      "Downloading file:  dresden_files/225.txt\n",
      "Downloading file:  dresden_files/226.txt\n",
      "Downloading file:  dresden_files/227.txt\n",
      "Downloading file:  dresden_files/228.txt\n",
      "Downloading file:  dresden_files/229.txt\n",
      "Downloading file:  dresden_files/230.txt\n",
      "Downloading file:  dresden_files/231.txt\n",
      "Downloading file:  dresden_files/232.txt\n",
      "Downloading file:  dresden_files/233.txt\n",
      "Downloading file:  dresden_files/234.txt\n",
      "Downloading file:  dresden_files/235.txt\n",
      "Downloading file:  dresden_files/236.txt\n",
      "Downloading file:  dresden_files/237.txt\n",
      "Downloading file:  dresden_files/238.txt\n",
      "Downloading file:  dresden_files/239.txt\n",
      "Downloading file:  dresden_files/240.txt\n",
      "Downloading file:  dresden_files/241.txt\n",
      "Downloading file:  dresden_files/242.txt\n",
      "Downloading file:  dresden_files/243.txt\n",
      "Downloading file:  dresden_files/244.txt\n",
      "Downloading file:  dresden_files/245.txt\n",
      "Downloading file:  dresden_files/246.txt\n",
      "Downloading file:  dresden_files/247.txt\n",
      "Downloading file:  dresden_files/248.txt\n",
      "Downloading file:  dresden_files/249.txt\n",
      "Downloading file:  dresden_files/250.txt\n",
      "Downloading file:  dresden_files/251.txt\n",
      "Downloading file:  dresden_files/252.txt\n",
      "Downloading file:  dresden_files/253.txt\n",
      "Downloading file:  dresden_files/254.txt\n",
      "Downloading file:  dresden_files/255.txt\n",
      "Downloading file:  dresden_files/256.txt\n",
      "Downloading file:  dresden_files/257.txt\n",
      "Downloading file:  dresden_files/258.txt\n",
      "Downloading file:  dresden_files/259.txt\n",
      "Downloading file:  dresden_files/260.txt\n",
      "Downloading file:  dresden_files/261.txt\n",
      "Downloading file:  dresden_files/262.txt\n",
      "Downloading file:  dresden_files/263.txt\n",
      "Downloading file:  dresden_files/264.txt\n",
      "Downloading file:  dresden_files/265.txt\n",
      "Downloading file:  dresden_files/266.txt\n",
      "Downloading file:  dresden_files/267.txt\n",
      "Downloading file:  dresden_files/268.txt\n",
      "Downloading file:  dresden_files/269.txt\n",
      "Downloading file:  dresden_files/270.txt\n",
      "Downloading file:  dresden_files/271.txt\n",
      "Downloading file:  dresden_files/272.txt\n",
      "Downloading file:  dresden_files/273.txt\n",
      "Downloading file:  dresden_files/274.txt\n",
      "Downloading file:  dresden_files/275.txt\n",
      "Downloading file:  dresden_files/276.txt\n",
      "Downloading file:  dresden_files/277.txt\n",
      "Downloading file:  dresden_files/278.txt\n",
      "Downloading file:  dresden_files/279.txt\n",
      "Downloading file:  dresden_files/280.txt\n",
      "Downloading file:  dresden_files/281.txt\n",
      "Downloading file:  dresden_files/282.txt\n",
      "Downloading file:  dresden_files/283.txt\n",
      "Downloading file:  dresden_files/284.txt\n",
      "Downloading file:  dresden_files/285.txt\n",
      "Downloading file:  dresden_files/286.txt\n",
      "Downloading file:  dresden_files/287.txt\n",
      "Downloading file:  dresden_files/288.txt\n",
      "Downloading file:  dresden_files/289.txt\n",
      "Downloading file:  dresden_files/290.txt\n",
      "Downloading file:  dresden_files/291.txt\n",
      "Downloading file:  dresden_files/292.txt\n",
      "Downloading file:  dresden_files/293.txt\n",
      "Downloading file:  dresden_files/294.txt\n",
      "Downloading file:  dresden_files/295.txt\n",
      "Downloading file:  dresden_files/296.txt\n",
      "Downloading file:  dresden_files/297.txt\n",
      "Downloading file:  dresden_files/298.txt\n",
      "Downloading file:  dresden_files/299.txt\n",
      "Downloading file:  dresden_files/300.txt\n",
      "Downloading file:  dresden_files/301.txt\n",
      "Downloading file:  dresden_files/302.txt\n",
      "Downloading file:  dresden_files/303.txt\n",
      "Downloading file:  dresden_files/304.txt\n",
      "Downloading file:  dresden_files/305.txt\n",
      "Downloading file:  dresden_files/306.txt\n",
      "Downloading file:  dresden_files/307.txt\n",
      "Downloading file:  dresden_files/308.txt\n",
      "Downloading file:  dresden_files/309.txt\n",
      "Downloading file:  dresden_files/310.txt\n",
      "Downloading file:  dresden_files/311.txt\n",
      "Downloading file:  dresden_files/312.txt\n",
      "Downloading file:  dresden_files/313.txt\n",
      "Downloading file:  dresden_files/314.txt\n",
      "Downloading file:  dresden_files/315.txt\n",
      "Downloading file:  dresden_files/316.txt\n",
      "Downloading file:  dresden_files/317.txt\n",
      "Downloading file:  dresden_files/318.txt\n",
      "Downloading file:  dresden_files/319.txt\n",
      "Downloading file:  dresden_files/320.txt\n",
      "Downloading file:  dresden_files/321.txt\n",
      "Downloading file:  dresden_files/322.txt\n",
      "Downloading file:  dresden_files/323.txt\n",
      "Downloading file:  dresden_files/324.txt\n",
      "Downloading file:  dresden_files/325.txt\n",
      "Downloading file:  dresden_files/326.txt\n",
      "Downloading file:  dresden_files/327.txt\n",
      "Downloading file:  dresden_files/328.txt\n",
      "Downloading file:  dresden_files/329.txt\n",
      "Downloading file:  dresden_files/330.txt\n",
      "Downloading file:  dresden_files/331.txt\n",
      "Downloading file:  dresden_files/332.txt\n",
      "Downloading file:  dresden_files/333.txt\n",
      "Downloading file:  dresden_files/334.txt\n",
      "Downloading file:  dresden_files/335.txt\n",
      "Downloading file:  dresden_files/336.txt\n",
      "Downloading file:  dresden_files/337.txt\n",
      "Downloading file:  dresden_files/338.txt\n",
      "Downloading file:  dresden_files/339.txt\n",
      "Downloading file:  dresden_files/340.txt\n",
      "Downloading file:  dresden_files/341.txt\n",
      "Downloading file:  dresden_files/342.txt\n",
      "Downloading file:  dresden_files/343.txt\n",
      "Downloading file:  dresden_files/344.txt\n",
      "Downloading file:  dresden_files/345.txt\n",
      "Downloading file:  dresden_files/346.txt\n",
      "Downloading file:  dresden_files/347.txt\n",
      "Downloading file:  dresden_files/348.txt\n",
      "Downloading file:  dresden_files/349.txt\n",
      "Downloading file:  dresden_files/350.txt\n",
      "Downloading file:  dresden_files/351.txt\n",
      "Downloading file:  dresden_files/352.txt\n",
      "Downloading file:  dresden_files/353.txt\n",
      "Downloading file:  dresden_files/354.txt\n",
      "Downloading file:  dresden_files/355.txt\n",
      "Downloading file:  dresden_files/356.txt\n",
      "Downloading file:  dresden_files/357.txt\n",
      "Downloading file:  dresden_files/358.txt\n",
      "Downloading file:  dresden_files/359.txt\n",
      "Downloading file:  dresden_files/360.txt\n",
      "Downloading file:  dresden_files/361.txt\n",
      "Downloading file:  dresden_files/362.txt\n",
      "Downloading file:  dresden_files/363.txt\n",
      "Downloading file:  dresden_files/364.txt\n",
      "Downloading file:  dresden_files/365.txt\n",
      "Downloading file:  dresden_files/366.txt\n",
      "Downloading file:  dresden_files/367.txt\n",
      "Downloading file:  dresden_files/368.txt\n",
      "Downloading file:  dresden_files/369.txt\n",
      "Downloading file:  dresden_files/370.txt\n",
      "Downloading file:  dresden_files/371.txt\n",
      "Downloading file:  dresden_files/372.txt\n",
      "Downloading file:  dresden_files/373.txt\n",
      "Downloading file:  dresden_files/374.txt\n",
      "Downloading file:  dresden_files/375.txt\n",
      "Downloading file:  dresden_files/376.txt\n",
      "Downloading file:  dresden_files/377.txt\n",
      "Downloading file:  dresden_files/378.txt\n",
      "Downloading file:  dresden_files/379.txt\n",
      "Downloading file:  dresden_files/380.txt\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/mscrnt/dwd/master/dresden_files/'\n",
    "\n",
    "# Create a directory if needed\n",
    "dir_name = 'dresden_files'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download a file if not present\"\"\"\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "num_files = 380\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,381)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.'%len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file dresden_files/001.txt\n",
      "Data size (Characters) (Document 0) 21038\n",
      "Sample string (Document 0) ['\\n\\n', 'ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\ni', ' h', 'ea', 'rd', ' t', 'he', ' m', 'ai', 'lm', 'an', ' a', 'pp', 'ro', 'ac', 'h ', 'my', ' o', 'ff', 'ic', 'e ', 'do', 'or', ', ', 'ha', 'lf', ' a', 'n ', 'ho', 'ur', ' e', 'ar', 'li', 'er', ' t', 'ha', 'n ', 'us', 'ua', 'l.', ' h', 'e ', 'di', 'dn']\n",
      "\n",
      "Processing file dresden_files/002.txt\n",
      "Data size (Characters) (Document 1) 20473\n",
      "Sample string (Document 1) ['\\n\\n', '\\nc', 'ha', 'pt', 'er', ' t', 'hr', 'ee', '\\n\\n', '\\ng', 'en', 'tl', 'em', 'an', ' j', 'oh', 'nn', 'y ', 'ma', 'rc', 'on', 'e ', 'di', 'dn', \"'t\", ' l', 'oo', 'k ', 'li', 'ke', ' t', 'he', ' s', 'or', 't ', 'of', ' m', 'an', ' w', 'ho', ' w', 'ou', 'ld', ' h', 'av', 'e ', 'my', ' l', 'eg', 's ']\n",
      "\n",
      "Processing file dresden_files/003.txt\n",
      "Data size (Characters) (Document 2) 24814\n",
      "Sample string (Document 2) ['\"d', 'um', 'b,', '\" ', 'ma', 'c ', 'sa', 'id', '.\\n', '\\n\"', 'i ', 'ju', 'st', ' s', 'ai', 'd ', 'sh', 'e ', 'wa', 's ', 'sm', 'ar', 't,', ' m', 'ac', '.\"', '\\n\\n', '\\n\\n', 'ma', \"c'\", 's ', 'fa', 'ce', ' f', 'li', 'ck', 'er', 'ed', ' i', 'nt', 'o ', 'th', 'at', ' s', 'mi', 'le', ', ', 'an', 'd ', 'it']\n",
      "\n",
      "Processing file dresden_files/004.txt\n",
      "Data size (Characters) (Document 3) 18978\n",
      "Sample string (Document 3) ['\\nc', 'ou', 'ld', ' i', ' m', 'ak', 'e ', 'th', 'e ', 'po', 'ti', 'on', ' b', 'y ', 'my', 'se', 'lf', '? ', 'i ', 'pr', 'ob', 'ab', 'ly', ' c', 'ou', 'ld', '. ', 'bu', 't ', 'i ', 'ha', 'd ', 'th', 'e ', 'si', 'nk', 'in', 'g ', 'fe', 'el', 'in', 'g ', 'th', 'at', ' i', 't ', 'mi', 'gh', 't ', 'no']\n",
      "\n",
      "Processing file dresden_files/005.txt\n",
      "Data size (Characters) (Document 4) 22220\n",
      "Sample string (Document 4) ['bi', 'an', 'ca', ' k', 'ep', 't ', 'on', ' s', 'ta', 'ri', 'ng', '. ', 'he', 'r ', 'to', 'ng', 'ue', ' f', 'li', 'ck', 'er', 'ed', ' a', 'ro', 'un', 'd ', 'he', 'r ', 'mo', 'ut', 'h ', 'ag', 'ai', 'n.', ' \"', 'co', 've', 'r ', 'it', ',\"', '\\n\\n', 'sh', 'e ', 'wh', 'is', 'pe', 're', 'd.', ' a', ' s']\n",
      "\n",
      "Processing file dresden_files/006.txt\n",
      "Data size (Characters) (Document 5) 20035\n",
      "Sample string (Document 5) ['\"t', 'hi', 's ', 'do', 'es', \"n'\", 't ', 'te', 'll', ' m', 'e ', 'an', 'yt', 'hi', 'ng', ' n', 'ew', ', ', 'ha', 'rr', 'y.', '\"\\n', '\\n\"', \"i'\", 'm ', 'ge', 'tt', 'in', 'g ', 'th', 'er', 'e,', ' i', \"'m\", ' g', 'et', 'ti', 'ng', ' t', 'he', 're', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'th', 'e ', 'am']\n",
      "\n",
      "Processing file dresden_files/007.txt\n",
      "Data size (Characters) (Document 6) 20414\n",
      "Sample string (Document 6) ['\" ', 'ha', 'rr', 'y,', '\" ', 'bo', 'b ', 'pr', 'ot', 'es', 'te', 'd,', ' \"', 'th', 'at', \"'s\", ' w', 'ha', 't ', \"i'\", 'm ', 'tr', 'yi', 'ng', ' t', 'o ', 'te', 'll', ' y', 'ou', '.\"', '\\n\\n', 'su', 'sa', 'n ', 'mu', 'rm', 'ur', 'ed', ', ', 'ag', 'ai', 'ns', 't ', 'my', ' c', 'he', 'st', ', ', '\"i']\n",
      "\n",
      "Processing file dresden_files/008.txt\n",
      "Data size (Characters) (Document 7) 22393\n",
      "Sample string (Document 7) ['\"t', 'om', 'or', 'ro', 'w ', 'mo', 'rn', 'in', 'g,', ' t', 'he', 'n,', '\" ', 'sh', 'e ', 'sa', 'id', '.\\n', '\\n\"', 'we', \"'l\", 'l ', 'se', 'e,', '\" ', 'i ', 'sa', 'id', '.\\n', '\\n\\n', '\\n\"', 'if', ' y', 'ou', ' a', 're', \"n'\", 't ', 'th', 'er', 'e ', 'in', ' t', 'he', ' m', 'or', 'ni', 'ng', ',\"', ' m']\n",
      "\n",
      "Processing file dresden_files/009.txt\n",
      "Data size (Characters) (Document 8) 23126\n",
      "Sample string (Document 8) ['i ', 'gr', 'in', 'ne', 'd,', ' b', 'ut', ' h', 'e ', 'di', 'dn', \"'t\", ' s', 'ee', 'm ', 'to', ' h', 'av', 'e ', 'no', 'ti', 'ce', 'd ', 'th', 'e ', 'do', 'ub', 'le', ' e', 'nt', 'en', 'dr', 'e.', ' y', 'ou', ' j', 'us', 't ', 'do', \"n'\", 't ', 'ge', 't ', 'qu', 'al', 'it', 'y ', 'lo', 'wl', 'if']\n",
      "\n",
      "Processing file dresden_files/010.txt\n",
      "Data size (Characters) (Document 9) 30446\n",
      "Sample string (Document 9) ['\\na', 'ny', ' m', 'ag', 'ic', ' t', 'ha', 't ', 'i ', 'co', 'ul', 'd ', 'wo', 'rk', ' f', 'as', 't ', 'en', 'ou', 'gh', ' t', 'o ', 'sh', 'at', 'te', 'r ', 'th', 'e ', 'cu', 'ff', 's ', 'in', ' t', 'im', 'e ', 'wo', 'ul', 'd ', 'pr', 'ob', 'ab', 'ly', ' k', 'il', 'l ', 'me', ' w', 'it', 'h ', 'fl']\n",
      "\n",
      "Processing file dresden_files/011.txt\n",
      "Data size (Characters) (Document 10) 11062\n",
      "Sample string (Document 10) ['\\n\"', 'ka', 'ls', 'ha', 'zz', 'ak', ',\"', ' v', 'ic', 'to', 'r ', 'wh', 'is', 'pe', 're', 'd ', 'ag', 'ai', 'n,', ' l', 'ou', 'de', 'r,', ' m', 'or', 'e ', 'de', 'ma', 'nd', 'in', 'g.', ' i', ' h', 'ea', 'rd', ' s', 'om', 'et', 'hi', 'ng', ', ', 'a ', 'wa', 'rb', 'li', 'ng', ' h', 'is', 's ', 'th']\n",
      "\n",
      "Processing file dresden_files/012.txt\n",
      "Data size (Characters) (Document 11) 20408\n",
      "Sample string (Document 11) ['ch', 'ap', 'te', 'r ', '1\\n', '\\ni', ' n', 'ev', 'er', ' u', 'se', 'd ', 'to', ' k', 'ee', 'p ', 'cl', 'os', 'e ', 'tr', 'ac', 'k ', 'of', ' t', 'he', ' p', 'ha', 'se', 's ', 'of', ' t', 'he', ' m', 'oo', 'n.', ' s', 'o ', 'i ', 'di', 'dn', \"'t\", ' k', 'no', 'w ', 'th', 'at', ' i', 't ', 'wa', 's ']\n",
      "\n",
      "Processing file dresden_files/013.txt\n",
      "Data size (Characters) (Document 12) 22871\n",
      "Sample string (Document 12) ['he', 'r ', 'bl', 'ue', ' e', 'ye', 's ', 'fi', 'xe', 'd ', 'on', ' m', 'e ', 'wi', 'th', ' a', ' g', 'la', 're', ' t', 'ha', 't ', 'ma', 'de', ' m', 'e ', 'le', 'an', ' b', 'ac', 'k ', 'ag', 'ai', 'ns', 't ', 'th', 'e ', 'ca', 'r ', 'do', 'or', '.\\n', '\\n\"', 'i ', 'am', ' n', 'ot', ' y', 'ou', 'r ']\n",
      "\n",
      "Processing file dresden_files/014.txt\n",
      "Data size (Characters) (Document 13) 17235\n",
      "Sample string (Document 13) ['\"i', \"t'\", 's ', 'a ', 'jo', 'ke', ', ', 'ha', 'rr', 'y.', ' s', 'ta', 'rs', ' a', 'lm', 'ig', 'ht', 'y,', ' y', 'ou', ' n', 'ev', 'er', ' g', 'et', ' o', 'ut', ', ', 'do', ' y', 'ou', '?\"', '\\n\\n', 'i ', 'ey', 'ed', ' t', 'he', ' g', 'ri', 'nn', 'in', 'g ', 'sk', 'ul', 'l ', 'an', 'd ', 'gr', 'ow']\n",
      "\n",
      "Processing file dresden_files/015.txt\n",
      "Data size (Characters) (Document 14) 22221\n",
      "Sample string (Document 14) ['\\ni', ' w', 'as', ' r', 'ig', 'ht', '. ', 'bu', 't ', 'th', 'e ', 'po', 'te', 'nt', 'ia', 'l ', 'ga', 'in', 's ', 'ma', 'de', ' t', 'he', ' r', 'is', 'k ', 'wo', 'rt', 'h ', 'it', '. ', 'i ', 'co', 'ul', 'd ', 'po', 'ss', 'ib', 'ly', ' f', 'in', 'd ', 'th', 'e ', 'ki', 'll', 'er', 's,', ' s', 'to']\n",
      "\n",
      "Processing file dresden_files/016.txt\n",
      "Data size (Characters) (Document 15) 21836\n",
      "Sample string (Document 15) ['he', ' w', 'en', 't ', 'on', ' l', 'ik', 'e ', 'th', 'at', ' u', 'nt', 'il', ' h', 'e ', 'sh', 'ra', 'nk', ' t', 'o ', 'th', 'e ', 'si', 'ze', ' o', 'f ', 'a ', 'pi', 'np', 'oi', 'nt', ' a', 'nd', ' v', 'an', 'is', 'he', 'd ', 'wi', 'th', ' a', ' l', 'it', 'tl', 'e,', ' i', 'mp', 'lo', 'di', 'ng']\n",
      "\n",
      "Processing file dresden_files/017.txt\n",
      "Data size (Characters) (Document 16) 20339\n",
      "Sample string (Document 16) ['\\ni', ' s', 'tu', 'di', 'ed', ' t', 'he', ' u', 'su', 'al', 'ly', ' q', 'ui', 'et', ' t', 'er', 'a.', ' s', 'he', ' w', 'as', ' w', 'al', 'ki', 'ng', ' w', 'it', 'h ', 'ex', 'ag', 'ge', 'ra', 'te', 'd ', 'mo', 'ti', 'on', 's,', ' p', 'la', 'nt', 'in', 'g ', 'he', 'r ', 'fe', 'et', ' d', 'ow', 'n ']\n",
      "\n",
      "Processing file dresden_files/018.txt\n",
      "Data size (Characters) (Document 17) 29809\n",
      "Sample string (Document 17) ['\\nh', 'er', ' b', 'ac', 'k ', 'ar', 'ch', 'ed', ' a', 's ', 'sh', 'e ', 'sp', 'un', ' a', 'nd', ' w', 'hi', 'rl', 'ed', ', ', 'of', 'fe', 'ri', 'ng', ' o', 'ut', ' h', 'er', ' b', 're', 'as', 'ts', ' t', 'o ', 'th', 'e ', 'ch', 'il', 'l ', 'ra', 'in', ', ', 'an', 'd ', 'he', 'r ', 'sk', 'in', ' w']\n",
      "\n",
      "Processing file dresden_files/019.txt\n",
      "Data size (Characters) (Document 18) 21321\n",
      "Sample string (Document 18) ['ch', 'ap', 'te', 'r ', '20', '\\n\\n', '\\ni', ' a', 'wo', 'ke', ' i', 'n ', 'a ', 'da', 'rk', ' p', 'la', 'ce', '. ', 'it', ' w', 'as', ' l', 'ik', 'e ', 'th', 'e ', 'in', 'si', 'de', ' o', 'f ', 'a ', 'wa', 're', 'ho', 'us', 'e,', ' o', 'r ', 'a ', 'bi', 'g,', ' u', 'nd', 'er', 'gr', 'ou', 'nd', ' g']\n",
      "\n",
      "Processing file dresden_files/020.txt\n",
      "Data size (Characters) (Document 19) 23957\n",
      "Sample string (Document 19) ['\\nb', 'ut', ' t', 'ha', 't ', 'di', 'dn', \"'t\", ' e', 'xp', 'la', 'in', ' w', 'ho', ' h', 'ad', ' d', 'on', 'e ', 'th', 'is', ' t', 'o ', 'me', '. ', 'or', ' w', 'hy', '.\\n', '\\ni', ' l', 'oo', 'ke', 'd ', 'ar', 'ou', 'nd', ' t', 'he', ' d', 'im', 'ne', 'ss', ' o', 'f ', 'th', 'e ', 'ga', 'ra', 'ge']\n",
      "\n",
      "Processing file dresden_files/021.txt\n",
      "Data size (Characters) (Document 20) 21288\n",
      "Sample string (Document 20) ['wh', 'en', ' y', 'ou', \"'r\", 'e ', 'ch', 'an', 'ge', 'd,', ' w', 'he', 'n ', 'yo', \"u'\", 're', ' a', ' b', 'ea', 'st', ', ', 'it', \"'s\", ' s', 'o ', 'in', 'cr', 'ed', 'ib', 'le', '. ', 'so', ' m', 'uc', 'h ', 'sp', 'ee', 'd,', ' p', 'ow', 'er', '. ', 'yo', 'ur', ' b', 'od', 'y ', 'ju', 'st', ' s']\n",
      "\n",
      "Processing file dresden_files/022.txt\n",
      "Data size (Characters) (Document 21) 23789\n",
      "Sample string (Document 21) ['\\n\"', 'ca', 'me', 'ra', 's,', '\" ', 'i ', 'mu', 'tt', 'er', 'ed', '. ', '\"h', 'el', 'l.', '\"\\n', '\\n\"', 'co', 'me', ', ', 'wi', 'za', 'rd', ',\"', ' t', 'er', 'a ', 'sa', 'id', ', ', 'cr', 'ou', 'ch', 'in', 'g ', 'do', 'wn', ' o', 'n ', 'al', 'l ', 'fo', 'ur', 's.', ' \"', 'we', ' h', 'av', 'e ', 'no']\n",
      "\n",
      "Processing file dresden_files/023.txt\n",
      "Data size (Characters) (Document 22) 22326\n",
      "Sample string (Document 22) ['\\n\\n', '\"y', 'ou', ' w', 'er', 'e ', 'ti', 'ed', ' u', 'p,', '\" ', 'i ', 'sa', 'id', '. ', '\"h', 'ow', \"'d\", ' y', 'ou', ' g', 'et', ' l', 'oo', 'se', '?\"', '\\n\\n', '\"s', 'he', ' h', 'ad', ' h', 'el', 'p,', '\" ', 'so', 'me', 'on', 'e ', 'sa', 'id', ' i', 'n ', 'a ', 'sl', 'ur', 're', 'd,', ' h', 'ea']\n",
      "\n",
      "Processing file dresden_files/024.txt\n",
      "Data size (Characters) (Document 23) 7031\n",
      "Sample string (Document 23) ['\\n\\n', 'mu', 'rp', 'hy', ' n', 'ar', 'ro', 'we', 'd ', 'he', 'r ', 'ey', 'es', ', ', 'pe', 'er', 'in', 'g ', 'do', 'wn', ' t', 'he', ' s', 'ha', 'ki', 'ng', ' b', 'ar', 're', 'l ', 'of', ' h', 'er', ' g', 'un', '. ', 'fl', 'am', 'e ', 'bl', 'os', 'so', 'me', 'd ', 'fr', 'om', ' t', 'he', ' b', 'ar']\n",
      "\n",
      "Processing file dresden_files/025.txt\n",
      "Data size (Characters) (Document 24) 18766\n",
      "Sample string (Document 24) ['th', 'er', 'e ', 'ar', 'e ', 're', 'as', 'on', 's ', 'i ', 'ha', 'te', ' t', 'o ', 'dr', 'iv', 'e ', 'fa', 'st', '. ', 'fo', 'r ', 'on', 'e,', ' t', 'he', ' b', 'lu', 'e ', 'be', 'et', 'le', ', ', 'th', 'e ', 'mi', 'sm', 'at', 'ch', 'ed', ' v', 'ol', 'ks', 'wa', 'ge', 'n ', 'bu', 'g ', 'th', 'at']\n",
      "\n",
      "Processing file dresden_files/026.txt\n",
      "Data size (Characters) (Document 25) 21040\n",
      "Sample string (Document 25) ['th', 'e ', 'da', 'y ', 'we', 'nt', ' f', 'ai', 'rl', 'y ', 'qu', 'ic', 'kl', 'y,', ' w', 'it', 'h ', 'a ', 'bu', 'nc', 'h ', 'of', ' t', 'he', ' u', 'su', 'al', ' b', 'us', 'in', 'es', 's.', ' i', ' w', 'hi', 'pp', 'ed', ' u', 'p ', 'a ', 'sp', 'el', 'l ', 'to', ' f', 'in', 'd ', 'a ', 'lo', 'st']\n",
      "\n",
      "Processing file dresden_files/027.txt\n",
      "Data size (Characters) (Document 26) 17211\n",
      "Sample string (Document 26) ['\\nw', 'e ', 'bo', 'th', ' p', 'ut', ' o', 'ur', ' h', 'an', 'ds', ' u', 'p ', 'on', ' t', 'op', ' o', 'f ', 'ou', 'r ', 'he', 'ad', 's,', ' a', 's ', 'a ', 'ch', 'ic', 'ag', 'o ', 'p.', 'd.', ' p', 'at', 'ro', 'lm', 'an', ', ', 'hi', 's ', 'ja', 'ck', 'et', ' a', 'nd', ' p', 'an', 'ts', ' s', 'ta']\n",
      "\n",
      "Processing file dresden_files/028.txt\n",
      "Data size (Characters) (Document 27) 18791\n",
      "Sample string (Document 27) ['my', ' h', 'ea', 'rt', ' p', 'ou', 'nd', 'in', 'g.', '\\n\\n', 'i ', 'ca', 'me', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'dr', 'ea', 'm,', ' b', 'li', 'nk', 'ed', ' o', 'pe', 'ne', 'd ', 'my', ' e', 'ye', 's.', ' m', 'y ', 'do', 'or', ' r', 'at', 'tl', 'ed', ' o', 'n ', 'it', 's ', 'fr', 'am', 'e ', 'un']\n",
      "\n",
      "Processing file dresden_files/029.txt\n",
      "Data size (Characters) (Document 28) 17612\n",
      "Sample string (Document 28) ['th', 'e ', 'ri', 'de', ' w', 'as', ' q', 'ui', 'et', ', ', 'ex', 'ce', 'pt', ' f', 'or', ' a', ' l', 'it', 'tl', 'e ', 'me', 'an', 'in', 'gl', 'es', 's ', 'ch', 'at', 'te', 'r ', 'fr', 'om', ' s', 'ta', 'll', 'in', 'gs', '. ', 'ru', 'do', 'lp', 'h ', 'ig', 'no', 're', 'd ', 'me', '. ', 'i ', 'cl']\n",
      "\n",
      "Processing file dresden_files/030.txt\n",
      "Data size (Characters) (Document 29) 19827\n",
      "Sample string (Document 29) ['\\n\"', 'kr', 'av', 'os', '?\"', ' s', 'us', 'an', ' a', 'sk', 'ed', '. ', '\"l', 'eo', 'ni', 'd ', 'kr', 'av', 'os', '?\"', '\\n\\n', '\"y', 'ea', 'h,', ' t', 'ha', 't ', 'mi', 'gh', 't ', 'ha', 've', ' b', 'ee', 'n ', 'it', ', ', 'i ', 'th', 'in', 'k.', '\"\\n', '\\n\"', 'gr', 'ea', 't,', '\" ', 'sh', 'e ', 'sa']\n",
      "\n",
      "Processing file dresden_files/031.txt\n",
      "Data size (Characters) (Document 30) 16507\n",
      "Sample string (Document 30) ['i ', 'cl', 'os', 'ed', ' m', 'y ', 'ey', 'es', ' a', 'nd', ' b', 're', 'at', 'he', 'd,', ' a', 'nd', ' t', 'ri', 'ed', ' t', 'o ', 'so', 'rt', ' t', 'hr', 'ou', 'gh', ' t', 'hi', 'ng', 's.', '\\n\\n', 'ly', 'di', 'a ', 'ha', 'd ', 'fl', 'ed', ' t', 'he', ' s', 'he', 'lt', 'er', ' o', 'f ', 'th', 'e ']\n",
      "\n",
      "Processing file dresden_files/032.txt\n",
      "Data size (Characters) (Document 31) 20192\n",
      "Sample string (Document 31) ['i ', 'st', 'ar', 'ed', ' f', 'or', ' a', ' s', 'ec', 'on', 'd ', 'in', ' s', 'he', 'er', ' h', 'or', 'ro', 'r ', 'at', ' t', 'he', ' w', 'ei', 'rd', 'ne', 'ss', ' o', 'f ', 'th', 'e ', 'si', 'gh', 't.', ' t', 'he', 'n ', 'bl', 'ur', 'te', 'd,', ' \"', 'ge', 't ', 'th', 'e ', 'he', 'll', ' o', 'ff']\n",
      "\n",
      "Processing file dresden_files/033.txt\n",
      "Data size (Characters) (Document 32) 15911\n",
      "Sample string (Document 32) ['am', 'or', 'ac', 'ch', 'iu', 's ', 'cu', 't ', 'in', 'to', ' t', 'he', ' n', 'ig', 'ht', 'ma', 're', \"'s\", ' m', 'id', 'se', 'ct', 'io', 'n,', ' a', 'nd', ' w', 'hi', 'te', ' f', 'ir', 'e ', 'er', 'up', 'te', 'd ', 'fr', 'om', ' t', 'he', ' w', 'ou', 'nd', '.\\n', '\\ni', ' h', 'ur', 'ri', 'ed', ' f']\n",
      "\n",
      "Processing file dresden_files/034.txt\n",
      "Data size (Characters) (Document 33) 18663\n",
      "Sample string (Document 33) ['de', 'mo', 'ns', ', ', 'ho', 'we', 've', 'r,', ' a', 're', ' a', ' d', 'if', 'fe', 're', 'nt', ' m', 'at', 'te', 'r.', ' d', 'em', 'on', 's ', 'ar', 'en', \"'t\", ' p', 'eo', 'pl', 'e.', ' t', 'he', 'y ', 'do', \"n'\", 't ', 'ha', 've', ' t', 'he', ' p', 'ro', 'bl', 'em', ' o', 'f ', 'ha', 'vi', 'ng']\n",
      "\n",
      "Processing file dresden_files/035.txt\n",
      "Data size (Characters) (Document 34) 16637\n",
      "Sample string (Document 34) ['\"u', 'm,', ' t', 'ha', 'nk', 's,', ' t', 'ho', 'ma', 's.', '\"\\n', '\\nh', 'e ', 'gl', 'an', 'ce', 'd ', 'as', 'id', 'e,', ' a', 'nd', ' f', 'ro', 'wn', 'ed', '. ', 'i ', 'fo', 'll', 'ow', 'ed', ' h', 'is', ' g', 'az', 'e.', ' j', 'us', 'ti', 'ne', ' h', 'ad', ' d', 'ri', 'ft', 'ed', ' a', 'wa', 'y ']\n",
      "\n",
      "Processing file dresden_files/036.txt\n",
      "Data size (Characters) (Document 35) 17395\n",
      "Sample string (Document 35) ['\"m', 'y ', 'ey', 'es', '?\"', ' s', 'us', 'an', ' s', 'ta', 'mm', 'er', 'ed', '.\\n', '\\n\"', 'no', '?\"', ' l', 'ea', ' a', 'sk', 'ed', '. ', '\"v', 'er', 'y ', 'we', 'll', '. ', 'yo', 'ur', ' n', 'am', 'e,', ' p', 'er', 'ha', 'ps', '? ', 'yo', 'ur', ' w', 'ho', 'le', ' n', 'am', 'e?', '\"\\n', '\\n\"', 'do']\n",
      "\n",
      "Processing file dresden_files/037.txt\n",
      "Data size (Characters) (Document 36) 20767\n",
      "Sample string (Document 36) ['i ', 'fr', 'ow', 'ne', 'd ', 'an', 'd ', 'gl', 'an', 'ce', 'd ', 'do', 'wn', ' a', 't ', 'th', 'e ', 'gi', 'rl', '. ', 'ev', 'en', ' w', 'or', 'ri', 'ed', ', ', 'sh', 'e ', 'wa', 's ', 'te', 'rr', 'ib', 'ly', ' b', 'ea', 'ut', 'if', 'ul', ', ', 'th', 'ou', 'gh', ' h', 'er', ' p', 'ro', 'xi', 'mi']\n",
      "\n",
      "Processing file dresden_files/038.txt\n",
      "Data size (Characters) (Document 37) 15404\n",
      "Sample string (Document 37) ['i ', 'th', 'ou', 'gh', 't ', 'of', ' t', 'he', ' f', 'ir', 'e,', ' t', 'ow', 'er', 'in', 'g ', 'up', ' i', 'n ', 'wa', 'll', 's ', 'of', ' s', 'ol', 'id', ' f', 'la', 'me', ', ', 're', 'ac', 'hi', 'ng', ' o', 'ut', ' w', 'it', 'h ', 'hu', 'ng', 'ry', ' a', 'rm', 's ', 'to', ' d', 'ra', 'g ', 'th']\n",
      "\n",
      "Processing file dresden_files/039.txt\n",
      "Data size (Characters) (Document 38) 17220\n",
      "Sample string (Document 38) ['\\nt', 'ho', 'ma', 's ', 'mu', 'rm', 'ur', 'ed', ', ', 'so', ' s', 'of', 'tl', 'y ', 'th', 'at', ' i', ' w', 'as', \"n'\", 't ', 'su', 're', ' i', \"'d\", ' h', 'ea', 'rd', ' h', 'im', '. ', '\"i', ' d', 'on', \"'t\", ' k', 'no', 'w ', 'ho', 'w ', 'lo', 'ng', ' i', ' c', 'an', ' d', 'is', 'tr', 'ac', 't ']\n",
      "\n",
      "Processing file dresden_files/040.txt\n",
      "Data size (Characters) (Document 39) 17316\n",
      "Sample string (Document 39) ['th', 'e ', 'wi', 'nd', ' r', 'os', 'e ', 'to', ' a', ' h', 'ow', 'l,', ' a', 'nd', ' s', 'ha', 'pe', 's ', 'be', 'ga', 'n ', 'to', ' c', 're', 'st', ' t', 'he', ' t', 'op', ' o', 'f ', 'th', 'e ', 'hi', 'll', ' w', 'it', 'h ', 'th', 'e ', 'do', 'lm', 'en', 's,', ' d', 'ar', 'k ', 'th', 'in', 'gs']\n",
      "\n",
      "Processing file dresden_files/041.txt\n",
      "Data size (Characters) (Document 40) 15553\n",
      "Sample string (Document 40) ['i ', 're', 'ac', 'he', 'd ', 'up', ' w', 'it', 'h ', 'my', ' o', 'th', 'er', ' s', 'ha', 'ki', 'ng', ' h', 'an', 'd,', ' a', 'nd', ' s', 'tr', 'ok', 'ed', ' a', 't ', 'he', 'r ', 'ha', 'ir', '. ', '\"s', 'us', 'an', ',\"', ' i', ' s', 'ai', 'd,', ' g', 'en', 'tl', 'e.', ' \"', 'su', 'sa', 'n.', ' s']\n",
      "\n",
      "Processing file dresden_files/042.txt\n",
      "Data size (Characters) (Document 41) 11866\n",
      "Sample string (Document 41) ['\\n\"', 'hi', 'ya', ', ', 'do', 'n,', '\" ', 'i ', 'sa', 'id', '. ', '\"i', \"'d\", ' l', 'ik', 'e ', 'to', ' l', 'od', 'ge', ' a', ' c', 'om', 'pl', 'ai', 'nt', '.\"', '\\n\\n', 'he', ' s', 'mi', 'le', 'd,', ' a', ' s', 'ho', 'w ', 'of', ' b', 'ro', 'ad', ', ', 'wh', 'it', 'e ', 'te', 'et', 'h.', ' \"', \"i'\"]\n",
      "\n",
      "Processing file dresden_files/043.txt\n",
      "Data size (Characters) (Document 42) 19008\n",
      "Sample string (Document 42) ['it', ' r', 'ai', 'ne', 'd ', 'to', 'ad', 's ', 'th', 'e ', 'da', 'y ', 'th', 'e ', 'wh', 'it', 'e ', 'co', 'un', 'ci', 'l ', 'ca', 'me', ' t', 'o ', 'to', 'wn', '.\\n', '\\ni', ' g', 'ot', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'bl', 'ue', ' b', 'ee', 'tl', 'e,', ' m', 'y ', 'be', 'at', '-u', 'p ', 'ol']\n",
      "\n",
      "Processing file dresden_files/044.txt\n",
      "Data size (Characters) (Document 43) 17732\n",
      "Sample string (Document 43) ['\\ns', 'he', ' s', 'mi', 'le', 'd.', ' a', ' s', 'lo', 'w ', 'sm', 'il', 'e,', ' c', 'ru', 'el', ' a', 's ', 'a ', 'ba', 'rb', 'ed', ' k', 'ni', 'fe', '. ', 'wh', 'en', ' s', 'he', ' s', 'po', 'ke', ', ', 'he', 'r ', 'vo', 'ic', 'e ', 'so', 'un', 'de', 'd ', 'ju', 'st', ' a', 's ', 'be', 'au', 'ti']\n",
      "\n",
      "Processing file dresden_files/045.txt\n",
      "Data size (Characters) (Document 44) 22806\n",
      "Sample string (Document 44) ['\\n\"', 'i ', 'kn', 'ow', '. ', 'i ', 'ne', 've', 'r ', 'sa', 'w ', 'yo', 'ur', ' f', 'ac', 'e ', 'be', 'fo', 're', ' t', 'od', 'ay', '.\"', ' s', 'he', ' m', 'ov', 'ed', ' t', 'he', 'n,', ' l', 'if', 'ti', 'ng', ' a', ' s', 'le', 'nd', 'er', ' s', 'ta', 'ff', ' o', 'f ', 'so', 'me', ' d', 'ar', 'k ']\n",
      "\n",
      "Processing file dresden_files/046.txt\n",
      "Data size (Characters) (Document 45) 17983\n",
      "Sample string (Document 45) ['\\n\"', 'li', 'ke', ' h', 'el', 'l ', 'i ', 'ha', 've', \"n'\", 't,', '\" ', 'i ', 'an', 'sw', 'er', 'ed', ' h', 'im', '. ', '\"i', ' b', 'ea', 't ', 'ju', 'st', 'in', ' d', 'um', 'or', 'ne', ' i', 'n ', 'a ', 'du', 'el', ' t', 'o ', 'th', 'e ', 'de', 'at', 'h.', '\\n\\n', 'is', ' t', 'ha', 't ', 'no', 't ']\n",
      "\n",
      "Processing file dresden_files/047.txt\n",
      "Data size (Characters) (Document 46) 15054\n",
      "Sample string (Document 46) ['\"i', \"t'\", 's ', 'yo', 'ur', ' j', 'ob', ' t', 'o ', 'st', 'op', ' t', 'he', ' b', 'ad', ' g', 'uy', 's,', '\" ', 'i ', 'sa', 'id', '. ', '\"b', 'ut', ' t', 'hi', 's ', 'mi', 'gh', 't ', 'no', 't ', 'be', ' a', ' g', 'uy', '. ', 'ma', 'yb', 'e ', 'no', 't ', 'ev', 'en', ' h', 'um', 'an', '. ', 'i ']\n",
      "\n",
      "Processing file dresden_files/048.txt\n",
      "Data size (Characters) (Document 47) 16163\n",
      "Sample string (Document 47) ['in', ' t', 'ha', 't ', 'mo', 'me', 'nt', ', ', 'i ', 'wa', 's ', 'su', 're', ' t', 'ha', 't ', 'i ', 'co', 'ul', 'd ', 'ha', 've', ' k', 'il', 'le', 'd ', 'hi', 'm.', ' h', 'e ', 'mi', 'gh', 't ', 'ha', 've', ' t', 'ak', 'en', ' m', 'e ', 'wi', 'th', ' h', 'im', ', ', 'bu', 't ', 'i ', 'co', 'ul']\n",
      "\n",
      "Processing file dresden_files/049.txt\n",
      "Data size (Characters) (Document 48) 19311\n",
      "Sample string (Document 48) ['th', 'e ', 'ma', 'n ', 'in', 'si', 'de', ' r', 'eu', 'el', \"'s\", ' a', 'pa', 'rt', 'me', 'nt', ' l', 'oo', 'ke', 'd ', 'li', 'ke', ' a', ' c', 'at', 'al', 'og', ' m', 'od', 'el', ' f', 'or', ' t', 'hu', 'gs', '-r', '-u', 's.', ' h', 'e ', 'st', 'oo', 'd ', 'a ', 'ha', 'nd', ' t', 'al', 'le', 'r ']\n",
      "\n",
      "Processing file dresden_files/050.txt\n",
      "Data size (Characters) (Document 49) 20114\n",
      "Sample string (Document 49) ['\"y', 'ea', 'h,', '\" ', 'i ', 'sa', 'id', '. ', '\"d', 'on', \"'t\", ' l', 'oo', 'k ', 'at', ' i', 't ', 'fo', 'r ', 'to', 'o ', 'lo', 'ng', '. ', 'fa', 'er', 'ie', ' l', 'ig', 'ht', 's ', 'ca', 'n ', 'be', ' d', 'is', 'or', 'ie', 'nt', 'in', 'g ', 'to', ' m', 'or', 'ta', 'ls', '.\"', '\\n\\n', '\\n\\n', '\"g']\n",
      "\n",
      "Processing file dresden_files/051.txt\n",
      "Data size (Characters) (Document 50) 16133\n",
      "Sample string (Document 50) ['\"y', 'ep', '. ', 'di', 'd ', 'yo', 'u ', 'ki', 'll', ' r', 'on', 'al', 'd ', 're', 'ue', 'l?', '\"\\n', '\\ns', 'la', 'te', ' b', 'ur', 'st', ' o', 'ut', ' l', 'au', 'gh', 'in', 'g.', ' \"', 'ch', 'ri', 'st', ', ', 'dr', 'es', 'de', 'n.', ' y', 'ou', ' d', 'on', \"'t\", ' w', 'as', 'te', ' t', 'im', 'e,']\n",
      "\n",
      "Processing file dresden_files/052.txt\n",
      "Data size (Characters) (Document 51) 13608\n",
      "Sample string (Document 51) ['\"h', 'el', 'l ', 'if', ' i', ' k', 'no', 'w,', '\" ', 'i ', 'sa', 'id', ' b', 'et', 'we', 'en', ' b', 'it', 'es', '. ', '\"s', 'he', ' w', 'as', ' i', 'n ', 'my', ' c', 'ar', ' l', 'ik', 'e ', 'th', 'at', '. ', 'wa', 'nt', 'ed', ' m', 'e ', 'to', ' b', 'ri', 'ng', ' h', 'er', ' h', 'er', 'e.', '\"\\n']\n",
      "\n",
      "Processing file dresden_files/053.txt\n",
      "Data size (Characters) (Document 52) 18788\n",
      "Sample string (Document 52) ['\\n\"', 'go', 'od', ' l', 'or', 'd,', '\" ', 'mu', 'rp', 'hy', ' s', 'ai', 'd ', 'so', 'ft', 'ly', '. ', '\"h', 'ar', 'ry', ', ', 'wh', 'at', \"'s\", ' h', 'ap', 'pe', 'ni', 'ng', '?\"', '\\n\\n', 'i ', 'ha', 'd ', 'al', 're', 'ad', 'y ', 'go', 'tt', 'en', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'bo', 'ot', 'h ']\n",
      "\n",
      "Processing file dresden_files/054.txt\n",
      "Data size (Characters) (Document 53) 18326\n",
      "Sample string (Document 53) ['\"d', 're', 'sd', 'en', ',\"', ' m', 'ur', 'ph', 'y ', 'sa', 'id', ', ', '\"m', 'y ', 'gu', 'n ', 'ha', 's ', 'be', 'en', ' a', 'bo', 'ut', ' a', 's ', 'us', 'ef', 'ul', ' a', 's ', 'fa', 'br', 'ic', ' s', 'of', 'te', 'ne', 'r ', 'in', ' a', ' s', 'te', 'el', ' m', 'il', 'l ', 'to', 'ni', 'gh', 't.']\n",
      "\n",
      "Processing file dresden_files/055.txt\n",
      "Data size (Characters) (Document 54) 19837\n",
      "Sample string (Document 54) ['\"b', 'ut', ' i', ' a', 'lr', 'ea', 'dy', ' h', 'av', 'e.', '\"\\n', '\\ni', ' f', 'ol', 'de', 'd ', 'my', ' a', 'rm', 's.', ' \"', 'wh', 'en', '?\"', '\\n\\n', '\"i', 'f ', 'yo', 'u ', 'wi', 'll', ' r', 'em', 'em', 'be', 'r ', 'th', 'at', ' n', 'ig', 'ht', ' i', 'n ', 'th', 'e ', 'bo', 'ne', 'ya', 'rd', ', ']\n",
      "\n",
      "Processing file dresden_files/056.txt\n",
      "Data size (Characters) (Document 55) 17183\n",
      "Sample string (Document 55) ['\"t', 'em', 'pt', 'in', 'g,', '\" ', 'el', 'ai', 'ne', ' s', 'ai', 'd.', ' \"', 'bu', 't ', 'i ', 'do', \"n'\", 't ', 'th', 'in', 'k ', 'it', ' w', 'il', 'l ', 'ma', 'ke', ' m', 'uc', 'h ', 'of', ' a', 'n ', 'im', 'pr', 'es', 'si', 'on', ' o', 'n ', 'th', 'e ', 'mo', 'th', 'er', 's ', 'if', ' w', 'e ']\n",
      "\n",
      "Processing file dresden_files/057.txt\n",
      "Data size (Characters) (Document 56) 17695\n",
      "Sample string (Document 56) ['i ', 'le', 't ', 'ou', 't ', 'a ', 'ha', 'rs', 'h ', 'br', 'ay', ' o', 'f ', 'la', 'ug', 'ht', 'er', '. ', '\"i', ' h', 'op', 'e ', 'yo', 'u ', 'go', 't ', 'th', 'at', ' i', 'n ', 'wr', 'it', 'in', 'g,', ' d', 'im', 'wi', 't.', ' d', 'o ', 'yo', 'u ', 're', 'al', 'ly', ' t', 'hi', 'nk', ' s', 'he']\n",
      "\n",
      "Processing file dresden_files/058.txt\n",
      "Data size (Characters) (Document 57) 22522\n",
      "Sample string (Document 57) ['i ', 'fu', 'mb', 'le', 'd ', 'fo', 'r ', 'my', ' a', 'mu', 'le', 't ', 'wi', 'th', ' c', 'ol', 'd ', 'fi', 'ng', 'er', 's,', ' b', 'ut', ' f', 'ix', ' a', 'nd', ' m', 'er', 'yl', ' b', 'ot', 'h ', 'be', 'at', ' m', 'e ', 'to', ' i', 't.', '\\n\\n', 'fi', \"x'\", 's ', 'to', 'ol', 'bo', 'x ', 'th', 'un']\n",
      "\n",
      "Processing file dresden_files/059.txt\n",
      "Data size (Characters) (Document 58) 13930\n",
      "Sample string (Document 58) ['\\n\"', 'yo', 'u ', 'pa', 'id', ' f', 'or', ' i', 't,', '\" ', 'i ', 'sa', 'id', '. ', '\"s', 'ta', 'y ', 'he', 're', '. ', 'st', 'ay', ' d', 'ow', 'n.', ' y', 'ou', \"'v\", 'e ', 'do', 'ne', ' e', 'no', 'ug', 'h.', '\"\\n', '\\ns', 'he', ' s', 'ho', 'ok', ' h', 'er', ' h', 'ea', 'd ', 'an', 'd ', 'sa', 'id']\n",
      "\n",
      "Processing file dresden_files/060.txt\n",
      "Data size (Characters) (Document 59) 19698\n",
      "Sample string (Document 59) ['so', 'me', ' t', 'hi', 'ng', 's ', 'ju', 'st', ' a', 're', \"n'\", 't ', 'me', 'an', 't ', 'to', ' g', 'o ', 'to', 'ge', 'th', 'er', '. ', 'th', 'in', 'gs', ' l', 'ik', 'e ', 'oi', 'l ', 'an', 'd ', 'wa', 'te', 'r.', ' o', 'ra', 'ng', 'e ', 'ju', 'ic', 'e ', 'an', 'd ', 'to', 'ot', 'hp', 'as', 'te']\n",
      "\n",
      "Processing file dresden_files/061.txt\n",
      "Data size (Characters) (Document 60) 16893\n",
      "Sample string (Document 60) ['\\n\\n', '\"a', 'n ', 'in', 've', 'st', 'ig', 'at', 'io', 'n ', 'is', ' u', 'nd', 'er', ' w', 'ay', ' a', 'nd', ' i', ' a', 'm ', 'gi', 'vi', 'ng', ' y', 'ou', 'r ', 'po', 'li', 'ce', ' d', 'ep', 'ar', 'tm', 'en', 't ', 'my', ' f', 'ul', 'l ', 'co', 'op', 'er', 'at', 'io', 'n.', ' b', 'ut', ' ', ' h']\n",
      "\n",
      "Processing file dresden_files/062.txt\n",
      "Data size (Characters) (Document 61) 18107\n",
      "Sample string (Document 61) ['\\n\"', 'un', 'of', 'fi', 'ci', 'al', ' m', 'ea', 'ns', ' n', 'ot', ' p', 'ai', 'd,', ' i', ' g', 'ue', 'ss', ',\"', ' i', ' s', 'ai', 'd.', '\\n\\n', '\"y', 'ou', ' u', 'p ', 'fo', 'r ', 'an', 'y ', 'pr', 'o ', 'bo', 'no', ' w', 'or', 'k?', '\" ', 'sh', 'e ', 'pa', 'us', 'ed', ' a', 'nd', ' t', 'he', 'n ']\n",
      "\n",
      "Processing file dresden_files/063.txt\n",
      "Data size (Characters) (Document 62) 14703\n",
      "Sample string (Document 62) ['\\nt', 'he', 'n ', 'it', 's ', 'bo', 'dy', ' t', 'op', 'pl', 'ed', ' s', 'lo', 'wl', 'y ', 'ov', 'er', ' t', 'o ', 'on', 'e ', 'si', 'de', ', ', 'le', 'av', 'in', 'g ', 'it', 's ', 'mo', 'ns', 'tr', 'ou', 's ', 'he', 'ad', ' l', 'yi', 'ng', ' o', 'n ', 'th', 'e ', 'al', 'le', 'y ', 'fl', 'oo', 'r.']\n",
      "\n",
      "Processing file dresden_files/064.txt\n",
      "Data size (Characters) (Document 63) 17248\n",
      "Sample string (Document 63) ['\\n\"', 'pr', 'ob', 'ab', 'ly', ',\"', ' b', 'ob', ' a', 'gr', 'ee', 'd.', ' \"', 'bu', 't ', 'if', ' i', 't ', 'wa', 's ', 're', 'al', 'ly', ' t', 'ha', 't ', 'ma', 'ny', ' d', 'is', 'ea', 'se', 's,', ' i', 't ', 'wa', 's ', 'a ', 'bi', 'g ', 'on', 'e.', '\"\\n', '\\n\"', 'ho', 'w ', 'bi', 'g?', '\"\\n', '\\n\"']\n",
      "\n",
      "Processing file dresden_files/065.txt\n",
      "Data size (Characters) (Document 64) 15200\n",
      "Sample string (Document 64) ['\\ni', ' g', 'la', 'nc', 'ed', ' p', 'as', 't ', 'hi', 'm.', ' \"', 'wh', 'at', ' d', 'o ', 'yo', 'u ', 'me', 'an', '?\"', '\\n\\n', 'a ', 'se', 'co', 'nd', ' l', 'at', 'er', ', ', 'a ', 'li', 'tt', 'le', ' g', 'ir', 'l ', 'st', 'ar', 'te', 'd ', 'do', 'wn', ' m', 'y ', 'st', 'ai', 'rs', ', ', 'on', 'e ']\n",
      "\n",
      "Processing file dresden_files/066.txt\n",
      "Data size (Characters) (Document 65) 15483\n",
      "Sample string (Document 65) ['\\n\"', 'no', 't ', 'th', 'at', ' p', 'la', 'yf', 'ul', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'as', ' l', 'on', 'g ', 'as', ' y', 'ou', ' h', 'av', 'e ', 'th', 'e ', 'sh', 'ro', 'ud', ', ', 'yo', 'ur', ' l', 'iv', 'es', ' a', 're', ' i', 'n ', 'da', 'ng', 'er', '.\\n', '\\ni', 'f ', 'yo', 'u ', 'co', 'me']\n",
      "\n",
      "Processing file dresden_files/067.txt\n",
      "Data size (Characters) (Document 66) 14394\n",
      "Sample string (Document 66) ['\\n\"', 'do', \"n'\", 't ', 'th', 'in', 'k ', 'so', '.\"', '\\n\\n', '\"y', 'ou', ' n', 'ee', 'd ', 're', 'st', '.\"', '\\n\\n', 'i ', 'wa', 've', 'd ', 'a ', 'ha', 'nd', ' a', 't ', 'th', 'e ', 'st', 'at', 'io', 'ne', 'ry', ' p', 'ad', '. ', '\"i', ' w', 'il', 'l.', ' i', ' j', 'us', 't ', 'ha', 've', ' t', 'o ']\n",
      "\n",
      "Processing file dresden_files/068.txt\n",
      "Data size (Characters) (Document 67) 13438\n",
      "Sample string (Document 67) ['\\n\"', 'th', 'er', 'e ', 'is', ' a', ' c', 'or', 'ps', 'e ', 'on', ' a', ' p', 'le', 'as', 'ur', 'e ', 'sh', 'ip', ' i', 'n ', 'bu', 'rn', 'ha', 'm ', 'ha', 'rb', 'or', ' a', 'nd', ' s', 'ev', 'er', 'al', ' e', 'ye', 'wi', 'tn', 'es', 'se', 's ', 'wh', 'o ', 'de', 'sc', 'ri', 'be', ' a', ' t', 'al']\n",
      "\n",
      "Processing file dresden_files/069.txt\n",
      "Data size (Characters) (Document 68) 16375\n",
      "Sample string (Document 68) ['\"t', 'he', ' o', 'ff', 'er', ' t', 'o ', 'ma', 'ke', ' m', 'e ', 'in', 'to', ' a', ' b', 'lo', 'od', '-d', 'ri', 'nk', 'in', 'g ', 'mo', 'ns', 'te', 'r ', 'in', ' e', 'te', 'rn', 'al', ' s', 'la', 've', 'ry', ' t', 'o ', 'yo', 'u?', '\\n\\n', 'wh', 'y ', 'wo', 'ul', 'd ', 'i ', 'wa', 'nt', ' t', 'o ']\n",
      "\n",
      "Processing file dresden_files/070.txt\n",
      "Data size (Characters) (Document 69) 20446\n",
      "Sample string (Document 69) ['\"n', 'ot', ' i', 'f ', 'va', 'ni', 'll', 'a ', 'ma', 'rt', 'in', ' d', 'iv', 'er', 'ts', ' t', 'he', 'ir', ' a', 'tt', 'en', 'ti', 'on', ' b', 'ef', 'or', 'e ', 'we', ' l', 'ea', 've', ' t', 'he', ' f', 'lo', 'or', '.\"', '\\n\\n', '\\n\\n', 'su', 'sa', \"n'\", 's ', 'ey', 'es', ' g', 'li', 'tt', 'er', 'ed']\n",
      "\n",
      "Processing file dresden_files/071.txt\n",
      "Data size (Characters) (Document 70) 16911\n",
      "Sample string (Document 70) ['\"i', ' g', 'ue', 'ss', ' y', 'ou', 'r ', 'hu', 'ma', 'n ', 're', 'so', 'ur', 'ce', 's ', 'de', 'pa', 'rt', 'me', 'nt', ' i', 'sn', \"'t\", ' e', 'xa', 'ct', 'ly', ' u', 'nd', 'er', ' s', 'ie', 'ge', ', ', 'is', ' i', 't,', '\" ', 'i ', 'sa', 'id', '.\\n', '\\nh', 'e ', 'ta', 'mp', 'ed', ' t', 'ob', 'ac']\n",
      "\n",
      "Processing file dresden_files/072.txt\n",
      "Data size (Characters) (Document 71) 16049\n",
      "Sample string (Document 71) ['ma', 'rt', 'in', ' n', 'od', 'de', 'd.', ' \"', 'do', ' y', 'ou', ' h', 'av', 'e ', 'a ', 'sa', 'fe', ' h', 'ou', 'se', '?\"', '\\n\\n', '\"m', 'y ', 'pl', 'ac', 'e.', ' i', \"'v\", 'e ', 'go', 't ', 'so', 'me', ' e', 'me', 'rg', 'en', 'cy', ' w', 'ar', 'ds', ' i', ' c', 'an', ' s', 'et', ' o', 'ff', '. ']\n",
      "\n",
      "Processing file dresden_files/073.txt\n",
      "Data size (Characters) (Document 72) 12162\n",
      "Sample string (Document 72) ['\\ni', ' t', 'ho', 'ug', 'ht', ' o', 'f ', 'ni', 'co', 'de', 'mu', 's ', 'an', 'd ', 'hi', 's ', 'kn', 'if', 'e.', ' o', 'f ', 'th', 'e ', 'sh', 'ee', 'r ', 'he', 'lp', 'le', 'ss', 'ne', 'ss', ' i', \"'d\", ' f', 'el', 't ', 'as', ' h', 'e ', 'dr', 'ew', ' m', 'y ', 'he', 'ad', ' b', 'ac', 'k,', ' b']\n",
      "\n",
      "Processing file dresden_files/074.txt\n",
      "Data size (Characters) (Document 73) 13737\n",
      "Sample string (Document 73) ['sh', 'e ', 'fa', 'ce', 'd ', 'me', ' f', 'ro', 'm ', 'be', 'hi', 'nd', ' a', ' p', 'er', 'fe', 'ct', 'ly', ' s', 'tr', 'ai', 'gh', 't ', 'fa', 'ce', '. ', 'bu', 't ', 'he', 'r ', 'vo', 'ic', 'e ', 'so', 'un', 'de', 'd ', 'a ', 'li', 'tt', 'le', ' c', 'ho', 'ke', 'd.', ' \"', 'no', '.\"', '\\n\\n', '\"y']\n",
      "\n",
      "Processing file dresden_files/075.txt\n",
      "Data size (Characters) (Document 74) 16327\n",
      "Sample string (Document 74) ['i ', 'gr', 'ab', 'be', 'd ', 'my', ' t', 'hi', 'ng', 's,', ' l', 'oc', 'ke', 'd ', 'up', ' m', 'y ', 'pl', 'ac', 'e ', 'be', 'hi', 'nd', ' m', 'e,', ' a', 'nd', ' g', 'ot', ' i', 'nt', 'o ', 'ma', 'rt', 'in', \"'s\", ' r', 'en', 'ta', 'l ', 'ca', 'r.', ' m', 'ar', 'ti', 'n ', 'wa', 'sn', \"'t\", ' i']\n",
      "\n",
      "Processing file dresden_files/076.txt\n",
      "Data size (Characters) (Document 75) 13047\n",
      "Sample string (Document 75) ['\\n\\n', '\"w', 'il', 'l ', 'it', ' s', 'ou', 'nd', ' l', 'ik', 'e ', 'al', 'ar', 'ms', ' g', 'oi', 'ng', ' o', 'ff', ' w', 'he', 'n ', 'th', 'ey', ' d', 'o?', '\" ', 'i ', 'gl', 'an', 'ce', 'd ', 'at', ' t', 'he', ' y', 'ou', 'ng', 'er', ' k', 'ni', 'gh', 't ', 'an', 'd ', 'sa', 'id', ', ', '\"s', 'an']\n",
      "\n",
      "Processing file dresden_files/077.txt\n",
      "Data size (Characters) (Document 76) 15018\n",
      "Sample string (Document 76) ['\\n\"', 'it', \"'s\", ' s', 'im', 'pl', 'e ', 'bu', 'si', 'ne', 'ss', ', ', 'mi', 'st', 'er', ' d', 're', 'sd', 'en', '. ', 'i ', 'ca', \"n'\", 't ', 'co', 'nd', 'uc', 't ', 'bu', 'si', 'ne', 'ss', ' w', 'it', 'h ', 'a ', 'mo', 'un', 'd ', 'of', ' c', 'or', 'ps', 'es', '.\"', '\\n\\n', '\"w', 'hy', ' d', 'on']\n",
      "\n",
      "Processing file dresden_files/078.txt\n",
      "Data size (Characters) (Document 77) 10351\n",
      "Sample string (Document 77) ['i ', 'ha', 'd ', 'so', 'up', '. ', 'it', ' w', 'as', ' e', 'xh', 'au', 'st', 'in', 'g.', ' i', ' t', 'oo', 'k ', 'a ', 'vi', 'co', 'di', 'n ', 'an', 'd ', 'sl', 'ep', 't ', 'wi', 'th', 'ou', 't ', 'dr', 'ea', 'mi', 'ng', '.\\n', '\\no', 've', 'r ', 'th', 'e ', 'ne', 'xt', ' c', 'ou', 'pl', 'e ', 'of']\n",
      "\n",
      "Processing file dresden_files/079.txt\n",
      "Data size (Characters) (Document 78) 14675\n",
      "Sample string (Document 78) ['th', 'e ', 'bu', 'il', 'di', 'ng', ' w', 'as', ' o', 'n ', 'fi', 're', ', ', 'an', 'd ', 'it', ' w', 'as', \"n'\", 't ', 'my', ' f', 'au', 'lt', '.\\n', '\\nm', 'y ', 'bo', 'ot', 's ', 'sl', 'ip', 'pe', 'd ', 'an', 'd ', 'sl', 'id', ' o', 'n ', 'th', 'e ', 'ti', 'le', ' f', 'lo', 'or', ' a', 's ', 'i ']\n",
      "\n",
      "Processing file dresden_files/080.txt\n",
      "Data size (Characters) (Document 79) 14715\n",
      "Sample string (Document 79) ['i ', 'gu', 'es', 's ', 'th', 'at', ' m', 'uc', 'h ', 'bl', 'oo', 'd ', 'le', 'av', 'es', ' a', ' p', 'er', 'ma', 'ne', 'nt', ' s', 'ta', 'in', ', ', 'no', ' m', 'at', 'te', 'r ', 'ho', 'w ', 'mu', 'ch', ' c', 'le', 'an', 'er', ' y', 'ou', ' d', 'um', 'p ', 'on', ' i', 't.', '\\n\\n', 'i ', 'pu', 't ']\n",
      "\n",
      "Processing file dresden_files/081.txt\n",
      "Data size (Characters) (Document 80) 14469\n",
      "Sample string (Document 80) ['mi', 'st', 'er', ' n', 'ar', 'ro', 'we', 'd ', 'hi', 's ', 'ey', 'es', ', ', 'pr', 'ow', 'le', 'd ', 'ov', 'er', ' t', 'o ', 'me', ', ', 'an', 'd ', 'sw', 'at', 'te', 'd ', 'at', ' t', 'he', ' p', 'up', 'py', ' w', 'it', 'h ', 'an', ' i', 'nd', 'ig', 'na', 'nt', ' p', 'aw', '.\\n', '\\n\"', 'ta', 'ke']\n",
      "\n",
      "Processing file dresden_files/082.txt\n",
      "Data size (Characters) (Document 81) 14260\n",
      "Sample string (Document 81) ['\"b', 'y ', 'fi', 've', '.\"', ' i', ' s', 'ig', 'he', 'd.', ' i', ' g', 'ru', 'mb', 'le', 'd ', 'to', ' m', 'ys', 'el', 'f ', 'as', ' i', ' w', 'al', 'ke', 'd ', 'ou', 't ', 'to', ' m', 'y ', 'ca', 'r ', 'an', 'd ', 'le', 'ft', ' f', 'or', ' m', 'y ', 'fi', 'rs', 't ', 'da', 'y ', 'on', ' t', 'he']\n",
      "\n",
      "Processing file dresden_files/083.txt\n",
      "Data size (Characters) (Document 82) 14495\n",
      "Sample string (Document 82) ['\"d', 'on', \"'t\", ' t', 'hi', 'nk', ' s', 'o.', '\"\\n', '\\nj', 'ak', 'e ', 'pi', 'ck', 'ed', ' u', 'p ', 'hi', 's ', 'sp', 'or', 'ts', ' b', 'ag', ', ', 'du', 'g ', 'in', ' i', 't,', ' a', 'nd', ' t', 'os', 'se', 'd ', 'me', ' a', ' s', 'et', ' o', 'f ', 'ke', 'ys', '. ', '\"h', 'er', 'e.', '\\n\\n', 'gi']\n",
      "\n",
      "Processing file dresden_files/084.txt\n",
      "Data size (Characters) (Document 83) 13824\n",
      "Sample string (Document 83) ['\"o', 'h,', '\" ', 'i ', 'sa', 'id', '. ', 'wh', 'en', ' i', 'n ', 'do', 'ub', 't,', ' b', 'e ', 'no', 'nc', 'om', 'mi', 'tt', 'al', '. ', '\"i', ' d', 'id', \"n'\", 't ', 'kn', 'ow', ' y', 'ou', ' h', 'ad', ' a', ' s', 'is', 'te', 'r.', '\"\\n', '\\n\\n', '\\n\"', 'sh', \"e'\", 's ', 'my', ' b', 'ab', 'y ', 'si']\n",
      "\n",
      "Processing file dresden_files/085.txt\n",
      "Data size (Characters) (Document 84) 18039\n",
      "Sample string (Document 84) ['\"h', 'e ', 'wo', 'un', 'd ', 'up', ' m', 'ak', 'in', 'g ', 'an', 'd ', 'di', 're', 'ct', 'in', 'g ', 'se', 'x ', 'fi', 'lm', 's.', ' d', 'id', ' w', 'el', 'l ', 'in', 've', 'st', 'in', 'g ', 'th', 'e ', 'mo', 'ne', 'y,', ' a', 'nd', ' h', \"e'\", 's ', 'wo', 'rt', 'h ', 'a ', 'li', 'tt', 'le', ' m']\n",
      "\n",
      "Processing file dresden_files/086.txt\n",
      "Data size (Characters) (Document 85) 23228\n",
      "Sample string (Document 85) ['i ', 'sh', 'oo', 'k ', 'ou', 't ', 'my', ' s', 'hi', 'el', 'd ', 'br', 'ac', 'el', 'et', ' a', 'nd', ' h', 'ar', 'de', 'ne', 'd ', 'my', ' w', 'il', 'l ', 'in', 'to', ' a', ' w', 'al', 'l ', 'of', ' u', 'ns', 'ee', 'n ', 'bu', 't ', 'so', 'li', 'd ', 'fo', 'rc', 'e ', 'in', ' f', 'ro', 'nt', ' o']\n",
      "\n",
      "Processing file dresden_files/087.txt\n",
      "Data size (Characters) (Document 86) 16056\n",
      "Sample string (Document 86) ['\\n\\n', 'lo', 'rd', ' r', 'ai', 'th', ' l', 'oo', 'ke', 'd ', 'la', 'ra', ' u', 'p ', 'an', 'd ', 'do', 'wn', '. ', '\"t', 'ha', \"t'\", 's ', '. ', '. ', '. ', 'qu', 'it', 'e ', 'a ', 'no', 've', 'l ', 'en', 'se', 'mb', 'le', ' y', 'ou', \"'r\", 'e ', 'we', 'ar', 'in', 'g.', '\"\\n', '\\n\"', 'it', \"'s\", ' b']\n",
      "\n",
      "Processing file dresden_files/088.txt\n",
      "Data size (Characters) (Document 87) 16219\n",
      "Sample string (Document 87) ['\"f', 'ir', 'st', ' c', 'or', 'in', 'th', 'ia', 'ns', ', ', 'ch', 'ap', 'te', 'r ', 'th', 'ir', 'te', 'en', ',\"', ' t', 'ho', 'ma', 's ', 'co', 'nf', 'ir', 'me', 'd.', ' \"', 'i ', 'pa', 'ra', 'ph', 'ra', 'se', 'd.', '\\n\\n', 'fa', 'th', 'er', ' m', 'ak', 'es', ' a', 'll', ' o', 'f ', 'us', ' m', 'em']\n",
      "\n",
      "Processing file dresden_files/089.txt\n",
      "Data size (Characters) (Document 88) 13916\n",
      "Sample string (Document 88) ['on', ' o', 'ne', ' s', 'id', 'e ', 'of', ' m', 'e ', 'lu', 'rk', 'ed', ' a', ' m', 'ys', 'te', 'ri', 'ou', 's ', 'wi', 'el', 'de', 'r ', 'of', ' a', ' s', 'lo', 'pp', 'y ', 'bu', 't ', 'le', 'th', 'al', ' c', 'ur', 'se', '.\\n', '\\no', 'n ', 'th', 'e ', 'ot', 'he', 'r ', 'si', 'de', ', ', 'a ', 'ho']\n",
      "\n",
      "Processing file dresden_files/090.txt\n",
      "Data size (Characters) (Document 89) 15894\n",
      "Sample string (Document 89) ['\"m', 'at', 'er', 'ia', 'li', 'sm', ' i', 's ', 'no', 't ', 'go', 'od', ' f', 'or', ' t', 'he', ' s', 'ou', 'l,', '\" ', 'ja', 'ke', ' s', 'ai', 'd.', ' \"', 'th', 'os', 'e ', 'ar', 'e ', 'th', 'e ', 'fo', 'lk', 's ', 'wh', 'o ', 'ca', 'n ', 'do', ' t', 'he', ' w', 'or', 'st', ', ', 'wh', 'en', ' t']\n",
      "\n",
      "Processing file dresden_files/091.txt\n",
      "Data size (Characters) (Document 90) 16737\n",
      "Sample string (Document 90) ['th', 're', 'e ', 'st', 're', 'ga', 's.', '\\n\\n', 'th', 're', 'e ', 'fo', 'rm', 'er', ' m', 'rs', '. ', 'ar', 'tu', 'ro', ' g', 'en', 'os', 'as', '.\\n', '\\nt', 'he', ' c', 'ur', 'se', ' t', 'ha', 't ', 'ki', 'll', 'ed', ' e', 'mm', 'a ', 'ha', 'd ', 'be', 'en', ' d', 'if', 'fe', 're', 'nt', '. ', 'it']\n",
      "\n",
      "Processing file dresden_files/092.txt\n",
      "Data size (Characters) (Document 91) 12928\n",
      "Sample string (Document 91) ['\"a', 'pp', 'ar', 'en', 'tl', 'y,', '\" ', 'mu', 'rp', 'hy', ' s', 'ai', 'd,', ' h', 'er', ' v', 'oi', 'ce', ' b', 'it', 'te', 'r,', ' \"', 'th', 'at', ' i', 'nc', 'lu', 'de', 's ', 'to', 'le', 'ra', 'ti', 'ng', ' f', 'el', 'on', 'ie', 's ', 'li', 'ke', ' p', 'ro', 'vi', 'di', 'ng', ' a', 'lc', 'oh']\n",
      "\n",
      "Processing file dresden_files/093.txt\n",
      "Data size (Characters) (Document 92) 18043\n",
      "Sample string (Document 92) ['ki', 'nc', 'ai', 'd ', 'do', 'nn', 'ed', ' a', ' s', 'ec', 'on', 'd ', 're', 'd ', 'cr', 'os', 's ', 'ja', 'ck', 'et', ' t', 'o ', 'he', 'lp', ' h', 'id', 'e ', 'al', 'l ', 'th', 'e ', 'we', 'ap', 'on', 'ry', ', ', 'an', 'd ', 'ad', 'de', 'd ', 'hi', 's ', 'ow', 'n ', 'ma', 'tc', 'hi', 'ng', ' c']\n",
      "\n",
      "Processing file dresden_files/094.txt\n",
      "Data size (Characters) (Document 93) 17702\n",
      "Sample string (Document 93) ['\"d', 'am', 'mi', 't.', '\" ', 'i ', 'sw', 'al', 'lo', 'we', 'd ', 'an', 'd ', 'to', 'ok', ' a', ' s', 'te', 'p ', 'ba', 'ck', ' f', 'ro', 'm ', 'th', 'e ', 'do', 'or', 'wa', 'y,', ' h', 'op', 'in', 'g ', 'th', 'e ', 'pr', 'es', 'en', 'ce', ' o', 'f ', 'my', ' m', 'ag', 'ic', ' w', 'ou', 'ld', \"n'\"]\n",
      "\n",
      "Processing file dresden_files/095.txt\n",
      "Data size (Characters) (Document 94) 13968\n",
      "Sample string (Document 94) ['\"w', 'ha', \"t'\", 's ', 'th', 'at', '?\"', ' m', 'ur', 'ph', 'y ', 'as', 'ke', 'd.', '\\n\\n', '\"b', 'lo', 'od', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'th', 'e ', 'de', 'st', 'ru', 'ct', 'io', 'n ', 'of', ' l', 'if', 'e.', ' t', 'he', ' s', 'ac', 'ri', 'fi', 'ce', ' o', 'f ', 'an', 'im', 'al', 's.', ' o']\n",
      "\n",
      "Processing file dresden_files/096.txt\n",
      "Data size (Characters) (Document 95) 15822\n",
      "Sample string (Document 95) ['he', ' m', 'ov', 'ed', ', ', 'go', 't ', 'th', 'e ', 'sa', 'ck', ', ', 'an', 'd ', 'hu', 'nk', 'er', 'ed', ' d', 'ow', 'n ', 'by', ' m', 'e ', 'so', ' t', 'ha', 't ', 'he', ' c', 'ou', 'ld', ' r', 'es', 't ', 'my', ' f', 'or', 'ea', 'rm', ' o', 've', 'r ', 'on', 'e ', 'kn', 'ee', ' a', 'nd', ' e']\n",
      "\n",
      "Processing file dresden_files/097.txt\n",
      "Data size (Characters) (Document 96) 18931\n",
      "Sample string (Document 96) ['so', ' i', ' s', 'tr', 'od', 'e ', 'to', ' m', 'ur', 'ph', \"y'\", 's ', 'bi', 'ke', '. ', '\"l', 'et', \"'s\", ' g', 'o.', '\"\\n', '\\nm', 'ur', 'ph', 'y ', 'ca', 'me', ' o', 've', 'r ', 'to', ' m', 'e,', ' e', 'ye', 's ', 'on', ' l', 'ar', 'a.', ' \"', 'sh', \"e'\", 'll', ' t', 'ur', 'n ', 'on', ' u', 's,']\n",
      "\n",
      "Processing file dresden_files/098.txt\n",
      "Data size (Characters) (Document 97) 17458\n",
      "Sample string (Document 97) ['\"b', 'ec', 'au', 'se', ' i', 'f ', 'sh', 'e ', 'ma', 'ke', 's ', 'a ', 'mi', 'st', 'ak', 'e ', 'in', ' t', 'he', ' r', 'it', 'ua', 'l ', 'th', 'er', \"e'\", 's ', 'go', 'in', 'g ', 'to', ' b', 'e ', 'so', 'me', ' b', 'ac', 'kl', 'as', 'h.', ' m', 'ay', 'be', ' i', 't ', 'wo', 'ul', 'dn', \"'t\", ' t']\n",
      "\n",
      "Processing file dresden_files/099.txt\n",
      "Data size (Characters) (Document 98) 11376\n",
      "Sample string (Document 98) ['\"t', 'ha', \"t'\", 's ', 'wo', 'rk', 'in', 'g ', 'ju', 'st', ' f', 'in', 'e,', ' d', 'oc', ',\"', ' i', ' m', 'um', 'bl', 'ed', '. ', '\"n', 'ot', ' t', 'ha', 't ', 'it', \"'s\", ' h', 'ad', ' m', 'uc', 'h ', 'us', 'e ', 'la', 'te', 'ly', '.\"', '\\n\\n', 'he', ' g', 'av', 'e ', 'me', ' a', ' b', 'ri', 'ef']\n",
      "\n",
      "Processing file dresden_files/100.txt\n",
      "Data size (Characters) (Document 99) 20500\n",
      "Sample string (Document 99) ['\\no', 'n ', 'th', 'e ', 'wh', 'ol', 'e,', ' w', \"e'\", 're', ' a', ' m', 'ur', 'de', 'ro', 'us', ' r', 'ac', 'e.', ' a', 'cc', 'or', 'di', 'ng', ' t', 'o ', 'ge', 'ne', 'si', 's,', ' i', 't ', 'to', 'ok', ' a', 's ', 'fe', 'w ', 'as', ' f', 'ou', 'r ', 'pe', 'op', 'le', ' t', 'o ', 'ma', 'ke', ' t']\n",
      "\n",
      "Processing file dresden_files/101.txt\n",
      "Data size (Characters) (Document 100) 15323\n",
      "Sample string (Document 100) ['i ', 'pi', 'ck', 'ed', ' u', 'p ', 'th', 'e ', 'ph', 'ot', 'o.', ' m', 'ur', 'ph', \"y'\", 's ', 'pi', 'ct', 'ur', 'e ', 'ha', 'd ', 'be', 'en', ' m', 'ar', 're', 'd.', ' t', 'he', ' d', 'ar', 'k ', 'en', 'er', 'gy', ' h', 'ad', ' l', 'ef', 't ', 'sc', 'or', 'ch', ' m', 'ar', 'ks', ' i', 'n ', 'th']\n",
      "\n",
      "Processing file dresden_files/102.txt\n",
      "Data size (Characters) (Document 101) 18345\n",
      "Sample string (Document 101) ['my', ' m', 'ou', 'th', ' f', 'el', 'l ', 'op', 'en', '. ', 'i ', 'sh', 'ut', ' i', 't ', 'an', 'd ', 'gl', 'ar', 'ed', ' a', 't ', 'hi', 'm.', ' \"', 'no', ' o', 'ne', ' l', 'ik', 'es', ' a', ' w', 'is', 'ea', 'ss', ', ', 'bu', 'tt', 'er', 's.', '\"\\n', '\\nh', 'e ', 'gr', 'in', 'ne', 'd.', ' \"', 'i ']\n",
      "\n",
      "Processing file dresden_files/103.txt\n",
      "Data size (Characters) (Document 102) 16584\n",
      "Sample string (Document 102) ['\"b', 'ec', 'au', 'se', ' i', \"t'\", 's ', 'te', 'rr', 'if', 'yi', 'ng', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'th', 'in', 'k ', 'ab', 'ou', 't ', 'it', '. ', 'yo', 'u ', 'fi', 'nd', ' o', 'ut', ' a', 'bo', 'ut', ' m', 'on', 'st', 'er', 's ', 'th', 'at', ' m', 'ak', 'e ', 'th', 'e ', 'cr', 'ea', 'tu']\n",
      "\n",
      "Processing file dresden_files/104.txt\n",
      "Data size (Characters) (Document 103) 16672\n",
      "Sample string (Document 103) ['\\n\"', 'oh', ',\"', ' s', 'he', ' s', 'ai', 'd,', ' a', 'nd', ' s', 'ti', 'ff', 'en', 'ed', ', ', 'be', 'co', 'mi', 'ng', ' v', 'er', 'y ', 'pa', 'le', '. ', '\"o', 'h.', ' u', 'm.', ' j', 'us', 't ', 'ta', 'ke', ' w', 'ha', 't ', 'yo', 'u ', 'wa', 'nt', '. ', 'i ', 'wo', \"n'\", 't ', 'do', ' a', 'ny']\n",
      "\n",
      "Processing file dresden_files/105.txt\n",
      "Data size (Characters) (Document 104) 13876\n",
      "Sample string (Document 104) ['\\n\"', 'i ', 'ca', \"n'\", 't ', 'te', 'll', ' y', 'ou', ' t', 'ha', 't,', '\" ', 'i ', 'sa', 'id', '.\\n', '\\nt', 'he', 're', ' w', 'as', ' a', 'n ', 'em', 'pt', 'y ', 'se', 'co', 'nd', ', ', 'an', 'd ', 'th', 'en', ' b', 'il', 'ly', \"'s\", ' v', 'oi', 'ce', ' t', 'ur', 'ne', 'd ', 'ca', 'ut', 'io', 'us']\n",
      "\n",
      "Processing file dresden_files/106.txt\n",
      "Data size (Characters) (Document 105) 13038\n",
      "Sample string (Document 105) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'lo', 'ng', ' s', 'il', 'en', 'ce', '.\\n', '\\n\"', 'oh', ',\"', ' m', 'or', 't ', 'sa', 'id', ' t', 'he', 'n.', ' \"', 'yo', 'u ', 'sh', 'ou', 'ld', ' h', 'av', 'e ', 'ju', 'st', ' s', 'ai', 'd ', 'so', '. ', \"i'\", 'll', ' a', 'sk', ' t', 'he', 'm.', '\"\\n', '\\ni', ' l']\n",
      "\n",
      "Processing file dresden_files/107.txt\n",
      "Data size (Characters) (Document 106) 15526\n",
      "Sample string (Document 106) ['\\ni', ' f', 'ou', 'nd', ' t', 'he', ' s', 'po', 't ', 'on', ' t', 'he', ' s', 'id', 'ew', 'al', 'k ', 'ou', 'ts', 'id', 'e ', 'of', ' a', ' c', 'or', 'ne', 'r ', 'ph', 'ar', 'ma', 'cy', '.\\n', '\\ni', 't ', 'wa', 's ', 'so', ' s', 'ma', 'll', ' i', ' h', 'ad', ' w', 'al', 'ke', 'd ', 'al', 'mo', 'st']\n",
      "\n",
      "Processing file dresden_files/108.txt\n",
      "Data size (Characters) (Document 107) 13854\n",
      "Sample string (Document 107) ['bu', 'tt', 'er', \"s'\", 's ', 'mo', 'ut', 'h ', 'ha', 'rd', 'en', 'ed', ' i', 'nt', 'o ', 'a ', 'fi', 'rm', ' l', 'in', 'e.', ' \"', 'th', 'at', \"'s\", ' ', 're', 'al', 'ly', ' w', 'ro', 'ng', '.\"', ' h', 'e ', 'fr', 'ow', 'ne', 'd ', 'ov', 'er', ' t', 'he', ' s', 'en', 'te', 'nc', 'e ', 'an', 'd ']\n",
      "\n",
      "Processing file dresden_files/109.txt\n",
      "Data size (Characters) (Document 108) 17697\n",
      "Sample string (Document 108) ['sh', 'ie', 'la', ' w', 'as', ' w', 'ea', 'ri', 'ng', ' t', 'he', ' s', 'am', 'e ', 'cl', 'ot', 'he', 's ', 'as', ' t', 'he', ' n', 'ig', 'ht', ' b', 'ef', 'or', 'e,', ' o', 'nl', 'y ', 'mo', 're', ' r', 'um', 'pl', 'ed', '. ', 'sh', \"e'\", 'd ', 'pu', 'll', 'ed', ' h', 'er', ' h', 'ai', 'r ', 'ba']\n",
      "\n",
      "Processing file dresden_files/110.txt\n",
      "Data size (Characters) (Document 109) 13516\n",
      "Sample string (Document 109) ['\"y', 'ou', ' l', 'oo', 'k ', 'mo', 're', ' l', 'ik', 'e ', 'a ', 'co', 'rp', 'se', ',\"', ' h', 'e ', 'an', 'sw', 'er', 'ed', '.\\n', '\\ni', ' s', 'qu', 'in', 'te', 'd ', 'at', ' h', 'im', '. ', 'he', ' s', 'at', ' c', 'al', 'ml', 'y ', 'in', ' h', 'is', ' s', 'ea', 't,', ' m', 'ir', 'ro', 'ri', 'ng']\n",
      "\n",
      "Processing file dresden_files/111.txt\n",
      "Data size (Characters) (Document 110) 14768\n",
      "Sample string (Document 110) ['\"a', ' l', 'ot', ' o', 'f ', 'pe', 'op', 'le', ' w', 'ou', 'ld', ' r', 'ea', 'ct', ' t', 'he', ' s', 'am', 'e ', 'wa', 'y,', '\" ', 'i ', 'sa', 'id', '.\\n', '\\n\"', 'a ', 'lo', 't ', 'of', ' p', 'eo', 'pl', 'e ', 'ar', 'en', \"'t\", ' m', 'ak', 'in', 'g ', 'th', 'em', 'se', 'lv', 'es', ' i', 'nt', 'o ']\n",
      "\n",
      "Processing file dresden_files/112.txt\n",
      "Data size (Characters) (Document 111) 14498\n",
      "Sample string (Document 111) ['\\n\\n', '\\ni', ' w', 'al', 'ke', 'd ', 'ba', 'ck', ' t', 'o ', 'th', 'e ', 'be', 'et', 'le', ' m', 'or', 'e ', 'sl', 'ow', 'ly', ' t', 'ha', 'n ', 'ev', 'er', ', ', 'an', 'd ', 'mo', 'us', 'e ', 'st', 'ay', 'ed', ' w', 'it', 'hi', 'n ', 'a ', 'st', 'ep', ' o', 'r ', 'tw', 'o ', 'th', 'e ', 'en', 'ti']\n",
      "\n",
      "Processing file dresden_files/113.txt\n",
      "Data size (Characters) (Document 112) 14534\n",
      "Sample string (Document 112) ['\\nl', 'iv', 'er', ' s', 'po', 'ts', ' g', 'es', 'tu', 're', 'd ', 'cu', 'rt', 'ly', ' a', 't ', 'bu', 'tt', 'er', 's.', ' i', 'n ', 're', 'pl', 'y ', 'i ', 'he', 'ld', ' u', 'p ', 'th', 'e ', 'co', 'py', ' o', 'f ', 'th', 'e ', 'nu', 'mb', 'er', 's,', ' t', 'he', 'n ', 'sl', 'ip', 'pe', 'd ', 'it']\n",
      "\n",
      "Processing file dresden_files/114.txt\n",
      "Data size (Characters) (Document 113) 13516\n",
      "Sample string (Document 113) ['\\ni', 't ', 'wa', 's ', 'an', ' o', 'dd', ' d', 're', 'am', ', ', 'be', 'ca', 'us', 'e ', 'i ', 'ha', 've', ' n', 'ev', 'er', ' i', 'n ', 'my', ' l', 'if', 'e ', 'be', 'en', ' i', 'n ', 'a ', 'ho', 't ', 'tu', 'b.', '\\n\\n', 'i ', 'op', 'en', 'ed', ' m', 'y ', 'ey', 'es', ' a', 'nd', ' l', 'oo', 'ke']\n",
      "\n",
      "Processing file dresden_files/115.txt\n",
      "Data size (Characters) (Document 114) 14210\n",
      "Sample string (Document 114) ['bu', 'tt', 'er', 's ', 'no', 'dd', 'ed', '. ', '\"f', 'ru', 'st', 'ra', 'ti', 'ng', ', ', 'hu', 'h.', '\"\\n', '\\n\"', 'ju', 'st', ' a', ' b', 'it', '.\"', '\\n\\n', '\\n\\n', '\"i', \"t'\", 's ', 'to', 'o ', 'ba', 'd ', 'yo', 'u ', 'do', \"n'\", 't ', 'ha', 've', ' a', ' p', 'ho', 'to', 'gr', 'ap', 'hi', 'c ', 'me']\n",
      "\n",
      "Processing file dresden_files/116.txt\n",
      "Data size (Characters) (Document 115) 15871\n",
      "Sample string (Document 115) ['he', 'r ', 'sm', 'il', 'e ', 'wi', 'de', 'ne', 'd.', ' \"', 'pr', 'et', 'ty', ' p', 'le', 'as', 'e?', '\"\\n', '\\n\"', 'do', \"n'\", 't ', 'pu', 'sh', ' m', 'e,', '\" ', 'i ', 'ha', 'lf', ' g', 'ro', 'wl', 'ed', ', ', 'bu', 't ', 'i ', 'do', 'ub', 't ', 'it', ' c', 'am', 'e ', 'ou', 't ', 've', 'ry', ' i']\n",
      "\n",
      "Processing file dresden_files/117.txt\n",
      "Data size (Characters) (Document 116) 20136\n",
      "Sample string (Document 116) ['th', 'e ', 'wh', 'ol', 'e ', 'la', 'yo', 'ut', ' o', 'f ', 'th', 'e ', 'pl', 'ac', 'e ', 'wa', 's ', 'me', 'an', 't ', 'to', ' d', 'is', 'pe', 'rs', 'e ', 'an', 'd ', 'di', 've', 'rt', ' d', 'an', 'ge', 'ro', 'us', ' o', 'r ', 'de', 'st', 'ru', 'ct', 'iv', 'e ', 'en', 'er', 'gi', 'es', ' t', 'ha']\n",
      "\n",
      "Processing file dresden_files/118.txt\n",
      "Data size (Characters) (Document 117) 18836\n",
      "Sample string (Document 117) ['bu', 'tt', 'er', 's ', 'bl', 'in', 'ke', 'd ', 'at', ' t', 'ho', 'ma', 's ', 'an', 'd ', 'th', 'en', ' a', 't ', 'me', '. ', '\"u', 'h.', ' w', 'ha', 't?', '\"\\n', '\\n\"', 'ha', 'rr', 'y!', '\" ', 'bo', 'b ', 'sa', 'id', ', ', 'or', 'an', 'ge', ' e', 'ye', ' l', 'ig', 'ht', 's ', 'gl', 'ow', 'in', 'g ']\n",
      "\n",
      "Processing file dresden_files/119.txt\n",
      "Data size (Characters) (Document 118) 15515\n",
      "Sample string (Document 118) ['i ', 'sh', 'iv', 'er', 'ed', '. ', 'pr', 'ob', 'ab', 'ly', ' f', 'ro', 'm ', 'th', 'e ', 'ra', 'in', ' a', 'nd', ' t', 'he', ' c', 'ol', 'd.', '\\n\\n', 'co', 'wl', ' t', 'oo', 'k ', 'a ', 'st', 'ep', ' b', 'ac', 'k ', 'fr', 'om', ' m', 'e ', 'an', 'd ', 'sa', 'id', ', ', 'vo', 'ic', 'e ', 'fi', 'rm']\n",
      "\n",
      "Processing file dresden_files/120.txt\n",
      "Data size (Characters) (Document 119) 14489\n",
      "Sample string (Document 119) ['\\ni', ' t', 'ur', 'ne', 'd ', 'sl', 'ow', 'ly', ' a', 'nd', ' f', 'ac', 'ed', ' l', 'as', 'ci', 'el', '.\\n', '\\n\\n', '\\ns', 'he', ' l', 'if', 'te', 'd ', 'he', 'r ', 'ey', 'eb', 'ro', 'ws', ', ', 'le', 'an', 'in', 'g ', 'a ', 'li', 'tt', 'le', ' f', 'or', 'wa', 'rd', ' i', 'n ', 'an', 'ti', 'ci', 'pa']\n",
      "\n",
      "Processing file dresden_files/121.txt\n",
      "Data size (Characters) (Document 120) 15638\n",
      "Sample string (Document 120) ['\\nm', 'ay', 'be', ' o', 'ne', ' w', 'ou', 'ld', ' s', 'ho', 'w ', 'up', ' t', 'o ', 'fa', 'ce', ' c', 'as', 'si', 'us', '.\\n', '\\nh', 'el', \"l'\", 's ', 'be', 'll', 's.', ' t', 'ha', 't ', 'wa', 's ', 'mi', 'gh', 'ty', ' t', 'hi', 'n.', '\\n\\n', 'bu', 't ', 'it', ' w', 'as', ' t', 'ec', 'hn', 'ic', 'al']\n",
      "\n",
      "Processing file dresden_files/122.txt\n",
      "Data size (Characters) (Document 121) 20109\n",
      "Sample string (Document 121) ['bu', 'tt', 'er', 's ', 'bl', 'in', 'ke', 'd.', ' \"', 'yo', 'u ', 'di', 'd?', ' h', 'ow', '?\"', '\\n\\n', 'i ', 'gr', 'un', 'te', 'd.', ' \"', 'ma', 'gi', 'c.', '\"\\n', '\\n\\n', '\\n\"', 'ok', 'ay', ',\"', ' h', 'e ', 'sa', 'id', '. ', '\"w', 'ha', 't ', 'di', 'd ', 'yo', 'u ', 'le', 'ar', 'n?', '\"\\n', '\\n\"', 'th']\n",
      "\n",
      "Processing file dresden_files/123.txt\n",
      "Data size (Characters) (Document 122) 19581\n",
      "Sample string (Document 122) ['mo', 'rg', 'an', ' l', 'as', 'he', 'd ', 'hi', 's ', 'fi', 'st', ' o', 'ut', ' a', 't ', 'me', ', ', 'sh', 'ou', 'ti', 'ng', ' s', 'om', 'et', 'hi', 'ng', ' t', 'ha', 't ', 'so', 'un', 'de', 'd ', 'va', 'gu', 'el', 'y ', 'gr', 'ee', 'k,', ' a', 'nd', ' t', 'he', ' v', 'er', 'y ', 'ro', 'ck', 's ']\n",
      "\n",
      "Processing file dresden_files/124.txt\n",
      "Data size (Characters) (Document 123) 8787\n",
      "Sample string (Document 123) ['i ', 'gr', 'ou', 'nd', ' m', 'y ', 'te', 'et', 'h.', ' b', 'ob', ' h', 'ad', \"n'\", 't ', 'be', 'en', ' a', 'bl', 'e ', 'to', ' h', 'el', 'p ', 'me', ', ', 'an', 'd ', 'i ', 'co', 'ul', 'dn', \"'t\", ' l', 'et', ' c', 'ow', 'l ', 'co', 'mp', 'le', 'te', ' t', 'he', ' s', 'pe', 'll', '. ', 'ev', 'en']\n",
      "\n",
      "Processing file dresden_files/125.txt\n",
      "Data size (Characters) (Document 124) 21181\n",
      "Sample string (Document 124) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\nb', 'lo', 'od', ' l', 'ea', 've', 's ', 'no', ' s', 'ta', 'in', ' o', 'n ', 'a ', 'wa', 'rd', 'en', 's', ' g', 're', 'y ', 'cl', 'oa', 'k.', ' i', ' d', 'id', 'n', 't ', 'kn', 'ow', ' t', 'ha', 't ', 'un', 'ti', 'l ', 'th', 'e ', 'da', 'y ', 'i ', 'wa', 'tc']\n",
      "\n",
      "Processing file dresden_files/126.txt\n",
      "Data size (Characters) (Document 125) 15749\n",
      "Sample string (Document 125) ['th', 'e ', 'be', 'et', 'le', ', ', 'st', 'al', 'wa', 'rt', ' c', 'ru', 'sa', 'de', 'r ', 'ag', 'ai', 'ns', 't ', 'th', 'e ', 'fo', 'rc', 'es', ' o', 'f ', 'ev', 'il', ' a', 'nd', ' a', 'lt', 'er', 'na', 'ti', 've', ' f', 'ue', 'ls', ', ', 'wa', 's ', 'st', 'il', 'l ', 'ru', 'nn', 'in', 'g,', ' t']\n",
      "\n",
      "Processing file dresden_files/127.txt\n",
      "Data size (Characters) (Document 126) 16274\n",
      "Sample string (Document 126) ['i', 'd ', 'be', 'en', ' s', 'pe', 'nd', 'in', 'g ', 'it', ' o', 'n ', 'wh', 'at', ' w', 'as', ' o', 'n ', 'my', ' w', 'or', 'kt', 'ab', 'le', '.\\n', '\\n', 'bo', 'b,', ' ', 'i ', 'sa', 'id', ', ', 'w', 'ak', 'e ', 'up', '.', '\\n\\n', 'or', 'an', 'gi', 'sh', ' f', 'la', 'me', 's ', 'ki', 'nd', 'le']\n",
      "\n",
      "Processing file dresden_files/128.txt\n",
      "Data size (Characters) (Document 127) 13501\n",
      "Sample string (Document 127) ['w', 'ha', 't ', 'br', 'ee', 'd ', 'is', ' h', 'e?', '\\n', '\\n', 'he', 's', ' a', ' w', 'es', 't ', 'hi', 'gh', 'la', 'nd', 's ', 'do', 'ga', 'sa', 'ur', 'us', ',', ' i', ' s', 'ai', 'd.', '\\n\\n', 'h', 'e', 's ', 'hu', 'ge', '.', '\\n\\n', '\\n\\n', 'i ', 'sa', 'id', ' n', 'ot', 'hi', 'ng', ', ', 'an']\n",
      "\n",
      "Processing file dresden_files/129.txt\n",
      "Data size (Characters) (Document 128) 13774\n",
      "Sample string (Document 128) ['i ', 'do', 'n', 't ', 'go', ' i', 'n ', 'bi', 'g ', 'fo', 'r ', 're', 'li', 'gi', 'on', 'b', 'ut', ' i', ' b', 'el', 'ie', 've', ' i', 'n ', 'th', 'e ', 'al', 'mi', 'gh', 'ty', '. ', 'i ', 'ha', 'd ', 'se', 'en', ' a', ' v', 'as', 't ', 'po', 'we', 'r ', 'at', ' w', 'or', 'k ', 'su', 'pp', 'or']\n",
      "\n",
      "Processing file dresden_files/130.txt\n",
      "Data size (Characters) (Document 129) 16004\n",
      "Sample string (Document 129) ['th', 'e ', 'si', 'gh', 't ', 'is', ' s', 'om', 'et', 'hi', 'ng', ' a', 'ny', 'on', 'e ', 'bo', 'rn', ' w', 'it', 'h ', 'en', 'ou', 'gh', ' t', 'al', 'en', 't ', 'ha', 's.', ' i', 't', 's ', 'an', ' e', 'xt', 'ra', ' s', 'en', 'se', ', ', 'th', 'ou', 'gh', ' w', 'he', 'n ', 'us', 'in', 'g ', 'it']\n",
      "\n",
      "Processing file dresden_files/131.txt\n",
      "Data size (Characters) (Document 130) 12550\n",
      "Sample string (Document 130) ['i', ' g', 'et', ' t', 'he', ' j', 'ob', ' d', 'on', 'e,', ' ', 'mu', 'rp', 'hy', ' s', 'ai', 'd ', 'ea', 'si', 'ly', '. ', 'i', ' c', 'an', ' h', 'el', 'p ', 'yo', 'u.', ' o', 'r ', 'i ', 'ca', 'n ', 'se', 'e ', 'to', ' i', 't ', 'th', 'at', ' t', 'he', ' p', 're', 'ss', ' k', 'no', 'ws', ' t']\n",
      "\n",
      "Processing file dresden_files/132.txt\n",
      "Data size (Characters) (Document 131) 14593\n",
      "Sample string (Document 131) ['\\n\\n', '\\ns', 'he', ' g', 'av', 'e ', 'me', ' a', ' s', 'ud', 'de', 'n,', ' s', 'wi', 'ft', ' g', 'ri', 'n.', ' ', 'wa', 'nt', 'ed', ' t', 'o ', 'ke', 'ep', ' y', 'ou', 'r ', 'mi', 'nd', ' o', 'n ', 'bu', 'si', 'ne', 'ss', '?\\n', '\\ni', ' t', 'hi', 'nk', ' i', 'l', 'l ', 'in', 'te', 'rp', 're', 't ']\n",
      "\n",
      "Processing file dresden_files/133.txt\n",
      "Data size (Characters) (Document 132) 16613\n",
      "Sample string (Document 132) ['\\n', 'yo', 'u ', 'we', 're', 'n', 't ', 'go', 'in', 'g ', 'to', ' t', 'el', 'l ', 'me', ',', ' i', ' s', 'ai', 'd.', '\\n\\n', 'he', ' r', 'ol', 'le', 'd ', 'a ', 'sh', 'ou', 'ld', 'er', ' i', 'n ', 'a ', 'sh', 'ru', 'g.', ' t', 'he', 'n ', 'he', ' n', 'od', 'de', 'd ', 'at', ' a', 'n ', 'en', 've']\n",
      "\n",
      "Processing file dresden_files/134.txt\n",
      "Data size (Characters) (Document 133) 14461\n",
      "Sample string (Document 133) ['\\n\\n', '\\ni', ' s', 'mi', 'le', 'd,', ' w', 'it', 'h ', 'te', 'et', 'h.', ' ', 'i', 'd ', 'be', ' w', 'il', 'li', 'ng', ' t', 'o ', 'fo', 'rg', 'et', ' w', 'ha', 't ', 'yo', 'u ', 'di', 'd ', 'at', ' b', 'il', 'ly', ' a', 'nd', ' g', 'eo', 'rg', 'ia', 's', ' w', 'ed', 'di', 'ng', '.', '\\n\\n', 'p']\n",
      "\n",
      "Processing file dresden_files/135.txt\n",
      "Data size (Characters) (Document 134) 12816\n",
      "Sample string (Document 134) ['\\n\\n', 'ch', 'ap', 'te', 'r\\n', '\\n\\n', 'tw', 'en', 'ty', '-t', 'wo', '\\n\\n', '\\n\\n', 'mu', 'rp', 'hy', ' a', 'nd', ' i', ' w', 'al', 'ke', 'd ', 'ar', 'ou', 'nd', ' t', 'he', ' h', 'ot', 'el', ', ', 'an', 'd ', 'as', ' w', 'e ', 'di', 'd ', 'i ', 'po', 'pp', 'ed', ' o', 'pe', 'n ', 'a ', 'fr', 'es', 'h ']\n",
      "\n",
      "Processing file dresden_files/136.txt\n",
      "Data size (Characters) (Document 135) 13503\n",
      "Sample string (Document 135) ['\\ns', 'he', ' t', 'il', 'te', 'd ', 'he', 'r ', 'he', 'ad', ' a', 'nd', ' s', 'tu', 'di', 'ed', ' m', 'e ', 'fo', 'r ', 'a ', 'mi', 'nu', 'te', '. ', 'th', 'en', ' s', 'he', ' s', 'ai', 'd,', ' ', 'wh', 'at', ' h', 'ap', 'pe', 'ns', ' w', 'he', 'n ', 'su', 'sa', 'n ', 'co', 'me', 's ', 'ba', 'ck']\n",
      "\n",
      "Processing file dresden_files/137.txt\n",
      "Data size (Characters) (Document 136) 16188\n",
      "Sample string (Document 136) ['sh', 'e ', 'fr', 'ow', 'ne', 'd.', ' ', 'wh', 'at', ' d', 'o ', 'yo', 'u ', 'me', 'an', ' b', 'y ', 'po', 'in', 'ts', ' o', 'f ', 'or', 'ig', 'in', ' a', 'nd', ' d', 'es', 'ti', 'na', 'ti', 'on', '?', '\\n\\n', '\\n\\n', 'l', 'in', 'ks', ',', ' i', ' t', 'ol', 'd ', 'he', 'r.', ' ', 'so', 'rt', ' o']\n",
      "\n",
      "Processing file dresden_files/138.txt\n",
      "Data size (Characters) (Document 137) 13166\n",
      "Sample string (Document 137) ['af', 'te', 'r ', 'i ', 'di', 'd,', ' r', 'aw', 'li', 'ns', ' o', 'bs', 'er', 've', 'd,', ' ', 'yo', 'u ', 'do', 'n', 't ', 'lo', 'ok', ' s', 'o ', 'go', 'od', '.', '\\n\\n', 'n', 'o,', ' ', 'i ', 'wh', 'is', 'pe', 're', 'd.', ' t', 'he', ' w', 'or', 'ds', ' t', 'as', 'te', 'd ', 'bi', 'tt', 'er']\n",
      "\n",
      "Processing file dresden_files/139.txt\n",
      "Data size (Characters) (Document 138) 16103\n",
      "Sample string (Document 138) ['\\n', 'i ', 'do', 'n', 't ', 'wa', 'nt', ' i', 't,', ' ', 'i ', 'sa', 'id', '.\\n', '\\n', 'wh', 'y ', 'ev', 'er', ' n', 'ot', '?', '\\n\\n', 'b', 'ec', 'au', 'se', ' a', ' f', 're', 'ak', 'in', 'g ', 'fa', 'll', 'en', ' a', 'ng', 'el', ' i', 's ', 'of', 'fe', 'ri', 'ng', ' i', 't,', ' t', 'ha', 't']\n",
      "\n",
      "Processing file dresden_files/140.txt\n",
      "Data size (Characters) (Document 139) 16140\n",
      "Sample string (Document 139) ['\\n', 'fo', 'r ', 'ga', 'in', ',', ' h', 'e ', 'sa', 'id', '. ', 'a', 'nd', ' f', 'or', ' e', 'nj', 'oy', 'me', 'nt', '. ', 'i ', 'do', 'n', 't ', 'le', 't ', 'an', 'y ', 'bu', 'ck', ' t', 'al', 'k ', 'to', ' m', 'e ', 'as', ' y', 'ou', ' d', 'id', '. ', 'si', 'nc', 'e ', 'i', 'd ', 'pl', 'an']\n",
      "\n",
      "Processing file dresden_files/141.txt\n",
      "Data size (Characters) (Document 140) 13783\n",
      "Sample string (Document 140) ['h', 'ar', 'ry', ',', ' h', 'e ', 'sa', 'id', '. ', 'th', 'en', ' h', 'is', ' e', 'ye', 's ', 'fl', 'ew', ' o', 'pe', 'n ', 'wi', 'de', ' a', 'nd', ' h', 'e ', 'st', 'ru', 'gg', 'le', 'd ', 'to', ' s', 'it', ' u', 'p.', '\\n\\n', 'm', 'ol', 'ly', '!', '\\n\\n', '\\n\\n', 'e', 'as', 'y,', ' e', 'as', 'y,']\n",
      "\n",
      "Processing file dresden_files/142.txt\n",
      "Data size (Characters) (Document 141) 16316\n",
      "Sample string (Document 141) ['h', 'ow', ' d', 'id', ' y', 'ou', ' k', 'no', 'w?', ' ', 'sh', 'e ', 'as', 'ke', 'd.', '\\n\\n', 'j', 'us', 't ', 'pu', 'tt', 'in', 'g ', 'lo', 'ts', ' o', 'f ', 'li', 'tt', 'le', ' t', 'hi', 'ng', 's ', 'to', 'ge', 'th', 'er', ',', ' i', ' s', 'ai', 'd.', ' ', 'pl', 'ea', 'se', ', ', 'ch', 'ar']\n",
      "\n",
      "Processing file dresden_files/143.txt\n",
      "Data size (Characters) (Document 142) 16553\n",
      "Sample string (Document 142) ['\\na', ' t', 'in', 'y ', 'mi', 'st', 'ak', 'e.', '\\n\\n', 'i ', 'wa', 's ', 'on', 'ly', ' h', 'um', 'an', '.\\n', '\\na', 'nd', ' i', ' h', 'ad', ' f', 'ai', 'le', 'd ', 'mo', 'll', 'y.', '\\n\\n', '\\n\\n', '\\n\\n', 'ch', 'ap', 'te', 'r\\n', '\\n\\n', 'th', 'ir', 'ty', '-f', 'ou', 'r\\n', '\\n\\n', '\\ni', ' t', 'ur', 'ne', 'd ']\n",
      "\n",
      "Processing file dresden_files/144.txt\n",
      "Data size (Characters) (Document 143) 19257\n",
      "Sample string (Document 143) ['ch', 'ap', 'te', 'r\\n', '\\n\\n', 'th', 'ir', 'ty', '-f', 'iv', 'e\\n', '\\n\\n', '\\ni', ' k', 'ic', 'ke', 'd ', 'th', 'e ', 'do', 'or', ' o', 'pe', 'n,', ' s', 'ta', 'ff', ' h', 'el', 'd ', 're', 'ad', 'y ', 'to', ' f', 'ig', 'ht', ', ', 'an', 'd ', 'sh', 'ou', 'te', 'd,', ' ', 'an', 'd ', 'i', 'm ', 'al']\n",
      "\n",
      "Processing file dresden_files/145.txt\n",
      "Data size (Characters) (Document 144) 22759\n",
      "Sample string (Document 144) ['\\n', 'co', 'ul', 'd ', 'yo', 'u ', 'do', ' t', 'ha', 't?', ' ', 'th', 'om', 'as', ' a', 'sk', 'ed', '.\\n', '\\ni', ' s', 'ho', 'ok', ' m', 'y ', 'he', 'ad', '. ', 'i', ' c', 'ou', 'ld', 'n', 't ', 'ma', 'ke', ' i', 't ', 'th', 'at', ' h', 'ot', '. ', 'no', 't ', 'at', ' t', 'he', ' h', 'ea', 'rt']\n",
      "\n",
      "Processing file dresden_files/146.txt\n",
      "Data size (Characters) (Document 145) 14951\n",
      "Sample string (Document 145) ['\\n', 'so', ' w', 'as', ' m', 'y ', 'da', 'd,', ' ', 'th', 'om', 'as', ' s', 'ai', 'd.', ' ', 'an', 'd ', 'lo', 'ok', ' h', 'ow', ' t', 'ha', 't ', 'tu', 'rn', 'ed', ' o', 'ut', '.', '\\n\\n', '\\n\\n', 't', 'he', 'n ', 'gi', 've', ' m', 'e ', 'th', 'e ', 'sw', 'or', 'd ', 'an', 'd ', 'i', 'll', ' d']\n",
      "\n",
      "Processing file dresden_files/147.txt\n",
      "Data size (Characters) (Document 146) 14776\n",
      "Sample string (Document 146) ['sh', 'e ', 'ru', 'bb', 'ed', ' h', 'er', ' h', 'an', 'ds', ' o', 'n ', 'he', 'r ', 'to', 'rn', ' s', 'ki', 'rt', 's ', 'an', 'd ', 'av', 'oi', 'de', 'd ', 'my', ' e', 'ye', 's.', ' ', 'i', ' w', 'ha', 't ', 'i ', 'pu', 't ', 'th', 'em', ' t', 'hr', 'ou', 'gh', '. ', 'i ', 'th', 'ou', 'gh', 't ']\n",
      "\n",
      "Processing file dresden_files/148.txt\n",
      "Data size (Characters) (Document 147) 14722\n",
      "Sample string (Document 147) ['\\ni', ' f', 'ou', 'nd', ' m', 'ys', 'el', 'f ', 'on', ' m', 'y ', 'fe', 'et', ' a', 's ', 'we', 'll', '. ', 'si', 'le', 'nc', 'e ', 'fe', 'll', ' h', 'ea', 'vy', ' a', 'nd', ' o', 'pp', 're', 'ss', 'iv', 'e ', 'on', ' t', 'he', ' r', 'oo', 'm.', '\\n\\n', 'm', 'om', 'ma', ',', ' m', 'ol', 'ly', ' s']\n",
      "\n",
      "Processing file dresden_files/149.txt\n",
      "Data size (Characters) (Document 148) 16658\n",
      "Sample string (Document 148) ['ch', 'ok', 'e ', 'on', ' t', 'ha', 't,', ' j', 'er', 'k,', ' i', ' t', 'ho', 'ug', 'ht', '. ', 'le', 't', 's ', 'se', 'e ', 'yo', 'u ', 'sn', 'ub', ' t', 'he', ' a', 'll', 'y ', 'wh', 'o ', 'ju', 'st', ' b', 'ai', 'le', 'd ', 'th', 'e ', 'co', 'un', 'ci', 'l ', 'ou', 't ', 'of', ' a', 'ss', '-d']\n",
      "\n",
      "Processing file dresden_files/150.txt\n",
      "Data size (Characters) (Document 149) 15054\n",
      "Sample string (Document 149) ['m', 'y ', 'mo', 'll', 'y.', ' m', 'ag', 'ic', '. ', 'is', 'n', 't ', 'th', 'at', ' s', 'or', 't ', 'of', ' t', 'hi', 'ng', ' p', 'as', 'se', 'd ', 'th', 'ro', 'ug', 'h ', 'bl', 'oo', 'dl', 'in', 'es', '?', '\\n\\n', 'u', 'su', 'al', 'ly', ',', ' i', ' s', 'ai', 'd.', ' ', 'bu', 't ', 'it', ' d']\n",
      "\n",
      "Processing file dresden_files/151.txt\n",
      "Data size (Characters) (Document 150) 8853\n",
      "Sample string (Document 150) ['i', ' h', 'op', 'e ', 'so', '. ', 'it', 's', ' g', 'oi', 'ng', ' t', 'o ', 'ta', 'ke', ' y', 'ou', ' a', ' l', 'on', 'g ', 'ti', 'me', ' t', 'o ', 'le', 'ar', 'n ', 'if', ' y', 'ou', ' c', 'an', 't', '.', '\\n\\n', 'sh', 'e ', 'sm', 'il', 'ed', ' a', ' l', 'it', 'tl', 'e.', ' ', 'wh', 'er', 'e ']\n",
      "\n",
      "Processing file dresden_files/152.txt\n",
      "Data size (Characters) (Document 151) 14934\n",
      "Sample string (Document 151) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\n\\n', '\\n\\n', '\\nm', 'an', 'y ', 'th', 'in', 'gs', ' a', 're', ' n', 'ot', ' a', 's ', 'th', 'ey', ' s', 'ee', 'm:', ' t', 'he', ' w', 'or', 'st', ' t', 'hi', 'ng', 's ', 'in', ' l', 'if', 'e ', 'ne', 've', 'r ', 'ar', 'e.', '\\n\\n', 'i ', 'pu', 'll', 'ed', ' m', 'y ']\n",
      "\n",
      "Processing file dresden_files/153.txt\n",
      "Data size (Characters) (Document 152) 13379\n",
      "Sample string (Document 152) ['i ', 'lo', 'ok', 'ed', ' a', 't ', 'mo', 'll', 'y.', ' \"', 'wh', 'o ', 'sa', 'id', ' t', 'ha', 't?', '\"\\n', '\\n\"', 'gr', 'oo', 'vy', ',\"', ' b', 'ut', 'te', 'rs', ' s', 'ai', 'd.', ' h', 'e ', 'wa', 'lk', 'ed', ' b', 'ac', 'k ', 'ov', 'er', ' t', 'o ', 'me', ' a', 'nd', ' o', 'ff', 'er', 'ed', ' m']\n",
      "\n",
      "Processing file dresden_files/154.txt\n",
      "Data size (Characters) (Document 153) 15521\n",
      "Sample string (Document 153) ['it', ' w', 'as', ' l', 'at', 'e ', 'af', 'te', 'rn', 'oo', 'n ', 'an', 'd ', 'th', 'e ', 'pl', 'ac', 'e ', 'wa', 's ', 'bu', 'si', 'er', ' t', 'ha', 'n ', 'it', ' s', 'ho', 'ul', 'd ', 'ha', 've', ' b', 'ee', 'n.', ' o', 'f ', 'th', 'e ', 'th', 'ir', 'te', 'en', ' t', 'ab', 'le', 's,', ' o', 'nl']\n",
      "\n",
      "Processing file dresden_files/155.txt\n",
      "Data size (Characters) (Document 154) 18507\n",
      "Sample string (Document 154) ['i ', 'sh', 'oo', 'k ', 'my', ' h', 'ea', 'd.', ' \"', 'an', 'd ', 'yo', 'u ', 'th', 'ou', 'gh', 't ', 'th', 'e ', 'gu', 'y ', 'in', ' t', 'he', ' c', 'lo', 'ak', ' w', 'as', ' m', 'e?', '\"\\n', '\\n\"', 'ho', 'w ', 'ma', 'ny', ' t', 'al', 'l,', ' g', 're', 'y-', 'cl', 'oa', 'ke', 'd ', 'me', 'n ', 'mo']\n",
      "\n",
      "Processing file dresden_files/156.txt\n",
      "Data size (Characters) (Document 155) 16138\n",
      "Sample string (Document 155) ['ev', 'er', 'yb', 'od', \"y'\", 's ', 'a ', 'cr', 'it', 'ic', '.\\n', '\\n\"', 'ok', 'ay', ', ', 'la', 'sh', ',\"', ' i', ' s', 'ai', 'd,', ' a', 'nd', ' k', 'ep', 't ', 'pl', 'ay', 'in', 'g.', ' \"', 'le', \"t'\", 's ', 'ta', 'lk', '.\"', '\\n\\n', '\"l', 'as', 'h?', '\" ', 'sa', 'id', ' a', ' q', 'ui', 'et', ' w']\n",
      "\n",
      "Processing file dresden_files/157.txt\n",
      "Data size (Characters) (Document 156) 18259\n",
      "Sample string (Document 156) ['\"w', 'hi', 'ch', ' e', 'xp', 'la', 'in', 's ', 'wh', 'y ', 'i ', 'ha', 'dn', \"'t\", ' h', 'ea', 'rd', ' a', 'ny', 'th', 'in', 'g ', 'ab', 'ou', 't ', 'it', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'if', ' e', 've', 'ry', 'on', 'e ', 'th', 'in', 'ks', ' t', 'he', ' w', 'ar', 'de', 'ns', ' a', 're', ' r']\n",
      "\n",
      "Processing file dresden_files/158.txt\n",
      "Data size (Characters) (Document 157) 15338\n",
      "Sample string (Document 157) ['\"g', 'ot', 'ch', 'a,', '\" ', 'i ', 'gr', 'ow', 'le', 'd ', 'un', 'de', 'r ', 'my', ' b', 're', 'at', 'h,', ' a', 'nd', ' d', 'ri', 'ft', 'ed', ' c', 'lo', 'se', ' t', 'o ', 'th', 'e ', 'ca', 'r,', ' h', 'ov', 'er', 'in', 'g ', 'ri', 'gh', 't ', 'ov', 'er', ' i', 'ts', ' r', 'ea', 'r ', 'bu', 'mp']\n",
      "\n",
      "Processing file dresden_files/159.txt\n",
      "Data size (Characters) (Document 158) 13280\n",
      "Sample string (Document 158) ['\"n', 'ot', ' y', 'ou', 'r ', 'fa', 'ul', 't,', '\" ', 'mu', 'rp', 'hy', ' s', 'ai', 'd,', ' \"', 'i ', 'kn', 'ow', '. ', 'bu', 't ', 'it', \"'s\", ' g', 'oi', 'ng', ' t', 'o ', 'lo', 'ok', ' a', 'wf', 'ul', 'ly', ' o', 'dd', '. ', 'my', ' c', 'ar', ' g', 'et', 's ', 'fi', 're', 'bo', 'mb', 'ed', '. ']\n",
      "\n",
      "Processing file dresden_files/160.txt\n",
      "Data size (Characters) (Document 159) 14389\n",
      "Sample string (Document 159) ['el', 'ai', 'ne', ' s', 'ho', 'ok', ' h', 'er', ' h', 'ea', 'd ', 'fi', 'rm', 'ly', '. ', '\"t', 'he', ' k', 'il', 'le', 'r ', 'kn', 'ow', 's ', 'wh', 'er', 'e ', 'yo', 'u ', 'ea', 'ch', ' l', 'iv', 'e.', ' h', 'e ', 'do', 'es', \"n'\", 't ', 'kn', 'ow', ' a', 'bo', 'ut', ' t', 'hi', 's ', 'pl', 'ac']\n",
      "\n",
      "Processing file dresden_files/161.txt\n",
      "Data size (Characters) (Document 160) 21009\n",
      "Sample string (Document 160) ['i ', 'co', 'ul', 'dn', \"'t\", ' d', 'o ', 'th', 'at', ', ', 'be', 'ca', 'us', 'e ', \"i'\", 'd ', 're', 'pl', 'ac', 'ed', ' t', 'he', ' s', 'in', 'gl', 'e ', 'si', 'lv', 'er', ' r', 'in', 'g ', 'wi', 'th', ' t', 'hr', 'ee', ' c', 'ir', 'cl', 'es', ' o', 'f ', 'si', 'lv', 'er', ' f', 'us', 'ed', ' i']\n",
      "\n",
      "Processing file dresden_files/162.txt\n",
      "Data size (Characters) (Document 161) 16343\n",
      "Sample string (Document 161) ['\"o', 'ur', ' l', 'iv', 'es', ',\"', ' h', 'is', 'se', 'd ', 'th', 'e ', 'wo', 'un', 'de', 'd ', 'gh', 'ou', 'l.', ' \"', 'pr', 'om', 'is', 'e ', 'us', ' o', 'ur', ' l', 'iv', 'es', ' a', 'nd', ' f', 're', 'ed', 'om', ', ', 'gr', 'ea', 't ', 'on', 'e.', ' g', 'iv', 'e ', 'us', ' y', 'ou', 'r ', 'wo']\n",
      "\n",
      "Processing file dresden_files/163.txt\n",
      "Data size (Characters) (Document 162) 14651\n",
      "Sample string (Document 162) ['\\n\\n', '\"i', \"'v\", 'e ', 'wo', 'rk', 'ed', ' w', 'it', 'h ', 'hi', 'm ', 'be', 'fo', 're', ',\"', ' i', ' s', 'ai', 'd.', ' \"', 'he', \"'s\", ' d', 'if', 'fe', 're', 'nt', '.\"', '\\n\\n', '\"h', 'ow', '? ', 'a ', 'lo', 't ', 'of', ' v', 'am', 'pi', 're', 's ', 'fe', 'el', ' r', 'em', 'or', 'se', ' a', 'bo']\n",
      "\n",
      "Processing file dresden_files/164.txt\n",
      "Data size (Characters) (Document 163) 15037\n",
      "Sample string (Document 163) ['\"m', 'ar', 'co', 'ne', ' d', 'oe', 'sn', \"'t\", ' l', 'ik', 'e ', 'an', 'yb', 'od', 'y,', '\" ', 'mu', 'rp', 'hy', ' r', 'ep', 'li', 'ed', '. ', '\"b', 'ut', ' h', 'e ', 're', 'sp', 'ec', 'ts', ' y', 'ou', '.\"', '\\n\\n', '\"l', 'ik', 'e ', 'th', 'at', ' s', 'ay', 's ', 'mu', 'ch', ' f', 'or', ' m', 'e.']\n",
      "\n",
      "Processing file dresden_files/165.txt\n",
      "Data size (Characters) (Document 164) 16118\n",
      "Sample string (Document 164) ['an', 'd ', 'he', ' w', 'as', ' s', 'ki', 'nn', 'ie', 'r.', ' n', 'ot', ' a', ' l', 'ot', ' s', 'ki', 'nn', 'ie', 'r ', 'or', ' a', 'ny', 'th', 'in', 'g,', ' b', 'ut', ' i', 't ', 'su', 'rp', 'ri', 'se', 'd ', 'me', ' h', 'ow', ' m', 'uc', 'h ', 'yo', 'un', 'ge', 'r ', 'it', ' a', 'nd', ' t', 'he']\n",
      "\n",
      "Processing file dresden_files/166.txt\n",
      "Data size (Characters) (Document 165) 14083\n",
      "Sample string (Document 165) ['el', 'ai', 'ne', '! ', 'i ', 'th', 'un', 'de', 're', 'd.', ' y', 'ou', ' a', 're', ' u', 'nd', 'er', ' a', ' p', 'sy', 'ch', 'ic', ' a', 'tt', 'ac', 'k!', ' p', 'ri', 'sc', 'il', 'la', ' i', 's ', 'th', 'e ', 'sk', 'av', 'is', '!\\n', '\\ne', 'la', 'in', \"e'\", 's ', 'ey', 'es', ' w', 'id', 'en', 'ed']\n",
      "\n",
      "Processing file dresden_files/167.txt\n",
      "Data size (Characters) (Document 166) 16166\n",
      "Sample string (Document 166) ['\"w', 'ho', '?\"', '\\n\\n', '\"y', 'ou', 'r ', 'pu', 'pi', 'l,', ' o', 'f ', 'co', 'ur', 'se', ',\"', ' l', 'as', 'ci', 'el', ' s', 'ai', 'd.', ' \"', 'do', ' y', 'ou', ' r', 'ea', 'll', 'y ', 'th', 'in', 'k ', 'sh', 'e ', 'ca', 'n ', 'ch', 'an', 'ge', '? ', 'do', ' y', 'ou', ' t', 'hi', 'nk', ' s', 'he']\n",
      "\n",
      "Processing file dresden_files/168.txt\n",
      "Data size (Characters) (Document 167) 15747\n",
      "Sample string (Document 167) ['\"o', 'ka', 'y,', '\" ', 'sh', 'e ', 'sa', 'id', '. ', 'sh', 'e ', 'lo', 'ok', 'ed', ' a', ' l', 'it', 'tl', 'e ', 'pa', 'le', '.\\n', '\\ni', ' p', 'ul', 'le', 'd ', 'a ', 'si', 'lv', 'er', ' c', 'yl', 'in', 'de', 'r ', 'ou', 't ', 'of', ' m', 'y ', 'po', 'ck', 'et', '. ', '\"t', 'hi', 's ', 'is', ' a']\n",
      "\n",
      "Processing file dresden_files/169.txt\n",
      "Data size (Characters) (Document 168) 21225\n",
      "Sample string (Document 168) ['mo', 'us', 'e ', 'mi', 'gh', 't ', 'ha', 've', ' b', 'ee', 'n ', 'th', 'e ', 'on', 'e ', 'to', ' d', 'o ', 'th', 'e ', 'ac', 'tu', 'al', ' k', 'il', 'li', 'ng', ', ', 'bu', 't ', 'th', 'e ', 'ac', 'co', 'rd', 's ', 're', 'ga', 'rd', 'ed', ' h', 'im', ' a', 's ', 'a ', 'me', 're', ' w', 'ea', 'po']\n",
      "\n",
      "Processing file dresden_files/170.txt\n",
      "Data size (Characters) (Document 169) 21995\n",
      "Sample string (Document 169) ['br', 'ok', 'en', ', ', 'bl', 'ee', 'di', 'ng', ' w', 're', 'ck', 'ag', 'e ', 'tu', 'mb', 'le', 'd ', 'li', 'mp', 'ly', ' d', 'ow', 'n.', '\\n\\n', '\"a', 'nd', ' t', 'he', ' w', 'iz', 'ar', 'ds', ',\"', ' i', ' s', 'na', 'rl', 'ed', ', ', '\"p', 'ic', 'k ', 'up', ' t', 'he', ' s', 'pa', 're', '.\"', '\\n\\n']\n",
      "\n",
      "Processing file dresden_files/171.txt\n",
      "Data size (Characters) (Document 170) 17067\n",
      "Sample string (Document 170) ['th', 'e ', 'gh', 'ou', 'l ', 're', 'co', 've', 're', 'd ', 'an', 'd ', 'th', 'ra', 'sh', 'ed', ' t', 'ow', 'ar', 'd ', 'me', ', ', 'ev', 'en', ' a', 's ', 'i ', 'sa', 'w ', 'th', 'om', 'as', ' a', 'pp', 'ea', 'r ', 'fr', 'om', ' t', 'he', ' r', 'an', 'ks', ' o', 'f ', 'va', 'mp', 'ir', 'es', ' a']\n",
      "\n",
      "Processing file dresden_files/172.txt\n",
      "Data size (Characters) (Document 171) 15675\n",
      "Sample string (Document 171) ['\\n\\n', 'th', 'e ', 'on', 'ly', ' l', 'ig', 'ht', ' w', 'as', ' t', 'he', ' d', 'im', ' s', 'ca', 'rl', 'et', ' g', 'lo', 'w ', 'fr', 'om', ' c', 'ow', \"l'\", 's ', 'ga', 'te', ', ', 'an', 'd ', 'ev', 'er', 'yt', 'hi', 'ng', ' h', 'ad', ' b', 'ec', 'om', 'e ', 'bl', 'oo', 'd ', 'an', 'd ', 'sh', 'ad']\n",
      "\n",
      "Processing file dresden_files/173.txt\n",
      "Data size (Characters) (Document 172) 14743\n",
      "Sample string (Document 172) ['\"m', 'y ', 'bo', 'yi', 'sh', ' c', 'ha', 'rm', '. ', 'ca', 'n ', 'yo', 'u ', 'ge', 't ', 'me', ' c', 'on', 'ta', 'ct', ' i', 'nf', 'or', 'ma', 'ti', 'on', ' f', 'or', ' t', 'he', ' v', 'ic', 'ti', 'ms', \"' \", 'fa', 'mi', 'li', 'es', '? ', \"i'\", 'll', ' g', 'et', ' s', 'om', 'eb', 'od', 'y ', 'to']\n",
      "\n",
      "Processing file dresden_files/174.txt\n",
      "Data size (Characters) (Document 173) 2139\n",
      "Sample string (Document 173) ['\"t', 'hi', 's ', 'is', ' a', ' t', 're', 'nd', 'y,', ' u', 'pp', 'er', '-c', 'la', 'ss', ' b', 'ou', 'ti', 'qu', 'e,', ' h', 'ar', 'ry', '. ', 'no', ' o', 'ne', ' e', 'xp', 'ec', 'ts', ' a', ' m', 'an', ' w', 'it', 'h ', 'a ', 'pl', 'ac', 'e ', 'li', 'ke', ' t', 'hi', 's ', 'to', ' b', 'e ', 'st']\n",
      "\n",
      "Processing file dresden_files/175.txt\n",
      "Data size (Characters) (Document 174) 17819\n",
      "Sample string (Document 174) ['\\nc', 'ha', 'pt', 'er', ' o', 'ne', '\\n\\n', '\\nw', ' i', 'nt', 'er', ' c', 'am', 'e ', 'ea', 'rl', 'y ', 'th', 'at', ' y', 'ea', 'r;', ' i', 't ', 'sh', 'ou', 'ld', ' h', 'av', 'e ', 'be', 'en', ' a', ' t', 'ip', 'o', 'ff', '.\\n', '\\na', ' s', 'no', 'wb', 'al', 'l ', 'so', 'ar', 'ed', ' t', 'hr', 'ou']\n",
      "\n",
      "Processing file dresden_files/176.txt\n",
      "Data size (Characters) (Document 175) 16453\n",
      "Sample string (Document 175) ['\\n', 'no', ' o', 'ne', ' g', 'ot', ' h', 'ur', 't,', ' ', 'i ', 'sa', 'id', '. ', 't', 'ha', 't', 's ', 'a ', 'pl', 'us', '.', '\\n\\n', 'mu', 'rp', 'hy', ' g', 'av', 'e ', 'me', ' a', ' c', 'ry', 'pt', 'ic', ' l', 'oo', 'k.', ' ', 'yo', 'u ', 'wi', 'll', 'in', 'g ', 'to', ' w', 'or', 'k ', 'of']\n",
      "\n",
      "Processing file dresden_files/177.txt\n",
      "Data size (Characters) (Document 176) 23608\n",
      "Sample string (Document 176) ['in', ' t', 'he', ' s', 'il', 'en', 'ce', ' o', 'f ', 'th', 'e ', 'sn', 'ow', 'y ', 'ev', 'en', 'in', 'g ', 'i ', 'he', 'ar', 'd ', 'a ', 'so', 'un', 'd ', 'co', 'mi', 'ng', ' f', 'ro', 'm ', 'th', 'e ', 'fa', 'r ', 'en', 'd ', 'of', ' t', 'he', ' a', 'll', 'ey', '.\\n', '\\nf', 'oo', 'ts', 'te', 'ps']\n",
      "\n",
      "Processing file dresden_files/178.txt\n",
      "Data size (Characters) (Document 177) 13109\n",
      "Sample string (Document 177) ['so', 'me', 'on', 'e ', 'ha', 'd ', 'sn', 'at', 'ch', 'ed', ' j', 'oh', 'n ', 'ma', 'rc', 'on', 'e,', ' a', 'nd', ' i', ' h', 'ad', ' t', 'o ', 'fi', 'nd', ' h', 'im', ' a', 'nd', ' p', 'ro', 'te', 'ct', ' h', 'im', '. ', 'i ', 'ha', 'd ', 'a ', 'fe', 'el', 'in', 'g ', 'th', 'at', ' m', 'ur', 'ph']\n",
      "\n",
      "Processing file dresden_files/179.txt\n",
      "Data size (Characters) (Document 178) 15679\n",
      "Sample string (Document 178) ['\\n', 'wo', 'n', 't ', 'th', 'ey', ' f', 'ig', 'ur', 'e ', 'it', ' o', 'ut', '?', '\\n\\n', 's', 'oo', 'ne', 'r ', 'or', ' l', 'at', 'er', ',', ' i', ' a', 'dm', 'it', 'te', 'd,', ' ', 'bu', 't ', 'it', ' s', 'ho', 'ul', 'd ', 'bu', 'y ', 'us', ' a', ' l', 'it', 'tl', 'e ', 'ti', 'me', ' t', 'o ']\n",
      "\n",
      "Processing file dresden_files/180.txt\n",
      "Data size (Characters) (Document 179) 14434\n",
      "Sample string (Document 179) ['\\ni', ' d', 'id', ', ', 'an', 'd ', 'th', 'e ', 'sh', 'ot', 'gu', 'n ', 'wa', 's ', 'tr', 'ai', 'ne', 'd ', 'on', ' m', 'y ', 'br', 'ot', 'he', 'r.', ' ', 'yo', 'u,', ' v', 'am', 'pi', 're', '. ', 'sw', 'or', 'd ', 'do', 'wn', '. ', 'fi', 'ng', 'er', 's ', 'la', 'ce', 'd ', 'be', 'hi', 'nd', ' y']\n",
      "\n",
      "Processing file dresden_files/181.txt\n",
      "Data size (Characters) (Document 180) 13280\n",
      "Sample string (Document 180) ['\\nt', 'ho', 'ma', 's ', 'tw', 'it', 'ch', 'ed', ' i', 'n ', 'su', 'rp', 'ri', 'se', ', ', 'an', 'd ', 'th', 'e ', 'hu', 'mm', 'er', ' w', 'en', 't ', 'in', 'to', ' a', ' s', 'lo', 'w ', 'sl', 'id', 'e ', 'on', ' t', 'he', ' s', 'no', 'w.', ' h', 'e ', 'tu', 'rn', 'ed', ' i', 'nt', 'o ', 'it', ' a']\n",
      "\n",
      "Processing file dresden_files/182.txt\n",
      "Data size (Characters) (Document 181) 15482\n",
      "Sample string (Document 181) ['he', ' g', 'ru', 'nt', 'ed', '. ', 'f', 'or', ' m', 'os', 'co', 'w ', 'i ', 'wa', 's ', 've', 'ry', ', ', 've', 'ry', ' o', 'dd', '. ', 'if', ' i', ' w', 'en', 't ', 'ou', 't ', 'to', ' a', 'ny', ' s', 'ma', 'll', 'er', ' t', 'ow', 'ns', ' w', 'he', 'n ', 'i ', 'wa', 's ', 'gr', 'ow', 'in', 'g ']\n",
      "\n",
      "Processing file dresden_files/183.txt\n",
      "Data size (Characters) (Document 182) 14953\n",
      "Sample string (Document 182) ['l', 'ad', 'y ', 'ha', 's ', 'a ', 'po', 'in', 't,', ' t', 'in', 'y,', ' ', 'i ', 'dr', 'aw', 'le', 'd.', ' ', 'th', 'er', 'e', 's ', 'no', 'th', 'in', 'g ', 'to', ' b', 'e ', 'ga', 'in', 'ed', ' h', 'er', 'e ', 'bu', 't ', 'tr', 'ou', 'bl', 'e,', ' a', 'nd', ' n', 'ot', 'hi', 'ng', ' t', 'o ']\n",
      "\n",
      "Processing file dresden_files/184.txt\n",
      "Data size (Characters) (Document 183) 13604\n",
      "Sample string (Document 183) ['i ', 'mo', 'st', ' l', 'ik', 'el', 'y ', 'di', 'dn', 't', ' h', 'av', 'e ', 'ti', 'me', ' t', 'o ', 'gr', 'il', 'l ', 'ev', 'en', ' o', 'ne', ' o', 'f ', 'th', 'e ', 'gu', 'nm', 'en', '. ', 'to', 're', 'll', 'i', 's ', 'go', 'on', ' w', 'as', ' h', 'ur', 't ', 'an', 'd ', 'pr', 'ob', 'ab', 'ly']\n",
      "\n",
      "Processing file dresden_files/185.txt\n",
      "Data size (Characters) (Document 184) 19768\n",
      "Sample string (Document 184) ['he', 'nd', 'ri', 'ck', 's ', 'wa', 'sn', 't', ' a', 's ', 'ta', 'll', ' a', 's ', 'me', ', ', 'wh', 'ic', 'h ', 'cu', 't ', 'do', 'wn', ' o', 'n ', 'th', 'e ', 'in', 'ti', 'mi', 'da', 'ti', 'on', ' f', 'ac', 'to', 'r.', ' o', 'n ', 'th', 'e ', 'ot', 'he', 'r ', 'ha', 'nd', ', ', 'he', ' h', 'ad']\n",
      "\n",
      "Processing file dresden_files/186.txt\n",
      "Data size (Characters) (Document 185) 20892\n",
      "Sample string (Document 185) ['\\n\\n', '\\nm', 'an', 'y ', 'mo', 're', 'd', 'oz', 'en', 's ', 'th', 'at', ' i', ' c', 'ou', 'ld', ' s', 'ee', 'h', 'ad', ' f', 'al', 'le', 'n ', 'to', 'o ', 'fa', 'r ', 'aw', 'ay', ' f', 'or', ' m', 'ic', 'ha', 'el', ' t', 'o ', 'ha', 've', ' r', 'ea', 'ch', 'ed', ' t', 'he', 'm ', 'wi', 'th', ' t']\n",
      "\n",
      "Processing file dresden_files/187.txt\n",
      "Data size (Characters) (Document 186) 16767\n",
      "Sample string (Document 186) ['\\n', 'we', 'll', ',', ' i', ' s', 'ai', 'd,', ' ', 'no', '.', '\\n\\n', 'w', 'hi', 'ch', ', ', 'i ', 'ta', 'ke', ' i', 't,', ' e', 'xp', 'la', 'in', 's ', 'wh', 'at', ' y', 'ou', ' w', 'er', 'e ', 'do', 'in', 'g ', 'at', ' t', 'he', ' s', 'ta', 'ti', 'on', '.', '\\n\\n', '\\n\\n', 'w', 'el', 'l,', ' ']\n",
      "\n",
      "Processing file dresden_files/188.txt\n",
      "Data size (Characters) (Document 187) 20265\n",
      "Sample string (Document 187) ['\\ni', ' s', 'to', 'pp', 'ed', ' m', 'ys', 'el', 'f ', 'fr', 'om', ' t', 'ak', 'in', 'g ', 'an', ' i', 'nv', 'ol', 'un', 'ta', 'ry', ' s', 'te', 'p ', 'ba', 'ck', ', ', 'bu', 't ', 'ju', 'st', ' b', 'ar', 'el', 'y.', '\\n\\n', 'i', 't ', 'is', ' a', ' m', 'et', 'ap', 'ho', 'r,', ' ', 'he', ' s', 'ai']\n",
      "\n",
      "Processing file dresden_files/189.txt\n",
      "Data size (Characters) (Document 188) 18645\n",
      "Sample string (Document 188) ['i ', 'fo', 'un', 'd ', 'my', 'se', 'lf', ' g', 'ri', 'nn', 'in', 'g.', ' f', 'in', 'e.', ' i', 'f ', 'th', 'at', ' w', 'as', ' t', 'he', ' g', 'am', 'e,', ' i', ' c', 'ou', 'ld', ' p', 'la', 'y ', 'to', 'o.', ' r', 'ea', 'dy', ' o', 'r ', 'no', 't,', ' h', 'er', 'e ', 'i ', 'co', 'me', '.\\n', '\\ni']\n",
      "\n",
      "Processing file dresden_files/190.txt\n",
      "Data size (Characters) (Document 189) 13071\n",
      "Sample string (Document 189) ['\\n', 'wh', 'er', 'e', 's ', 'cu', 'jo', '?', ' i', ' a', 'sk', 'ed', '.\\n', '\\ng', 'ar', 'd ', 'ga', 've', ' m', 'e ', 'a ', 'bl', 'an', 'k ', 'lo', 'ok', '.\\n', '\\n', 'he', 'nd', 'ri', 'ck', 's.', '\\n', '\\n', 'ah', ',', ' s', 'he', ' s', 'ai', 'd.', ' ', 'lo', 'ok', 'ou', 't.', ' h', 'e', 'll']\n",
      "\n",
      "Processing file dresden_files/191.txt\n",
      "Data size (Characters) (Document 190) 14483\n",
      "Sample string (Document 190) ['\\n', 'i ', 'wo', 'ul', 'dn', 't', ' h', 'av', 'e ', 'ca', 'll', 'ed', ', ', 'ot', 'he', 'rw', 'is', 'e,', ' ', 'ni', 'co', 'de', 'mu', 's ', 're', 'pl', 'ie', 'd.', ' ', 'bu', 't ', 'le', 't', 's ', 'ke', 'ep', ' t', 'hi', 's ', 'a ', 'bi', 't ', 'si', 'mp', 'le', 'r,', ' s', 'ha', 'll', ' w']\n",
      "\n",
      "Processing file dresden_files/192.txt\n",
      "Data size (Characters) (Document 191) 13567\n",
      "Sample string (Document 191) ['\\na', 'ft', 'er', ' t', 'ha', 't ', 'th', 'in', 'g ', 'wi', 'th', ' p', 'ut', 'ti', 'ng', ' y', 'ou', 'r ', 'sw', 'or', 'd ', 'to', ' m', 'y ', 'th', 'ro', 'at', ' a', 'nd', ' a', 'll', '.', '\\n\\n', 'mi', 'ch', 'ae', 'l ', 'le', 't ', 'hi', 's ', 'he', 'ad', ' f', 'al', 'l ', 'ba', 'ck', ' a', 'nd']\n",
      "\n",
      "Processing file dresden_files/193.txt\n",
      "Data size (Characters) (Document 192) 21881\n",
      "Sample string (Document 192) ['\\nh', 'e ', 'tu', 'rn', 'ed', ' t', 'he', ' h', 'ea', 'te', 'r ', 'up', ' a', ' l', 'it', 'tl', 'e ', 'wh', 'il', 'e ', 'i ', 'st', 'om', 'pe', 'd ', 'sn', 'ow', ' o', 'ff', ' o', 'f ', 'my', ' s', 'ho', 'es', '.\\n', '\\n', 'so', ',', ' h', 'e ', 'sa', 'id', ', ', 'st', 'ar', 'ti', 'ng', ' d', 'ow']\n",
      "\n",
      "Processing file dresden_files/194.txt\n",
      "Data size (Characters) (Document 193) 22738\n",
      "Sample string (Document 193) ['\\nh', 'is', ' e', 'ye', 's ', 'na', 'rr', 'ow', 'ed', ' a', 's ', 'he', ' s', 'ta', 're', 'd ', 'at', ' t', 'he', ' b', 'la', 'de', ', ', 'an', 'd ', 'i ', 'no', 'ti', 'ce', 'd,', ' f', 'or', ' t', 'he', ' f', 'ir', 'st', ' t', 'im', 'e,', ' t', 'ha', 't ', 'he', ' w', 'or', 'e ', 'a ', 'sw', 'or']\n",
      "\n",
      "Processing file dresden_files/195.txt\n",
      "Data size (Characters) (Document 194) 17716\n",
      "Sample string (Document 194) ['\\ni', 'n ', 'th', 'at', ' o', 'rd', 'er', '?', '\\n\\n', 'th', 'e ', 'gr', 'uf', 'f ', 'mi', 'rr', 'or', 'ed', ' m', 'y ', 'ge', 'st', 'ur', 'e,', ' h', 'is', ' e', 'ye', 's ', 'qu', 'es', 'ti', 'on', 'in', 'g.', '\\n\\n', 'i ', 'fu', 'mb', 'le', 'd ', 'in', ' m', 'y ', 'po', 'ck', 'et', ' a', 'nd', ' c']\n",
      "\n",
      "Processing file dresden_files/196.txt\n",
      "Data size (Characters) (Document 195) 16736\n",
      "Sample string (Document 195) ['\\nh', 'er', 'oe', 's ', 'ke', 'ep', ' t', 'he', 'ir', ' p', 'ro', 'mi', 'se', 's.', '\\n\\n', 'm', 'ol', 'ly', ',', ' i', ' s', 'ai', 'd ', 'qu', 'ie', 'tl', 'y.', ' ', 'i', 'm ', 'so', 'rr', 'y.', '\\n', '\\ns', 'he', ' l', 'oo', 'ke', 'd ', 'up', ' a', 't ', 'me', ', ', 'an', 'd ', 'he', 'r ', 'li']\n",
      "\n",
      "Processing file dresden_files/197.txt\n",
      "Data size (Characters) (Document 196) 7479\n",
      "Sample string (Document 196) ['\\ni', ' l', 'oo', 'ke', 'd ', 'in', 'te', 'nt', 'ly', ' a', 't ', 'he', 'r,', ' f', 'ro', 'wn', 'in', 'g.', ' ', 'no', ', ', 'mu', 'rp', 'h.', ' y', 'ou', ' a', 're', 'n', 't.', '\\n', '\\n', 'th', 'er', 'e ', 'ar', 'e ', 'ba', 'd ', 'th', 'in', 'gs', ' h', 'ap', 'pe', 'ni', 'ng', ',', ' s', 'he']\n",
      "\n",
      "Processing file dresden_files/198.txt\n",
      "Data size (Characters) (Document 197) 15443\n",
      "Sample string (Document 197) ['\\nc', 'ha', 'pt', 'er', ' o', 'ne', '\\n\\n', '\\nt', 'he', ' s', 'um', 'me', 'r ', 'su', 'n ', 'wa', 's ', 'bu', 'sy', ' b', 'ro', 'il', 'in', 'g ', 'th', 'e ', 'as', 'ph', 'al', 't ', 'fr', 'om', ' c', 'hi', 'ca', 'go', 's', ' s', 'tr', 'ee', 'ts', ', ', 'th', 'e ', 'ag', 'on', 'y ', 'in', ' m', 'y ']\n",
      "\n",
      "Processing file dresden_files/199.txt\n",
      "Data size (Characters) (Document 198) 19133\n",
      "Sample string (Document 198) ['i ', 'si', 'gh', 'ed', '. ', 'l', 'oo', 'k,', ' i', 'm', ' n', 'ot', ' a', 'sk', 'in', 'g ', 'he', 'r ', 'to', ' d', 'ea', 'ct', 'iv', 'at', 'e ', 'th', 'e ', 'tr', 'ac', 'to', 'r ', 'be', 'am', ', ', 're', 'sc', 'ue', ' t', 'he', ' p', 'ri', 'nc', 'es', 's,', ' a', 'nd', ' e', 'sc', 'ap', 'e ']\n",
      "\n",
      "Processing file dresden_files/200.txt\n",
      "Data size (Characters) (Document 199) 16204\n",
      "Sample string (Document 199) ['\\ni', ' d', 'id', 'n', 't ', 'sa', 'y ', 'an', 'yt', 'hi', 'ng', ' i', 'n ', 're', 'pl', 'y.', ' i', 'n ', 'a ', 'mi', 'nu', 'te', ', ', 'he', 'r ', 'ca', 'r ', 'cr', 'un', 'ch', 'ed', ' t', 'o ', 'a ', 'st', 'op', ' i', 'n ', 'th', 'e ', 'li', 'tt', 'le', ' g', 'ra', 've', 'l ', 'lo', 't ', 'ou']\n",
      "\n",
      "Processing file dresden_files/201.txt\n",
      "Data size (Characters) (Document 200) 14824\n",
      "Sample string (Document 200) ['\\n\\n', 'th', 'om', 'as', ' s', 'ta', 're', 'd ', 'ha', 'rd', ' a', 't ', 'th', 'e ', 'fl', 'oo', 'r ', 'be', 'lo', 'w,', ' a', 'nd', ' t', 'he', 'n ', 'no', 'dd', 'ed', ' o', 'nc', 'e,', ' a', 's ', 'if', ' i', 'n ', 're', 'co', 'gn', 'it', 'io', 'n.', ' ', 'ha', 'rr', 'y,', ' ', 'he', ' s', 'ai']\n",
      "\n",
      "Processing file dresden_files/202.txt\n",
      "Data size (Characters) (Document 201) 16847\n",
      "Sample string (Document 201) ['i ', 'sh', 'oo', 'k ', 'my', ' h', 'ea', 'd.', ' ', 'no', 't ', 'un', 'ti', 'l ', 'i ', 'gi', 've', ' h', 'im', ' t', 'he', ' s', 'co', 're', '. ', 'he', ' s', 'ee', 's ', 'me', ' c', 'om', 'in', 'g ', 'in', ' w', 'it', 'h ', 'a ', 'va', 'mp', 'ir', 'e ', 'in', ' t', 'ow', ', ', 'he', 's', ' g']\n",
      "\n",
      "Processing file dresden_files/203.txt\n",
      "Data size (Characters) (Document 202) 18863\n",
      "Sample string (Document 202) ['i ', 'ey', 'ed', ' a', 'll', ' t', 'ho', 'se', ' e', 'no', 'rm', 'ou', 's ', 'fa', 'ng', 's ', 'an', 'd ', 'ha', 'd ', 'an', ' a', 'cu', 'te', 'ly', ' u', 'nc', 'om', 'fo', 'rt', 'ab', 'le', ' f', 'la', 'sh', 'ba', 'ck', ' t', 'o ', 'mo', 'rg', 'an', ' d', 'ri', 'vi', 'ng', ' h', 'is', ' s', 'tr']\n",
      "\n",
      "Processing file dresden_files/204.txt\n",
      "Data size (Characters) (Document 203) 14602\n",
      "Sample string (Document 203) ['\\ni', ' w', 'as', ' a', 'bo', 'ut', ' t', 'o ', 'go', ' o', 'n ', 'pa', 'st', ' h', 'im', ', ', 'bu', 't ', 'in', 'st', 'ea', 'd ', 'sw', 'er', 've', 'd ', 'an', 'd ', 'ca', 'me', ' t', 'o ', 'a ', 'st', 'op', ' s', 'ta', 'nd', 'in', 'g ', 'ov', 'er', ' h', 'im', '.\\n', '\\nh', 'e ', 'co', 'nt', 'in']\n",
      "\n",
      "Processing file dresden_files/205.txt\n",
      "Data size (Characters) (Document 204) 15673\n",
      "Sample string (Document 204) ['\\n\\n', 's', 'ta', 'y ', 'cl', 'os', 'e!', ' ', 'i ', 'sh', 'ou', 'te', 'd.', ' i', ' t', 'hr', 'us', 't ', 'th', 'e ', 'en', 'd ', 'of', ' m', 'y ', 'st', 'af', 'f ', 'in', 'to', ' t', 'he', ' g', 'ra', 've', 'l ', 'an', 'd ', 'dr', 'ag', 'ge', 'd ', 'it', ' t', 'hr', 'ou', 'gh', ', ', 'dr', 'aw']\n",
      "\n",
      "Processing file dresden_files/206.txt\n",
      "Data size (Characters) (Document 205) 13732\n",
      "Sample string (Document 205) ['y', 'es', ', ', 'th', 'at', 's', ' w', 'he', 're', ' t', 'he', ' r', 'ap', 'sc', 'al', 'li', 'on', ' h', 'as', ' m', 'ad', 'e ', 'hi', 's ', 'la', 'ir', '!', ' h', 'is', ' h', 'an', 'd ', 'fl', 'as', 'he', 'd ', 'to', ' h', 'is', ' w', 'ai', 'st', ' a', 'nd', ' h', 'e ', 'dr', 'ew', ' h', 'is']\n",
      "\n",
      "Processing file dresden_files/207.txt\n",
      "Data size (Characters) (Document 206) 16339\n",
      "Sample string (Document 206) ['\\ni', 'f ', 'th', 'e ', 'ey', 'es', ' a', 're', ' t', 'he', ' w', 'in', 'do', 'ws', ' t', 'o ', 'th', 'e ', 'so', 'ul', ', ', 'th', 'en', ' w', 'iz', 'ar', 'ds', ' a', 're', ' t', 'he', ' s', 'ou', 'ls', ' ', 'vo', 'ye', 'ur', 's.', ' w', 'he', 'n ', 'a ', 'wi', 'za', 'rd', ' l', 'oo', 'ks', ' i']\n",
      "\n",
      "Processing file dresden_files/208.txt\n",
      "Data size (Characters) (Document 207) 18253\n",
      "Sample string (Document 207) ['\\nl', 'ar', 'a ', 'na', 'rr', 'ow', 'ed', ' h', 'er', ' e', 'ye', 's ', 'at', ' t', 'ha', 't.', ' ', 'yo', 'u ', 'wo', 'n', 't ', 'fi', 'nd', ' a', 'ny', 'th', 'in', 'g,', ' ', 'sh', 'e ', 'sa', 'id', ' i', 'n ', 'a ', 'fi', 'rm', ' c', 'ol', 'd ', 'to', 'ne', '.\\n', '\\n', 'be', 'ca', 'us', 'e ']\n",
      "\n",
      "Processing file dresden_files/209.txt\n",
      "Data size (Characters) (Document 208) 14796\n",
      "Sample string (Document 208) ['a', 'nd', ' w', 'il', 'so', 'n ', 'lo', 'st', ' b', 'ot', 'h ', 'ey', 'es', ',', ' l', 'ar', 'a ', 'mu', 'rm', 'ur', 'ed', '.\\n', '\\nt', 'he', ' m', 'ed', 'ic', ' a', 'vo', 'id', 'ed', ' l', 'oo', 'ki', 'ng', ' a', 't ', 'he', 'r.', ' ', 'ye', 's,', ' m', 'a', 'am', '.', '\\n\\n', 'v', 'er', 'y ']\n",
      "\n",
      "Processing file dresden_files/210.txt\n",
      "Data size (Characters) (Document 209) 14482\n",
      "Sample string (Document 209) ['sh', 'e ', 'pr', 'od', 'uc', 'ed', ' a', ' p', 'ie', 'ce', ' o', 'f ', 'no', 'te', 'bo', 'ok', ' p', 'ap', 'er', '. ', 'g', 'eo', 'rg', 'ia', ' c', 'al', 'le', 'd.', ' h', 'er', 'e', 's ', 'wh', 'er', 'e ', 'an', 'di', ' i', 's.', ' t', 'he', 'y', 're', ' s', 'ti', 'll', ' w', 'it', 'h ', 'he']\n",
      "\n",
      "Processing file dresden_files/211.txt\n",
      "Data size (Characters) (Document 210) 13087\n",
      "Sample string (Document 210) ['mu', 'rp', 'hy', ' y', 'aw', 'ne', 'd.', '\\n\\n', '\\n\\n', 'b', 'lo', 'od', 'y ', 'he', 'll', ',', ' h', 'e ', 'sn', 'ar', 'le', 'd.', ' ', 'ju', 'st', ' o', 'ne', ' o', 'f ', 'th', 'em', '. ', 'i ', 'ha', 've', 'n', 't ', 'ea', 'te', 'n ', 'si', 'nc', 'e ', 'ye', 'st', 'er', 'da', 'y.', '\\n', '\\ni']\n",
      "\n",
      "Processing file dresden_files/212.txt\n",
      "Data size (Characters) (Document 211) 14270\n",
      "Sample string (Document 211) ['sh', 'e ', 'ro', 'll', 'ed', ' d', 'ow', 'n ', 'th', 'e ', 'pa', 'ss', 'en', 'ge', 'r-', 'si', 'de', ' w', 'in', 'do', 'w ', 'an', 'd ', 'le', 't ', 'on', 'e ', 'ha', 'nd', ' h', 'an', 'g ', 'ou', 't.', ' ', 'it', 's', ' a', ' p', 'ro', 'bl', 'em', ' b', 'ec', 'au', 'se', ' y', 'ou', ' n', 'ev']\n",
      "\n",
      "Processing file dresden_files/213.txt\n",
      "Data size (Characters) (Document 212) 18784\n",
      "Sample string (Document 212) ['mu', 'rp', 'hy', ' s', 'ho', 'ok', ' h', 'er', ' h', 'ea', 'd.', ' ', 'al', 'l ', 'of', ' t', 'hi', 's ', 'ov', 'er', ' o', 'ne', ' m', 'an', '.', '\\n\\n', 'o', 've', 'r ', 'a ', 'he', 'ro', ' o', 'f ', 'th', 'e ', 'co', 'un', 'ci', 'l,', ' ', 'i ', 'sa', 'id', ' q', 'ui', 'et', 'ly', '. ', 'o']\n",
      "\n",
      "Processing file dresden_files/214.txt\n",
      "Data size (Characters) (Document 213) 14463\n",
      "Sample string (Document 213) ['t', 'he', 'n ', 'if', ' i', 'm', ' t', 'ak', 'en', ' o', 'ff', ' t', 'he', ' b', 'oa', 'rd', ', ', 'se', 'e ', 'mu', 'rp', 'hy', ',', ' i', ' s', 'ai', 'd.', ' ', 'sh', 'e ', 'kn', 'ow', 's ', 'ev', 'er', 'yt', 'hi', 'ng', ' i', ' d', 'o ', 'ab', 'ou', 't ', 'th', 'e ', 'ca', 'se', '. ', 'li']\n",
      "\n",
      "Processing file dresden_files/215.txt\n",
      "Data size (Characters) (Document 214) 18692\n",
      "Sample string (Document 214) ['i ', 'si', 'gh', 'ed', '. ', 'wi', 'za', 'rd', 's.', ' ', 'so', '? ', 'wh', 'at', ' i', 's ', 'th', 'is', ' p', 'la', 'ce', '?', '\\n\\n', 'he', ' c', 'on', 'si', 'de', 're', 'd ', 'hi', 's ', 'wo', 'rd', 's ', 'fo', 'r ', 'a ', 'mo', 'me', 'nt', '. ', 'w', 'ha', 't ', 'do', ' y', 'ou', ' t', 'hi']\n",
      "\n",
      "Processing file dresden_files/216.txt\n",
      "Data size (Characters) (Document 215) 22503\n",
      "Sample string (Document 215) ['d', 'es', 'pi', 'te', ' t', 'he', ' r', 'ec', 'en', 't ', 'ef', 'fo', 'rt', 's ', 'ma', 'de', ' o', 'n ', 'yo', 'ur', ' b', 'eh', 'al', 'f ', 'by', ' m', 'y ', 'ki', 'ng', ' a', 'nd', ' h', 'is', ' c', 'ou', 'rt', '?', ' l', 'ar', 'a ', 'as', 'ke', 'd.', '\\n\\n', 'ma', 'i ', 'fa', 'ce', 'd ', 'he']\n",
      "\n",
      "Processing file dresden_files/217.txt\n",
      "Data size (Characters) (Document 216) 23860\n",
      "Sample string (Document 216) ['i ', 'st', 'oo', 'd ', 'th', 'er', 'e ', 'wi', 'th', 'ou', 't ', 'al', 'li', 'es', ', ', 'wi', 'th', 'ou', 't ', 'mo', 'st', ' o', 'f ', 'my', ' w', 'ea', 'po', 'ns', ', ', 'an', 'd ', 'gr', 'ew', ' s', 'ic', 'k ', 'wi', 'th', ' h', 'or', 'ro', 'r ', 'as', ' t', 'he', ' s', 'ki', 'nw', 'al', 'ke']\n",
      "\n",
      "Processing file dresden_files/218.txt\n",
      "Data size (Characters) (Document 217) 15471\n",
      "Sample string (Document 217) ['\\ni', ' l', 'oo', 'ke', 'd ', 'at', ' e', 'be', 'ne', 'za', 'r.', ' ', 'wh', 'at', ' h', 'ap', 'pe', 'ne', 'd?', '\\n', '\\nh', 'e ', 'gr', 'un', 'te', 'd.', ' ', 'wh', 'oe', 've', 'r ', 'ca', 'me', ' t', 'hr', 'ou', 'gh', ' t', 'he', ' n', 'ev', 'er', 'ne', 've', 'r ', 'op', 'en', 'ed', ' a', ' g']\n",
      "\n",
      "Processing file dresden_files/219.txt\n",
      "Data size (Characters) (Document 218) 20305\n",
      "Sample string (Document 218) ['he', 'lp', ' b', 'ut', ' t', 'hi', 'nk', ' t', 'ha', 't ', 'th', 'er', 'e ', 'is', ' s', 'uc', 'h ', 'a ', 'th', 'in', 'g ', 'as', ' f', 'at', 'e', 'or', ' a', 't ', 'le', 'as', 't ', 'a ', 'hi', 'gh', 'er', ' p', 'ow', 'er', ' o', 'f ', 'so', 'me', ' s', 'or', 't,', '\\n\\n', 'at', 'te', 'mp', 'ti']\n",
      "\n",
      "Processing file dresden_files/220.txt\n",
      "Data size (Characters) (Document 219) 13812\n",
      "Sample string (Document 219) ['a', 'ls', 'o ', 'tr', 'ue', ',', ' t', 'he', ' g', 'at', 'ek', 'ee', 'pe', 'r ', 'sa', 'id', '. ', 't', 'ho', 'ug', 'h ', 'i ', 'wo', 'ul', 'd ', 'su', 'gg', 'es', 't ', 'th', 'at', ' y', 'ou', 'r ', 'fo', 're', 'th', 'ou', 'gh', 't ', 'al', 'lo', 'we', 'd ', 'yo', 'u ', 'to', ' m', 'ak', 'e ']\n",
      "\n",
      "Processing file dresden_files/221.txt\n",
      "Data size (Characters) (Document 220) 2673\n",
      "Sample string (Document 220) ['hi', 's ', 'ha', 'nd', 's ', 'cl', 'os', 'ed', ' i', 'nt', 'o ', 'fi', 'st', 's,', ' k', 'nu', 'ck', 'le', 's ', 'po', 'pp', 'in', 'g.', '\\n\\n', 's', 'ti', 'll', ' s', 'ti', 'ng', 's,', ' d', 'oe', 'sn', 't', ' i', 't?', ' ', 'i ', 'sa', 'id', ' q', 'ui', 'et', 'ly', '. ', 's', 'ti', 'll', ' b']\n",
      "\n",
      "Processing file dresden_files/222.txt\n",
      "Data size (Characters) (Document 221) 18586\n",
      "Sample string (Document 221) ['i ', 'an', 'sw', 'er', 'ed', ' t', 'he', ' p', 'ho', 'ne', ', ', 'an', 'd ', 'su', 'sa', 'n ', 'ro', 'dr', 'ig', 'ue', 'z ', 'sa', 'id', ', ', 't', 'he', 'y', 've', ' t', 'ak', 'en', ' o', 'ur', ' d', 'au', 'gh', 'te', 'r.', '\\n', '\\ni', ' s', 'at', ' t', 'he', 're', ' f', 'or', ' a', ' l', 'on']\n",
      "\n",
      "Processing file dresden_files/223.txt\n",
      "Data size (Characters) (Document 222) 19694\n",
      "Sample string (Document 222) ['i ', 'ne', 've', 'r ', 'go', 't ', 'mo', 're', ' o', 'f ', 'a ', 'lo', 'ok', ' t', 'ha', 'n ', 'th', 'at', 'f', 'li', 'ck', 'er', 'in', 'g ', 'sh', 'ad', 'ow', 's ', 'mo', 'vi', 'ng', ' w', 'it', 'h ', 'si', 'ni', 'st', 'er', ' p', 'ur', 'po', 'se', '. ', 'bu', 't ', 'i ', 'kn', 'ew', ' w', 'ha']\n",
      "\n",
      "Processing file dresden_files/224.txt\n",
      "Data size (Characters) (Document 223) 20146\n",
      "Sample string (Document 223) ['it', ' w', 'as', 'n', 't ', 'un', 'ti', 'l ', 'th', 'en', ' t', 'ha', 't ', 'i ', 'no', 'ti', 'ce', 'd ', 'th', 'at', ' w', 'hi', 'le', ' m', 'y ', 'br', 'ai', 'n ', 'ha', 'd ', 'be', 'en', ' c', 'al', 'ml', 'y ', 'pa', 'dd', 'li', 'ng', ' d', 'ow', 'n ', 'th', 'e ', 'st', 're', 'am', ' o', 'f ']\n",
      "\n",
      "Processing file dresden_files/225.txt\n",
      "Data size (Characters) (Document 224) 18285\n",
      "Sample string (Document 224) ['\\n\\n', '\\ni', ' w', 'en', 't ', 'ou', 't ', 'to', ' g', 'et', ' t', 'he', ' m', 'ai', 'l ', 'an', 'd ', 'wa', 'lk', ' t', 'he', ' d', 'og', ' a', 'ro', 'un', 'd ', 'th', 'e ', 'li', 'tt', 'le', ' b', 'ac', 'ky', 'ar', 'd ', 'wh', 'il', 'e ', 'mo', 'll', 'y,', ' s', 'us', 'an', ', ', 'an', 'd ', 'ma']\n",
      "\n",
      "Processing file dresden_files/226.txt\n",
      "Data size (Characters) (Document 225) 20136\n",
      "Sample string (Document 225) ['\\ni', ' r', 'an', ' a', 't ', 'it', ', ', 'fo', 'cu', 'si', 'ng', ' m', 'y ', 'wi', 'll', ' b', 'en', 'ea', 'th', ' m', 'e,', ' p', 'la', 'nt', 'ed', ' m', 'y ', 'st', 'af', 'f ', 'on', ' t', 'he', ' e', 'ar', 'th', ', ', 'an', 'd ', 'sw', 'un', 'g ', 'my', ' l', 'eg', 's ', 'up', ' i', 'n ', 'a ']\n",
      "\n",
      "Processing file dresden_files/227.txt\n",
      "Data size (Characters) (Document 226) 20322\n",
      "Sample string (Document 226) ['sh', 'e ', 'in', 'cl', 'in', 'ed', ' h', 'er', ' h', 'ea', 'd ', 'to', ' m', 'e,', ' a', ' g', 'ra', 'ci', 'ou', 's ', 'vi', 'ct', 'or', '. ', 'le', 'a ', 'wa', 's ', 'be', 'tt', 'er', ' a', 't ', 'th', 'at', ' s', 'or', 't ', 'of', ' w', 'or', 'dp', 'la', 'y ', 'th', 'an', ' m', 'e,', ' h', 'av']\n",
      "\n",
      "Processing file dresden_files/228.txt\n",
      "Data size (Characters) (Document 227) 20958\n",
      "Sample string (Document 227) ['\\n', 'co', 'me', ' o', 'n,', ' ', 'i ', 'sa', 'id', '. ', 'l', 'et', 's', ' g', 'et', ' o', 'ut', ' o', 'f ', 'he', 're', ' b', 'ef', 'or', 'e ', 'th', 'e ', 'ca', 'va', 'lr', 'y ', 'ar', 'ri', 've', 's.', ' o', 'h,', ' h', 'er', 'e.', ' ', 'i ', 'pa', 'ss', 'ed', ' h', 'er', ' t', 'he', ' a']\n",
      "\n",
      "Processing file dresden_files/229.txt\n",
      "Data size (Characters) (Document 228) 18723\n",
      "Sample string (Document 228) ['\\ni', ' g', 'ru', 'nt', 'ed', ' a', 'nd', ' f', 'in', 'is', 'he', 'd ', 'th', 'e ', 'bi', 'sc', 'ui', 't.', ' ', 'th', 'e ', 're', 'd ', 'co', 'ur', 't ', 'is', ' o', 'n ', 'th', 'e ', 'mo', 've', '. ', 'tr', 'ou', 'bl', 'e ', 'is', ' b', 'ei', 'ng', ' s', 'ti', 'rr', 'ed', ' u', 'p ', 'be', 'tw']\n",
      "\n",
      "Processing file dresden_files/230.txt\n",
      "Data size (Characters) (Document 229) 15648\n",
      "Sample string (Document 229) ['\\ni', 'v', 'e ', 'ta', 'ke', 'n ', 'on', ' s', 'om', 'e ', 'to', 'ug', 'h ', 'cu', 'st', 'om', 'er', 's ', 'in', ' m', 'y ', 'ti', 'me', '. ', 'bu', 't ', 'no', 'ne', ' o', 'f ', 'th', 'em', ' h', 'ad', ' b', 'ee', 'n ', 'go', 'dl', 'ik', 'e ', 'be', 'in', 'gs', 'o', 'r ', 'th', 'e ', 're', 'mn']\n",
      "\n",
      "Processing file dresden_files/231.txt\n",
      "Data size (Characters) (Document 230) 19268\n",
      "Sample string (Document 230) ['\\ni', ' w', 'as', ' k', 'id', 'di', 'ng', ' m', 'ys', 'el', 'f ', 'if', ' i', ' t', 'ho', 'ug', 'ht', ' i', ' c', 'ou', 'ld', ' r', 'un', ' f', 'ro', 'm ', 'so', 'me', 'th', 'in', 'g ', 'th', 'at', ' f', 'as', 't.', ' i', ' h', 'ad', ' a', ' s', 'te', 'p ', 'or', ' t', 'wo', ' o', 'n ', 'th', 'e ']\n",
      "\n",
      "Processing file dresden_files/232.txt\n",
      "Data size (Characters) (Document 231) 21089\n",
      "Sample string (Document 231) ['\\nm', 'ol', 'ly', ' w', 'as', 'n', 't ', 're', 'ad', 'y.', ' n', 'ot', ' f', 'or', ' s', 'om', 'et', 'hi', 'ng', ' l', 'ik', 'e ', 'we', ' w', 'er', 'e ', 'ab', 'ou', 't ', 'to', ' d', 'o.', ' s', 'he', ' h', 'ad', ' t', 'oo', ' m', 'uc', 'h ', 'co', 'nf', 'id', 'en', 'ce', ' a', 'nd', ' n', 'ot']\n",
      "\n",
      "Processing file dresden_files/233.txt\n",
      "Data size (Characters) (Document 232) 19551\n",
      "Sample string (Document 232) ['\\nt', 'he', ' c', 'ol', 'd ', 'vo', 'ic', 'e ', 'la', 'ug', 'he', 'd ', 'th', 'ro', 'ug', 'h ', 'th', 'e ', 'le', 'an', 'an', 'si', 'dh', 'e', 's ', 'li', 'ps', '. ', 't', 'hi', 's ', 'co', 'nv', 'er', 'sa', 'ti', 'on', ' w', 'ou', 'ld', ' q', 'ui', 'ck', 'ly', ' g', 'ro', 'w ', 'te', 'di', 'ou']\n",
      "\n",
      "Processing file dresden_files/234.txt\n",
      "Data size (Characters) (Document 233) 15966\n",
      "Sample string (Document 233) ['\\ni', ' r', 'os', 'e,', ' a', 'nd', ' m', 'ou', 'se', ' r', 'os', 'e ', 'wi', 'th', ' m', 'e.', ' ', 'co', 'ol', '. ', 'ma', 'yb', 'e ', 'ge', 't ', 'st', 'ar', 'te', 'd ', 'on', ' t', 'ho', 'se', ' c', 'al', 'ls', ', ', 'pa', 'dr', 'e.', '\\n', '\\nf', 'or', 'th', 'il', 'l ', 'ga', 've', ' m', 'e ']\n",
      "\n",
      "Processing file dresden_files/235.txt\n",
      "Data size (Characters) (Document 234) 16204\n",
      "Sample string (Document 234) ['\\n', 'i ', 'th', 'in', 'k ', 'yo', 'u', 're', ' p', 'la', 'yi', 'ng', ' w', 'it', 'h ', 'dy', 'na', 'mi', 'te', ' a', 'ga', 'in', ', ', 'ti', 'll', 'y,', ' ', 'sa', 'id', ' m', 'ur', 'ph', 'y', 's ', 'vo', 'ic', 'e.', '\\n\\n', 'm', 'ur', 'ph', ',', ' i', ' s', 'ai', 'd,', ' r', 'el', 'ie', 've']\n",
      "\n",
      "Processing file dresden_files/236.txt\n",
      "Data size (Characters) (Document 235) 25500\n",
      "Sample string (Document 235) ['\\ni', 'll', 'us', 'io', 'ns', ' a', 're', ' a', ' f', 'as', 'ci', 'na', 'ti', 'ng', ' b', 'ra', 'nc', 'h ', 'of', ' m', 'ag', 'ic', '. ', 'th', 'er', 'e ', 'ar', 'e ', 'tw', 'o ', 'ba', 'si', 'c ', 'wa', 'ys', ' t', 'o ', 'ma', 'na', 'ge', ' t', 'he', 'm.', ' o', 'ne', ', ', 'yo', 'u ', 'ca', 'n ']\n",
      "\n",
      "Processing file dresden_files/237.txt\n",
      "Data size (Characters) (Document 236) 18301\n",
      "Sample string (Document 236) ['\\n', 'he', 'h,', ' ', 'i ', 'sa', 'id', '. ', 't', 'hi', 's ', 'is', ' t', 'he', ' b', 'as', 's ', 'pr', 'o ', 'in', ' b', 'ol', 'in', 'gb', 'ro', 'ok', ', ', 'i ', 'th', 'in', 'k.', ' m', 'ak', 'es', ' s', 'en', 'se', ', ', 'i ', 'gu', 'es', 's.', '\\n', '\\n', 'i ', 'di', 'dn', 't', ' m', 'ea']\n",
      "\n",
      "Processing file dresden_files/238.txt\n",
      "Data size (Characters) (Document 237) 20192\n",
      "Sample string (Document 237) ['\\ni', 'f ', 'i ', 'wa', 's ', 'on', ' t', 'he', ' r', 'oa', 'd ', 'to', ' h', 'el', 'l,', ' a', 't ', 'le', 'as', 't ', 'i ', 'wa', 's ', 'go', 'in', 'g ', 'in', ' s', 'ty', 'le', '.\\n', '\\n\\n', '\\n\\n', '\\n4', '0\\n', '\\nt', 'he', 're', ' w', 'as', ' r', 'oo', 'm ', 'fo', 'r ', 'ev', 'er', 'yo', 'ne', ' i']\n",
      "\n",
      "Processing file dresden_files/239.txt\n",
      "Data size (Characters) (Document 238) 19990\n",
      "Sample string (Document 238) ['\\n', 'ye', 's,', ' ', 'su', 'sa', 'n ', 'sa', 'id', ', ', 'he', 'r ', 'vo', 'ic', 'e ', 'ti', 'gh', 't.', '\\n\\n', 'we', 'll', '. ', 'no', 'th', 'in', 'g', 's ', 'ev', 'er', ' s', 'im', 'pl', 'e,', ' i', 's ', 'it', '?\\n', '\\nt', 'ha', 't ', 'ch', 'an', 'ge', 'd ', 'ev', 'er', 'yt', 'hi', 'ng', '. ']\n",
      "\n",
      "Processing file dresden_files/240.txt\n",
      "Data size (Characters) (Document 239) 24209\n",
      "Sample string (Document 239) ['\\na', 'ri', 'an', 'na', ' s', 'mi', 'le', 'd ', 'sl', 'ig', 'ht', 'ly', ' a', 't ', 'me', '. ', 'g', 'iv', 'e ', 'my', ' f', 'at', 'he', 'r ', 'my', ' t', 'ha', 'nk', 's,', ' a', 'nd', ' t', 'el', 'l ', 'hi', 'm ', 'th', 'at', ' i', ' w', 'il', 'l ', 'jo', 'in', ' h', 'im', ' i', 'n ', 'th', 'e ']\n",
      "\n",
      "Processing file dresden_files/241.txt\n",
      "Data size (Characters) (Document 240) 21231\n",
      "Sample string (Document 240) ['\\nh', 'is', ' s', 'te', 'rn', 'um', ' c', 'ra', 'ck', 'ed', ' a', 'ud', 'ib', 'ly', ', ', 'an', 'd ', 'he', ' f', 'le', 'w ', 'ba', 'ck', 'wa', 'rd', ' a', 's ', 'if', ' r', 'am', 'me', 'd ', 'by', ' a', ' t', 'ru', 'ck', '. ', 'he', ' h', 'it', ' t', 'he', ' s', 'to', 'ne', ' w', 'al', 'l ', 'be']\n",
      "\n",
      "Processing file dresden_files/242.txt\n",
      "Data size (Characters) (Document 241) 10581\n",
      "Sample string (Document 241) ['\\ni', ' n', 'od', 'de', 'd ', 'an', 'd ', 'ro', 'll', 'ed', ' o', 'ne', ' t', 'ir', 'ed', ' a', 'nd', ' c', 'om', 'pl', 'ai', 'ni', 'ng', ' s', 'ho', 'ul', 'de', 'r.', ' ', 'sh', 'e ', 'is', '.', '\\n\\n', 'd', 'o ', 'yo', 'u ', 'wa', 'nt', ' s', 'om', 'eo', 'ne', ' e', 'ls', 'e ', 'to', ' t', 'ak']\n",
      "\n",
      "Processing file dresden_files/243.txt\n",
      "Data size (Characters) (Document 242) 16451\n",
      "Sample string (Document 242) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\nl', 'if', 'e ', 'is', ' h', 'ar', 'd.', '\\n\\n', 'dy', 'in', 'g', 's ', 'ea', 'sy', '.\\n', '\\ns', 'o ', 'ma', 'ny', ' t', 'hi', 'ng', 's ', 'mu', 'st', ' a', 'li', 'gn', ' i', 'n ', 'or', 'de', 'r ', 'to', ' c', 're', 'at', 'e ', 'li', 'fe', '. ', 'it', ' h', 'as']\n",
      "\n",
      "Processing file dresden_files/244.txt\n",
      "Data size (Characters) (Document 243) 21772\n",
      "Sample string (Document 243) ['he', ' w', 'as', ' h', 'ug', 'e.', ' n', 'ot', ' b', 'ui', 'lt', ' l', 'ik', 'e ', 'a ', 'we', 'ig', 'ht', ' l', 'if', 'te', 'r ', 'or', ' a', 'ny', 'th', 'in', 'g,', ' j', 'us', 't ', 'a ', 'na', 'tu', 'ra', 'll', 'y ', 'bi', 'g-', 'bo', 'ne', 'd,', ' b', 'ra', 'wn', 'y ', 'ma', 'n ', 'st', 'an']\n",
      "\n",
      "Processing file dresden_files/245.txt\n",
      "Data size (Characters) (Document 244) 18591\n",
      "Sample string (Document 244) ['th', 'e ', 'gu', 'nm', 'an', ' h', 'ad', ' h', 'ea', 'rd', ' m', 'or', 't ', 'sh', 'ou', 'ti', 'ng', ' a', 't ', 'me', '. ', 'he', ' c', 'am', 'e ', 'to', 'wa', 'rd', ' t', 'he', ' d', 'oo', 'r ', 'to', ' t', 'he', ' s', 'tu', 'dy', ', ', 'mo', 'vi', 'ng', ' l', 'ig', 'ht', 'ly', ' f', 'or', ' a']\n",
      "\n",
      "Processing file dresden_files/246.txt\n",
      "Data size (Characters) (Document 245) 20373\n",
      "Sample string (Document 245) ['i ', 'sh', 'ud', 'de', 're', 'd.', ' ', 'ye', 'ah', ',', ' i', ' s', 'ai', 'd.', ' ', 'we', 'll', '. ', 'i ', 'do', 'n', 't ', 'ha', 've', ' t', 'im', 'e ', 'fo', 'r ', 'th', 'at', '.', '\\n\\n', 'y', 'ou', ' h', 'av', 'e ', 'no', 'th', 'in', 'g ', 'bu', 't ', 'ti', 'me', ', ', 'dr', 'es', 'de']\n",
      "\n",
      "Processing file dresden_files/247.txt\n",
      "Data size (Characters) (Document 246) 17548\n",
      "Sample string (Document 246) ['m', 's.', ' m', 'ur', 'ph', 'y,', ' ', 'mo', 'rt', 'y ', 'sa', 'id', ', ', 'no', 'dd', 'in', 'g ', 'to', ' h', 'er', '.\\n', '\\n', 'li', 'nd', 'qu', 'is', 't,', ' i', 'sn', 't', ' i', 't?', ' ', 'mu', 'rp', 'h ', 'as', 'ke', 'd.', ' ', 'th', 'e ', 'me', 'di', 'um', '?', '\\n\\n', 'y', 'es', '.']\n",
      "\n",
      "Processing file dresden_files/248.txt\n",
      "Data size (Characters) (Document 247) 21097\n",
      "Sample string (Document 247) ['my', ' a', 'pp', 're', 'nt', 'ic', 'e ', 'no', 'dd', 'ed', ' a', 'nd', ' d', 'id', 'n', 't ', 'me', 'et', ' m', 'ur', 'ph', 'y', 's ', 'ey', 'es', '.\\n', '\\n', 'so', ',', ' m', 'ur', 'ph', 'y ', 'sa', 'id', '. ', 'h', 'e', 's ', 're', 'al', 'ly', ' .', ' .', ' .', ' r', 'ea', 'll', 'y ', 'go']\n",
      "\n",
      "Processing file dresden_files/249.txt\n",
      "Data size (Characters) (Document 248) 17927\n",
      "Sample string (Document 248) ['i ', 'tu', 'rn', 'ed', ' a', 'wa', 'y ', 'fr', 'om', ' h', 'im', ' a', 'nd', ' p', 'lu', 'ng', 'ed', ' t', 'hr', 'ou', 'gh', ' s', 'ev', 'er', 'al', ' w', 'al', 'ls', ' a', 'nd', ' o', 'ut', ' t', 'he', ' s', 'id', 'e ', 'of', ' t', 'he', ' b', 'ui', 'ld', 'in', 'g,', ' c', 'le', 'nc', 'hi', 'ng']\n",
      "\n",
      "Processing file dresden_files/250.txt\n",
      "Data size (Characters) (Document 249) 17305\n",
      "Sample string (Document 249) ['i ', 'st', 'ar', 'ed', ' f', 'or', ' a', ' s', 'ec', 'on', 'd.', ' a', ' s', 'pi', 'ri', 't ', 'co', 'ul', 'dn', 't', ' p', 'ro', 'je', 'ct', ' i', 'ts', ' p', 'ow', 'er', ' a', 'cr', 'os', 's ', 'a ', 'ci', 'rc', 'le', 'a', 'nd', ' i', ' w', 'as', ' s', 'ur', 'e ', 'th', 'at', ' p', 'ow', 'er']\n",
      "\n",
      "Processing file dresden_files/251.txt\n",
      "Data size (Characters) (Document 250) 19105\n",
      "Sample string (Document 250) ['p', 'ro', 'ba', 'bl', 'y,', ' ', 'bo', 'b ', 'sa', 'id', ' c', 'he', 'er', 'fu', 'll', 'y.', ' ', 'bu', 't ', 'it', 's', ' p', 'ro', 'ba', 'bl', 'y ', 'li', 'mi', 'te', 'd ', 'to', ' p', 're', 'tt', 'y ', 'mu', 'ch', ' d', 'es', 'tr', 'uc', 'ti', 've', ', ', 'po', 'is', 'on', 'ou', 's,', ' d']\n",
      "\n",
      "Processing file dresden_files/252.txt\n",
      "Data size (Characters) (Document 251) 22103\n",
      "Sample string (Document 251) ['go', 'od', ' a', 'dv', 'ic', 'e,', ' t', 'ha', 't.', '\\n\\n', 'th', 'e ', 'qu', 'es', 'ti', 'on', ' w', 'as', ', ', 'ho', 'w?', '\\n\\n', 'no', 'rm', 'al', 'ly', ', ', 'i ', 'wo', 'ul', 'd ', 'ha', 've', ' t', 'ra', 'ck', 'ed', ' m', 'ol', 'ly', ' d', 'ow', 'n ', 'wi', 'th', ' a', ' f', 'ai', 'rl', 'y ']\n",
      "\n",
      "Processing file dresden_files/253.txt\n",
      "Data size (Characters) (Document 252) 16163\n",
      "Sample string (Document 252) ['it', 's', ' m', 'e,', ' k', 'id', ', ', 'i ', 'th', 'ou', 'gh', 't,', ' a', 's ', 'lo', 'ud', 'ly', ' a', 's ', 'i ', 'co', 'ul', 'd.', ' d', 'on', 't', ' f', 'ig', 'ht', ' m', 'e.', '\\n\\n', 'i ', 'di', 'dn', 't', ' k', 'no', 'w ', 'wh', 'at', ' t', 'he', ' s', 'ea', 'sh', 'el', 'l ', 'wo', 'ul']\n",
      "\n",
      "Processing file dresden_files/254.txt\n",
      "Data size (Characters) (Document 253) 16490\n",
      "Sample string (Document 253) ['y', 'ou', 'r', 'e ', 'a ', 'fa', 'n?', '\\n', '\\n', 'no', ',', ' i', ' s', 'ai', 'd.', ' ', 'it', ' s', 'ho', 'ul', 'dn', 't', ' b', 'e ', 'li', 'ke', ' t', 'hi', 's.', ' t', 'he', ' g', 'an', 'gs', ' a', 're', ' d', 'an', 'ge', 'ro', 'us', ' c', 'ri', 'mi', 'na', 'ls', '. ', 'th', 'ey', ' r']\n",
      "\n",
      "Processing file dresden_files/255.txt\n",
      "Data size (Characters) (Document 254) 24731\n",
      "Sample string (Document 254) ['h', 'er', 'e', 's ', 'a ', 'th', 'ou', 'gh', 't,', ' g', 'en', 'iu', 's,', ' ', 'i ', 'sa', 'id', ' t', 'o ', 'me', '. ', 'm', 'ay', 'be', ' i', 't', 's ', 'go', 't ', 'so', 'me', 'th', 'in', 'g ', 'to', ' d', 'o ', 'wi', 'th', ' m', 'ag', 'gi', 'e.', '\\n', '\\nm', 'ag', 'gi', 'e.', ' m', 'y ']\n",
      "\n",
      "Processing file dresden_files/256.txt\n",
      "Data size (Characters) (Document 255) 19741\n",
      "Sample string (Document 255) ['it', ' h', 'ur', 't ', 'so', ' m', 'uc', 'h,', ' i', ' w', 'on', 'de', 're', 'd ', 'if', ' h', 'e ', 'al', 're', 'ad', 'y ', 'ha', 'd.', '\\n\\n', 'i ', 'pu', 't ', 'my', ' h', 'ea', 'd ', 'do', 'wn', ' a', 'nd', ' r', 'an', ' f', 'as', 'te', 'r,', ' m', 'y ', 'te', 'ar', 's ', 'ma', 'ki', 'ng', ' t']\n",
      "\n",
      "Processing file dresden_files/257.txt\n",
      "Data size (Characters) (Document 256) 16484\n",
      "Sample string (Document 256) ['i ', 'st', 'ar', 'ed', ' a', 't ', 'th', 'e ', 'fl', 'am', 'es', ' a', 'nd', ' s', 'aw', ' a', ' s', 'ha', 'pe', ' w', 'it', 'hi', 'n ', 'it', 'o', 'r,', ' r', 'at', 'he', 'r,', ' i', ' s', 'aw', ' a', ' c', 're', 'at', 'ur', 'e-', 'sh', 'ap', 'ed', ' v', 'oi', 'd ', 'wh', 'er', 'e ', 'th', 'e ']\n",
      "\n",
      "Processing file dresden_files/258.txt\n",
      "Data size (Characters) (Document 257) 18354\n",
      "Sample string (Document 257) ['i ', 'bl', 'in', 'ke', 'd.', ' ', 'um', '. ', 'wh', 'at', '?', '\\n\\n', 'y', 'ou', ' t', 'hi', 'nk', ' y', 'ou', 'r ', 'un', 'iv', 'er', 'se', ' i', 's ', 'th', 'e ', 'on', 'ly', ' u', 'ni', 've', 'rs', 'e?', ' h', 'ar', 'ry', ', ', 'co', 'me', ' o', 'n.', ' c', 're', 'at', 'io', 'n,', ' t', 'ot']\n",
      "\n",
      "Processing file dresden_files/259.txt\n",
      "Data size (Characters) (Document 258) 17299\n",
      "Sample string (Document 258) ['ar', 'is', 'te', 'de', 's ', 'wa', 's ', 'no', 'th', 'in', 'g ', 'mo', 're', ' t', 'ha', 'n ', 'a ', 'st', 're', 'ak', ' i', 'n ', 'th', 'e ', 'ai', 'r ', 'as', ' h', 'e ', 'cl', 'os', 'ed', ' o', 'n ', 'da', 'ni', 'el', ', ', 'sl', 'am', 'mi', 'ng', ' i', 'nt', 'o ', 'hi', 'm,', ' k', 'no', 'ck']\n",
      "\n",
      "Processing file dresden_files/260.txt\n",
      "Data size (Characters) (Document 259) 27168\n",
      "Sample string (Document 259) ['w', 'he', 'n?', ' ', 'he', ' a', 'sk', 'ed', '.\\n', '\\n', 'wh', 'en', ' e', 'ls', 'e?', ' ', 'i ', 'an', 'sw', 'er', 'ed', '. ', 's', 'un', 'do', 'wn', '.', '\\n\\n', '\\n\\n', '\\n\\n', 'ch', 'ap', 'te', 'r ', 'fo', 'rt', 'y\\n', '\\ni', ' v', 'an', 'is', 'he', 'd ', 'fr', 'om', ' i', 'ns', 'id', 'e ', 'th']\n",
      "\n",
      "Processing file dresden_files/261.txt\n",
      "Data size (Characters) (Document 260) 22739\n",
      "Sample string (Document 260) ['a', ' w', 'or', 'th', 'y ', 'ef', 'fo', 'rt', ' f', 'or', ' a', ' n', 'ov', 'ic', 'e,', ' ', 'ev', 'il', ' b', 'ob', ' s', 'ai', 'd.', ' ', 'i ', 'wi', 'sh', ' y', 'ou', ' t', 'o ', 'kn', 'ow', ' t', 'ha', 't ', 'i ', 're', 'gr', 'et', ' y', 'ou', 'r ', 'de', 'at', 'h ', 'as', ' t', 'he', ' l']\n",
      "\n",
      "Processing file dresden_files/262.txt\n",
      "Data size (Characters) (Document 261) 20160\n",
      "Sample string (Document 261) ['jo', 'sh', ' h', 'es', 'it', 'at', 'ed', ' a', ' m', 'om', 'en', 't ', 'an', 'd ', 'th', 'en', ' h', 'el', 'd ', 'ou', 't ', 'hi', 's ', 'ha', 'nd', '. ', 'mu', 'rp', 'hy', ' e', 'xa', 'mi', 'ne', 'd ', 'it', '. ', 'd', 'oe', 'sn', 't', ' l', 'oo', 'k ', 'to', 'o ', 'de', 'ep', '. ', 'it', 's']\n",
      "\n",
      "Processing file dresden_files/263.txt\n",
      "Data size (Characters) (Document 262) 19619\n",
      "Sample string (Document 262) ['i ', 'co', 'ul', 'd ', 'fe', 'el', ' p', 'ow', 'er', ' f', 'li', 'ck', 'er', 'in', 'g ', 'be', 'tw', 'ee', 'n ', 'th', 'em', ', ', 'th', 'ou', 'gh', ', ', 'li', 'ke', ' b', 'ur', 'st', 's ', 'of', ' h', 'ea', 't ', 'co', 'mi', 'ng', ' o', 'ut', ' o', 'f ', 'a ', 'fu', 'rn', 'ac', 'e,', ' a', 's ']\n",
      "\n",
      "Processing file dresden_files/264.txt\n",
      "Data size (Characters) (Document 263) 16577\n",
      "Sample string (Document 263) ['sh', 'e ', 'sh', 'iv', 'er', 'ed', '. ', 'th', 'en', ' s', 'he', ' s', 'ai', 'd,', ' ', 'ho', 'w ', 'do', ' y', 'ou', ' w', 'an', 't ', 'to', ' d', 'o ', 'it', '?', '\\n\\n', 'b', 'ri', 'ng', ' m', 'e ', 'a ', 'ph', 'on', 'e,', ' ', 'i ', 'sa', 'id', '. ', 'n', 'ee', 'd ', 'to', ' m', 'ak', 'e ']\n",
      "\n",
      "Processing file dresden_files/265.txt\n",
      "Data size (Characters) (Document 264) 16817\n",
      "Sample string (Document 264) ['i ', 'we', 'nt', ' b', 'ac', 'k ', 'ov', 'er', ' t', 'o ', 'ur', 'ie', 'l ', 'to', ' f', 'in', 'd ', 'hi', 'm ', 'co', 'nv', 'er', 'si', 'ng', ' w', 'it', 'h ', 'si', 'r ', 'st', 'ua', 'rt', '.\\n', '\\n', 'do', 'n', 't ', 'kn', 'ow', ',', ' s', 'ir', ' s', 'tu', 'ar', 't ', 'wa', 's ', 'sa', 'yi']\n",
      "\n",
      "Processing file dresden_files/266.txt\n",
      "Data size (Characters) (Document 265) 17822\n",
      "Sample string (Document 265) ['\\n\\n', '\\no', 'ne', '\\n\\n', '\\n\\n', '\\n\\n', 'ma', 'b,', ' t', 'he', ' q', 'ue', 'en', ' o', 'f ', 'ai', 'r ', 'an', 'd ', 'da', 'rk', 'ne', 'ss', ', ', 'mo', 'na', 'rc', 'h ', 'of', ' t', 'he', ' w', 'in', 'te', 'r ', 'co', 'ur', 't ', 'of', ' t', 'he', ' s', 'id', 'he', ', ', 'ha', 's ', 'un', 'iq', 'ue']\n",
      "\n",
      "Processing file dresden_files/267.txt\n",
      "Data size (Characters) (Document 266) 20560\n",
      "Sample string (Document 266) ['\\n', 'th', 'is', ' i', 's ', 'th', 'e ', 're', 'al', 'it', 'y,', ' s', 'ar', 'is', 'sa', ',', ' i', ' s', 'ai', 'd ', 'qu', 'ie', 'tl', 'y.', ' ', 'i', 'm ', 'th', 'e ', 'wi', 'nt', 'er', ' k', 'ni', 'gh', 't.', ' i', 'v', 'e ', 'go', 't ', 'ma', 'b', 's ', 'fa', 'vo', 'r ', 'an', 'd ', 'bl']\n",
      "\n",
      "Processing file dresden_files/268.txt\n",
      "Data size (Characters) (Document 267) 22789\n",
      "Sample string (Document 267) ['s', 'ir', ' k', 'ni', 'gh', 't,', ' ', 'sa', 'id', ' t', 'he', ' s', 'id', 'he', ' h', 'ol', 'di', 'ng', ' s', 'ar', 'is', 'sa', '. ', 'he', ' h', 'ad', ' s', 'tr', 'ai', 'gh', 't ', 'bl', 'ac', 'k ', 'ha', 'ir', ' b', 'en', 'ea', 'th', ' t', 'he', ' c', 'ap', ', ', 'an', 'd ', 'ch', 'ee', 'kb']\n",
      "\n",
      "Processing file dresden_files/269.txt\n",
      "Data size (Characters) (Document 268) 19139\n",
      "Sample string (Document 268) ['b', 'ec', 'au', 'se', ' i', 'm', ' n', 'ot', ' a', ' g', 'od', 'da', 'mn', 'ed', ' s', 'ex', ' d', 'ol', 'l.', '\\n', '\\n', 'th', 'at', 's', ' a', ' g', 'oo', 'd ', 're', 'as', 'on', ' t', 'o ', 'av', 'oi', 'd ', 'at', 'te', 'nt', 'io', 'n ', 'th', 'at', ' i', 's ', 'fo', 'rc', 'ed', ' o', 'n ']\n",
      "\n",
      "Processing file dresden_files/270.txt\n",
      "Data size (Characters) (Document 269) 13599\n",
      "Sample string (Document 269) ['i', 'll', ' g', 'iv', 'e ', 'bo', 'b ', 'th', 'is', ' m', 'uc', 'h', 'th', 'e ', 'li', 'tt', 'le', ' c', 're', 'ep', ' h', 'ad', ' g', 'oo', 'd ', 'ta', 'st', 'e.', ' c', 'ha', 'ng', 'in', 'g ', 'in', 'to', ' a', ' w', 'ol', 'f ', 'mu', 'st', ' b', 'e ', 'a ', 're', 'al', 'ly', ' f', 'an', 'ta']\n",
      "\n",
      "Processing file dresden_files/271.txt\n",
      "Data size (Characters) (Document 270) 20400\n",
      "Sample string (Document 270) ['\\na', 's ', 'th', 'e ', 'he', 'av', 'y ', 'ol', 'd ', 'pi', 'ec', 'e ', 'of', ' d', 'et', 'ro', 'it', ' i', 'ro', 'n ', 'be', 'ga', 'n ', 'to', ' s', 'li', 'de', ' o', 'n ', 'th', 'e ', 'as', 'ph', 'al', 't,', ' i', ' s', 'aw', ' a', 'n ', 'ob', 'je', 'ct', ' t', 'um', 'bl', 'e ', 'ou', 't ', 'of']\n",
      "\n",
      "Processing file dresden_files/272.txt\n",
      "Data size (Characters) (Document 271) 16249\n",
      "Sample string (Document 271) ['\\n', 'su', 're', ' i', ' d', 'o,', ' ', 'i ', 'sa', 'id', '.\\n', '\\ns', 'he', ' s', 'ta', 'rt', 'ed', ' p', 'ut', 'ti', 'ng', ' t', 'he', ' k', 'it', ' a', 'wa', 'y.', ' ', 'th', 'in', 'k ', 'of', ' t', 'he', ' c', 'lo', 'th', 'es', ' a', 's ', '. ', '. ', '. ', 'as', ' a', ' b', 'ir', 'th', 'da']\n",
      "\n",
      "Processing file dresden_files/273.txt\n",
      "Data size (Characters) (Document 272) 19066\n",
      "Sample string (Document 272) ['i', 'f ', 'so', 'me', 'on', 'e ', 'se', 't ', 'us', ' u', 'p ', 'to', ' b', 'e ', 'he', 're', ',', ' t', 'ho', 'ma', 's ', 'sa', 'id', ', ', 'i', 't', 's ', 'a ', 'tr', 'ap', '.', '\\n\\n', 'i', 'f ', 'it', 's', ' a', ' t', 'ra', 'p,', ' t', 'he', 'y ', 'su', 're', ' a', 's ', 'he', 'll', ' d']\n",
      "\n",
      "Processing file dresden_files/274.txt\n",
      "Data size (Characters) (Document 273) 16959\n",
      "Sample string (Document 273) ['\\n', 'th', 'ey', ' a', 're', ' a', 'lw', 'ay', 's ', 'da', 'ng', 'er', 'ou', 's.', ' b', 'ut', ' t', 'he', 'y ', 'ha', 've', ' t', 'he', ' l', 'ea', 'st', ' o', 'pp', 'or', 'tu', 'ni', 'ty', ' t', 'o ', 'ex', 'pr', 'es', 's ', 'it', ' h', 'er', 'e.', '\\n', '\\ni', ' b', 'li', 'nk', 'ed', '. ', 'th']\n",
      "\n",
      "Processing file dresden_files/275.txt\n",
      "Data size (Characters) (Document 274) 13951\n",
      "Sample string (Document 274) ['\\n', 'th', 'om', 'as', '!', ' i', ' s', 'ho', 'ut', 'ed', '. ', 't', 'hr', 'ot', 'tl', 'e ', 'do', 'wn', '! ', 'le', 't ', 'th', 'em', ' c', 'at', 'ch', ' u', 'p ', 'to', ' u', 's ', 'an', 'd ', 'th', 'en', ' g', 'un', ' i', 't!', '\\n', '\\nt', 'ho', 'ma', 's ', 'sl', 'ow', 'ed', ' t', 'he', ' b']\n",
      "\n",
      "Processing file dresden_files/276.txt\n",
      "Data size (Characters) (Document 275) 14784\n",
      "Sample string (Document 275) ['g', 'ra', 'ss', 'ho', 'pp', 'er', ',', ' i', ' s', 'ai', 'd.', '\\n\\n', 'o', 'n ', 'it', ',', ' m', 'ol', 'ly', ' s', 'ai', 'd.', ' s', 'he', ' r', 'os', 'e ', 'to', ' h', 'er', ' f', 'ee', 't,', ' f', 'ro', 'wn', 'in', 'g,', ' h', 'er', ' e', 'ye', 's ', 'mo', 'st', 'ly', ' c', 'lo', 'se', 'd,']\n",
      "\n",
      "Processing file dresden_files/277.txt\n",
      "Data size (Characters) (Document 276) 17878\n",
      "Sample string (Document 276) ['\\n', 'ca', 'n ', 'i ', 'do', ' t', 'hi', 's?', ' ', 'i ', 'as', 'ke', 'd ', 'hi', 's ', 'ba', 'ck', '.\\n', '\\n', 'yo', 'u ', 'ca', 'n.', '\\n', '\\ni', ' m', 'ad', 'e ', 'an', ' e', 'xa', 'sp', 'er', 'at', 'ed', ' s', 'ou', 'nd', '. ', 'h', 'ow', ' d', 'o ', 'yo', 'u ', 'kn', 'ow', '?', '\\n\\n', 'od']\n",
      "\n",
      "Processing file dresden_files/278.txt\n",
      "Data size (Characters) (Document 277) 16781\n",
      "Sample string (Document 277) ['\\n', 'ha', 'rr', 'y,', ' w', 'e ', 'ne', 'ed', ' t', 'o ', 'mo', 've', ',', ' t', 'ho', 'ma', 's ', 'sa', 'id', ', ', 'ur', 'ge', 'nc', 'y ', 'ti', 'gh', 'te', 'ni', 'ng', ' h', 'is', ' v', 'oi', 'ce', '.\\n', '\\ni', ' c', 'ou', 'ld', ' h', 'ea', 'r ', 'th', 'e ', 'si', 're', 'ns', ' n', 'ow', '. ']\n",
      "\n",
      "Processing file dresden_files/279.txt\n",
      "Data size (Characters) (Document 278) 19136\n",
      "Sample string (Document 278) ['i ', 'ma', 'de', ' a', ' l', 'ow', ' g', 'ro', 'wl', 'in', 'g ', 'so', 'un', 'd.', ' ', 'th', 'is', ' i', 's ', 'in', 'sa', 'ne', '.', '\\n\\n', 'f', 'un', ',', ' m', 'ae', 've', ' s', 'ai', 'd,', ' h', 'er', ' n', 'os', 'e ', 'wr', 'in', 'kl', 'in', 'g,', ' ', 'is', 'n', 't ', 'it', '?', '\\n\\n']\n",
      "\n",
      "Processing file dresden_files/280.txt\n",
      "Data size (Characters) (Document 279) 15528\n",
      "Sample string (Document 279) ['m', 'ea', 'ni', 'ng', ' w', 'ha', 't?', ' ', 'bu', 'tt', 'er', 's ', 'as', 'ke', 'd.', ' ', 'si', 't ', 'up', ' s', 'o ', 'i ', 'ca', 'n ', 'dr', 'es', 's ', 'th', 'es', 'e.', '\\n', '\\ni', ' s', 'at', ' u', 'p ', 'an', 'd ', 'li', 'ft', 'ed', ' m', 'y ', 'ar', 'ms', ' o', 'ut', ' o', 'f ', 'th']\n",
      "\n",
      "Processing file dresden_files/281.txt\n",
      "Data size (Characters) (Document 280) 17579\n",
      "Sample string (Document 280) ['o', 'ka', 'y,', ' ', 'i ', 'sa', 'id', '. ', 'i', 'm', ' g', 'oi', 'ng', ' t', 'o ', 'op', 'en', ' t', 'he', ' d', 'oo', 'r ', 'no', 'w.', ' ', 'i ', 'cr', 'ac', 'ke', 'd ', 'op', 'en', ' t', 'he', ' o', 've', 'n ', 'do', 'or', ' a', 'nd', ' o', 'pe', 'ne', 'd ', 'it', ' s', 'lo', 'wl', 'y,']\n",
      "\n",
      "Processing file dresden_files/282.txt\n",
      "Data size (Characters) (Document 281) 18679\n",
      "Sample string (Document 281) ['a ', 're', 'la', 'ti', 've', 'ly', ' s', 'ma', 'll', ' f', 'lo', 'ck', ' o', 'f ', 'bi', 'rd', 's,', ' o', 'nl', 'y ', 'a ', 'fe', 'w ', 'hu', 'nd', 're', 'd,', ' b', 'lu', 'rr', 'ed', ' b', 'y ', 'be', 'tw', 'ee', 'n ', 'me', ' a', 'nd', ' t', 'it', 'an', 'ia', '. ', 'wh', 'en', ' t', 'he', 'y ']\n",
      "\n",
      "Processing file dresden_files/283.txt\n",
      "Data size (Characters) (Document 282) 19243\n",
      "Sample string (Document 282) ['\\nw', 'e ', 'ar', 'e?', ' w', 'e', 're', ' n', 'ot', '?\\n', '\\ni', ' k', 'ep', 't ', 'a ', 'st', 'ra', 'ig', 'ht', ' f', 'ac', 'e ', 'wh', 'il', 'e ', 'my', ' i', 'nn', 'er', ' n', 'ea', 'nd', 'er', 'th', 'al', ' s', 'pl', 'ut', 'te', 're', 'd ', 'an', 'd ', 'th', 'en', ' w', 'en', 't ', 'on', ' a']\n",
      "\n",
      "Processing file dresden_files/284.txt\n",
      "Data size (Characters) (Document 283) 14603\n",
      "Sample string (Document 283) ['n', 'o,', ' i', 't ', 'wo', 'n', 't.', ' ', 'i ', 'si', 'gh', 'ed', '.\\n', '\\nl', 'ac', 'un', 'a ', 'pe', 'er', 'ed', ' a', 't ', 'my', ' s', 'hi', 'rt', '. ', 'a', 'er', '-o', '-s', 'mi', 'th', '. ', 'ar', 'ro', 'ws', 'mi', 'th', '. ', 'do', 'es', ' t', 'he', ' s', 'hi', 'rt', ' b', 'el', 'on']\n",
      "\n",
      "Processing file dresden_files/285.txt\n",
      "Data size (Characters) (Document 284) 18056\n",
      "Sample string (Document 284) ['\\n', 'ri', 'gh', 't,', ' ', 'mo', 'll', 'y ', 'sa', 'id', '. ', 'c', 'om', 'e ', 'on', ', ', 'mo', 'us', 'e.', '\\n', '\\nt', 'he', ' b', 'ig', ' d', 'og', ' c', 'am', 'e ', 'up', ' b', 'es', 'id', 'e ', 'mo', 'll', 'y,', ' a', 'nd', ' s', 'he', ' d', 'id', 'n', 't ', 'ev', 'en', ' h', 'av', 'e ']\n",
      "\n",
      "Processing file dresden_files/286.txt\n",
      "Data size (Characters) (Document 285) 18020\n",
      "Sample string (Document 285) ['c', 'hr', 'is', 't,', ' ', 'i ', 'sa', 'id', '.\\n', '\\nk', 'ar', 'ri', 'n ', 'lo', 'ok', 'ed', ' d', 'ow', 'n ', 'at', ' t', 'he', ' f', 'lo', 'or', ' a', 'nd', ' s', 'mi', 'le', 'd ', 'br', 'ie', 'fl', 'y,', ' t', 'he', 'n ', 'lo', 'ok', 'ed', ' b', 'ac', 'k ', 'up', ' a', 't ', 'me', '. ', 'i']\n",
      "\n",
      "Processing file dresden_files/287.txt\n",
      "Data size (Characters) (Document 286) 21150\n",
      "Sample string (Document 286) ['th', 'er', 'e ', 'wa', 's ', 'a ', 'lo', 'w ', 'ru', 'mb', 'le', ' a', 's ', 'th', 'e ', 'ha', 'rl', 'ey', ' c', 'am', 'e ', 'pu', 'rr', 'in', 'g ', 'sl', 'ow', 'ly', ' o', 've', 'r ', 'th', 'e ', 'gr', 'ou', 'nd', ' t', 'ow', 'ar', 'd ', 'us', '. ', 'ka', 'rr', 'in', ' s', 'ta', 're', 'd ', 'at']\n",
      "\n",
      "Processing file dresden_files/288.txt\n",
      "Data size (Characters) (Document 287) 22337\n",
      "Sample string (Document 287) ['l', 'ik', 'e ', 'ma', 'b ', 'wo', 'ul', 'dn', 't', ' h', 'av', 'e ', 'fi', 'gu', 're', 'd ', 'it', ' o', 'ut', ',', ' i', ' s', 'ai', 'd.', ' ', 'li', 'ke', ' s', 'he', ' d', 'id', ' w', 'he', 'n ', 'yo', 'u ', 'in', 'fe', 'ct', 'ed', ' l', 'ea', '.', '\\n\\n', 'f', 'ur', 'th', 'er', ' c', 'on']\n",
      "\n",
      "Processing file dresden_files/289.txt\n",
      "Data size (Characters) (Document 288) 19014\n",
      "Sample string (Document 288) ['al', 'l ', 'ri', 'gh', 't,', ' t', 'ha', 't ', 'wa', 'sn', 't', ' l', 'it', 'er', 'al', 'ly', ' t', 'ru', 'e.', ' t', 'he', 're', ' w', 'er', 'e ', 'tw', 'en', 'ty', ' o', 'f ', 'th', 'em', ', ', 'pl', 'us', ' o', 'ne', ' o', 'th', 'er', ' m', 'or', 'ta', 'l.', '\\n\\n', 'an', 'd ', 'de', 'mo', 'nr']\n",
      "\n",
      "Processing file dresden_files/290.txt\n",
      "Data size (Characters) (Document 289) 19362\n",
      "Sample string (Document 289) ['\\nw', 'hi', 'ch', ' h', 'ad', ' b', 'ee', 'n ', 'ma', 'b', 's ', 'fr', 'ea', 'ki', 'ng', ' o', 'rd', 'er', ' i', 'n ', 'th', 'e ', 'fi', 'rs', 't ', 'pl', 'ac', 'e.', '\\n\\n', 'he', ' g', 'la', 'nc', 'ed', ' a', 'si', 'de', '. ', 'y', 'ou', ' k', 'no', 'w ', 'sh', 'e', 's ', 'an', ' i', 'mm', 'or']\n",
      "\n",
      "Processing file dresden_files/291.txt\n",
      "Data size (Characters) (Document 290) 13673\n",
      "Sample string (Document 290) ['\\nd', 'ir', 'ec', 'tl', 'y ', 'in', 'to', ' s', 'ar', 'is', 'sa', '.\\n', '\\ns', 'ar', 'is', 'sa', 's', ' e', 'ye', 's ', 'wi', 'de', 'ne', 'd ', 'in', ' h', 'or', 'ro', 'r,', ' a', 'nd', ' s', 'he', ' l', 'if', 'te', 'd ', 'he', 'r ', 'ar', 'ms', ' i', 'n ', 'an', ' i', 'ns', 'ti', 'nc', 'ti', 've']\n",
      "\n",
      "Processing file dresden_files/292.txt\n",
      "Data size (Characters) (Document 291) 6063\n",
      "Sample string (Document 291) ['a', 'bo', 'ut', ' w', 'ha', 't?', '\\n', '\\nm', 'ou', 'se', ' l', 'oo', 'ke', 'd ', 'ba', 'ck', ' a', 'nd', ' f', 'or', 'th', ' b', 'et', 'we', 'en', ' u', 's ', 'an', 'd ', 'st', 'ar', 'te', 'd ', 'wa', 'gg', 'in', 'g ', 'hi', 's ', 'ta', 'il', ' h', 'op', 'ef', 'ul', 'ly', '.\\n', '\\n', 'qu', 'ie']\n",
      "\n",
      "Processing file dresden_files/293.txt\n",
      "Data size (Characters) (Document 292) 18410\n",
      "Sample string (Document 292) ['on', 'e\\n', '\\n\\n', '\\t\\t', '\\tt', 'he', 're', ' w', 'as', ' a', ' t', 'ic', 'ki', 'ng', ' t', 'im', 'e ', 'bo', 'mb', ' i', 'ns', 'id', 'e ', 'my', ' h', 'ea', 'd ', 'an', 'd ', 'th', 'e ', 'on', 'e ', 'pe', 'rs', 'on', ' i', ' t', 'ru', 'st', 'ed', ' t', 'o ', 'go', ' i', 'n ', 'an', 'd ', 'ge', 't ']\n",
      "\n",
      "Processing file dresden_files/294.txt\n",
      "Data size (Characters) (Document 293) 16831\n",
      "Sample string (Document 293) ['\\t\\t', '\\tm', 'ab', ' t', 'ur', 'ne', 'd ', 'he', 'r ', 'he', 'ad', ' v', 'er', 'y ', 'sl', 'ow', 'ly', ' t', 'o ', 'ni', 'co', 'de', 'mu', 's.', ' ', 'as', ' i', ' r', 'em', 'em', 'be', 'r ', 'it', ',', ' s', 'he', ' s', 'ai', 'd,', ' h', 'er', ' t', 'on', 'e ', 'ar', 'ct', 'ic', ', ', 'w', 'he']\n",
      "\n",
      "Processing file dresden_files/295.txt\n",
      "Data size (Characters) (Document 294) 19502\n",
      "Sample string (Document 294) ['\\n\\n', '\\n\\n', 'se', 've', 'n\\n', '\\n\\n', '\\t\\t', '\\tw', 'e ', 'go', 't ', 'ou', 't ', 'of', ' k', 'ar', 'ri', 'n', 's ', 'li', 'tt', 'le', ' s', 'uv', ' a', 'nd', ' h', 'ea', 'de', 'd ', 'to', 'wa', 'rd', ' t', 'he', ' c', 're', 'ep', 'y ', 'ol', 'd ', 'sl', 'au', 'gh', 'te', 'rh', 'ou', 'se', ' f', 'ul']\n",
      "\n",
      "Processing file dresden_files/296.txt\n",
      "Data size (Characters) (Document 295) 19012\n",
      "Sample string (Document 295) ['\\n\\t', '\\t\\t', 'va', 'lm', 'on', 't ', 'lo', 'ok', 'ed', ' b', 'ac', 'k ', 'an', 'd ', 'fo', 'rt', 'h ', 'be', 'tw', 'ee', 'n ', 'us', ' f', 'or', ' a', ' s', 'ec', 'on', 'd,', ' h', 'er', ' e', 'xp', 're', 'ss', 'io', 'n ', 'cl', 'os', 'ed', '. ', 't', 'hi', 's ', 'jo', 'b ', 'wa', 's ', 'an', ' a']\n",
      "\n",
      "Processing file dresden_files/297.txt\n",
      "Data size (Characters) (Document 296) 18308\n",
      "Sample string (Document 296) ['\\t\\t', '\\ti', ' b', 'li', 'nk', 'ed', ' a', 't ', 'th', 'at', ' i', 'nf', 'or', 'ma', 'ti', 'on', '. ', 's', 'in', 'ce', ' w', 'he', 'n ', 'ha', 've', ' y', 'ou', ' b', 'ee', 'n ', 'al', 'l ', 'sa', 'vv', 'y ', 'on', ' t', 'he', ' s', 'up', 'er', 'na', 'tu', 'ra', 'l ', 'sc', 'en', 'e?', '\\n', '\\n\\t']\n",
      "\n",
      "Processing file dresden_files/298.txt\n",
      "Data size (Characters) (Document 297) 14667\n",
      "Sample string (Document 297) ['\\t\\t', '\\ts', 'he', ' g', 'as', 'pe', 'd ', 'a ', 'fe', 'w ', 'se', 'co', 'nd', 's ', 'la', 'te', 'r,', ' t', 'wi', 'st', 'in', 'g ', 'an', 'd ', 'tw', 'it', 'ch', 'in', 'g ', 'in', ' r', 'ea', 'ct', 'io', 'n,', ' a', 'nd', ' i', ' c', 'ou', 'ld', ' s', 'ee', ' h', 'er', ' a', 'ga', 'in', ' i', 'n ']\n",
      "\n",
      "Processing file dresden_files/299.txt\n",
      "Data size (Characters) (Document 298) 21697\n",
      "Sample string (Document 298) ['\\t\\t', '\\ts', 'o.', ' t', 'he', ' w', 'if', 'e ', 'wa', 's ', 'cu', 'tt', 'in', 'g ', 'in', ' o', 'n ', 'ni', 'co', 'de', 'mu', 's', 's ', 'ac', 'ti', 'on', 'a', 'ss', 'um', 'in', 'g ', 'de', 'ir', 'dr', 'e ', 'wa', 's ', 'te', 'll', 'in', 'g ', 'th', 'e ', 'tr', 'ut', 'h,', ' w', 'hi', 'ch', ' i']\n",
      "\n",
      "Processing file dresden_files/300.txt\n",
      "Data size (Characters) (Document 299) 13796\n",
      "Sample string (Document 299) ['\\t\\t', '\\tb', 'ea', 'ut', 'if', 'ul', ' s', 'us', 'an', ', ', 'wh', 'o ', 'i', 'd ', 'fa', 'il', 'ed', ', ', 'ju', 'st', ' l', 'ik', 'e ', 'i', 'd ', 'fa', 'il', 'ed', ' h', 'ar', 've', 'y.', '\\n\\n', '\\t\\t', '\\ta', 'nd', ' a', 'ft', 'er', ' t', 'ha', 't,', ' i', 'd', ' t', 'ak', 'en', ' m', 'ol', 'ly']\n",
      "\n",
      "Processing file dresden_files/301.txt\n",
      "Data size (Characters) (Document 300) 13676\n",
      "Sample string (Document 300) ['\\n\\n', 'tw', 'en', 'ty', '-o', 'ne', '\\n\\n', '\\n\\t', '\\t\\t', 'i ', 'ro', 'll', 'ed', ' b', 'ac', 'k ', 'up', ' t', 'o ', 'th', 'e ', 'sl', 'au', 'gh', 'te', 'rh', 'ou', 'se', ' j', 'us', 't ', 'be', 'fo', 're', ' t', 'he', ' r', 'en', 'te', 'd ', 'to', 'wn', ' c', 'ar', 's', ' t', 'ra', 'ns', 'mi', 'ss']\n",
      "\n",
      "Processing file dresden_files/302.txt\n",
      "Data size (Characters) (Document 301) 17782\n",
      "Sample string (Document 301) ['\\t\\t', '\\ti', ' g', 'lo', 'we', 're', 'd ', 'at', ' h', 'im', '. ', 'i', 't ', 'ki', 'nd', ' o', 'f ', 'do', 'es', ',', ' i', ' s', 'ai', 'd.', ' t', 'he', 'n,', ' t', 'ho', 'ug', 'ht', 'fu', 'll', 'y,', ' i', ' r', 'ai', 'se', 'd ', 'my', ' v', 'oi', 'ce', ' t', 'o ', 'ca', 'rr', 'y ', 'a ', 'li']\n",
      "\n",
      "Processing file dresden_files/303.txt\n",
      "Data size (Characters) (Document 302) 16690\n",
      "Sample string (Document 302) ['\\t\\t', '\\t', 'he', 's', ' b', 'ee', 'n ', '. ', '. ', '.', ' s', 'he', ' f', 'ol', 'de', 'd ', 'he', 'r ', 'ar', 'ms', '. ', 'y', 'ou', '-i', 'ng', ', ', 'i ', 'su', 'pp', 'os', 'e.', ' w', 'it', 'h ', 'yo', 'u ', 'go', 'ne', ' f', 'ro', 'm ', 'th', 'e ', 'ci', 'ty', ' a', 'nd', ' m', 'ol', 'ly']\n",
      "\n",
      "Processing file dresden_files/304.txt\n",
      "Data size (Characters) (Document 303) 23075\n",
      "Sample string (Document 303) ['\\t\\t', '\\t', 'ma', 'rc', 'on', 'e ', 'is', ' n', 'ot', ' a', ' d', 'um', 'my', ',', ' i', ' s', 'ai', 'd.', ' ', 'he', 's', ' g', 'on', 'e ', 'up', ' a', 'ga', 'in', 'st', ' s', 'up', 'er', 'na', 'tu', 'ra', 'l ', 'po', 'we', 'rs', ' m', 'or', 'e ', 'th', 'an', ' o', 'nc', 'e.', ' h', 'e ', 'kn']\n",
      "\n",
      "Processing file dresden_files/305.txt\n",
      "Data size (Characters) (Document 304) 17350\n",
      "Sample string (Document 304) ['\\n\\t', '\\t\\t', 'sh', 'e ', 'di', 'dn', 't', ' s', 'ay', ' a', 'ny', 'th', 'in', 'g ', 'ba', 'ck', '. ', 'ka', 'rr', 'in', 's', ' n', 'ev', 'er', ' b', 'ee', 'n ', 'a ', 'bi', 'g ', 'on', 'e ', 'fo', 'r ', 'ba', 'ck', 'ta', 'lk', 'in', 'g ', 'th', 'e ', 'ba', 'd ', 'gu', 'ys', ' w', 'it', 'ho', 'ut']\n",
      "\n",
      "Processing file dresden_files/306.txt\n",
      "Data size (Characters) (Document 305) 13922\n",
      "Sample string (Document 305) ['\\n\\t', '\\t\\t', 'i', 't ', 'is', ' w', 'ha', 't ', 'ma', 'ke', 's ', 'me', ' a', 'n ', 'an', 'ge', 'l,', ' ', 'ur', 'ie', 'l ', 'sa', 'id', '.\\n', '\\n\\t', '\\t\\t', 'm', 'er', 'ci', 'fu', 'l ', 'mo', 'th', 'er', ' o', 'f ', 'go', 'd,', ' ', 'mi', 'ch', 'ae', 'l ', 'sa', 'id', ', ', 'hi', 's ', 'vo', 'ic']\n",
      "\n",
      "Processing file dresden_files/307.txt\n",
      "Data size (Characters) (Document 306) 19035\n",
      "Sample string (Document 306) ['\\n\\t', '\\t\\t', 'tr', 'an', 'sl', 'at', 'io', 'n:', ' h', 'e ', 'wa', 'sn', 't', ' g', 'oi', 'ng', ' t', 'o ', 'do', ' a', 'ny', 'th', 'in', 'g', 'or', ' n', 'ot', ' d', 'o ', 'an', 'yt', 'hi', 'ng', 't', 'ha', 't ', 'mi', 'gh', 't ', 'sc', 're', 'w ', 'up', ' u', 'ri', 'el', 's', ' g', 'ra', 'ce']\n",
      "\n",
      "Processing file dresden_files/308.txt\n",
      "Data size (Characters) (Document 307) 19791\n",
      "Sample string (Document 307) ['\\t\\t', '\\t', 'th', 'e ', 'he', 'll', ' i', 't ', 'is', 'n', 't.', ' y', 'ou', ' h', 'ad', ' a', ' c', 'ha', 'nc', 'e ', 'fo', 'r ', 'th', 'at', ' a', 'nd', ' y', 'ou', ' t', 'ur', 'ne', 'd ', 'it', ' d', 'ow', 'n?', ' y', 'ou', 'r', 'e ', 'a ', 'fu', 'ck', 'in', 'g ', 'id', 'io', 't.', ' i', 'm']\n",
      "\n",
      "Processing file dresden_files/309.txt\n",
      "Data size (Characters) (Document 308) 22163\n",
      "Sample string (Document 308) ['\\n\\t', '\\t\\t', 'a', ' l', 'it', 'tl', 'e ', 'to', 'o ', 'si', 'mp', 'le', ',', ' s', 'he', ' s', 'ai', 'd,', ' a', 'nd', ' s', 'ta', 'rt', 'ed', ' t', 'ak', 'in', 'g ', 'of', 'f ', 'he', 'r ', 'ow', 'n ', 'th', 'or', 'n ', 'ma', 'na', 'cl', 'es', '.\\n', '\\n\\t', '\\t\\t', 's', 'ur', 'e,', ' ', 'i ', 'sa']\n",
      "\n",
      "Processing file dresden_files/310.txt\n",
      "Data size (Characters) (Document 309) 17345\n",
      "Sample string (Document 309) ['\\n\\t', '\\t\\t', 'ni', 'co', 'de', 'mu', 's ', 'in', 'cl', 'in', 'ed', ' h', 'is', ' h', 'ea', 'd ', 'to', ' m', 'e,', ' v', 'er', 'y ', 'sl', 'ig', 'ht', 'ly', '. ', 'th', 'en', ' h', 'e ', 'sa', 'id', ', ', 'y', 'ou', ' h', 'av', 'e ', 'ne', 've', 'r ', 'be', 'at', 'en', ' m', 'e,', ' s', 'ir', ' k']\n",
      "\n",
      "Processing file dresden_files/311.txt\n",
      "Data size (Characters) (Document 310) 20711\n",
      "Sample string (Document 310) ['\\t\\t', '\\t', 'dr', 'es', 'de', 'n,', ' ', 'sh', 'e ', 'sa', 'id', ', ', 'i', 'm', ' i', 'n ', 'th', 'is', ' f', 'or', ' t', 'he', ' m', 'on', 'ey', ', ', 'an', 'd ', 're', 've', 'ng', 'e ', 'if', ' i', ' g', 'et', ' a', ' c', 'ha', 'nc', 'e ', 'at', ' i', 't.', ' i', 'm', ' n', 'ot', ' h', 'er']\n",
      "\n",
      "Processing file dresden_files/312.txt\n",
      "Data size (Characters) (Document 311) 22807\n",
      "Sample string (Document 311) ['\\t\\t', '\\t', 'by', ' a', 'll', ' m', 'ea', 'ns', '.', '\\n\\n', '\\t\\t', '\\t', 'fi', 'rs', 't,', ' ', 'i ', 'sa', 'id', ', ', 'n', 'ic', 'od', 'em', 'us', ' i', 's ', 'af', 'te', 'r ', 'so', 'me', 'th', 'in', 'g ', 'po', 'we', 'rf', 'ul', '. ', 'i ', 'do', 'n', 't ', 'kn', 'ow', ' w', 'ha', 't ', 'it']\n",
      "\n",
      "Processing file dresden_files/313.txt\n",
      "Data size (Characters) (Document 312) 19100\n",
      "Sample string (Document 312) ['\\n\\t', '\\t\\t', 'as', 'su', 'mi', 'ng', ', ', 'of', ' c', 'ou', 'rs', 'e,', ' t', 'ha', 't ', 'my', ' r', 'ea', 'so', 'ni', 'ng', ' w', 'as', ' c', 'or', 're', 'ct', '. ', 'if', ' i', 't ', 'wa', 'sn', 't', ', ', 'i ', 'wa', 's ', 'ab', 'ou', 't ', 'to', ' t', 'ra', 'p ', 'my', 'se', 'lf', ' w', 'it']\n",
      "\n",
      "Processing file dresden_files/314.txt\n",
      "Data size (Characters) (Document 313) 18108\n",
      "Sample string (Document 313) ['\\n\\t', '\\t\\t', 'k', 'ee', 'p ', 'co', 'un', 'ti', 'ng', ',', ' i', ' s', 'ai', 'd.', ' ', 'it', ' m', 'ig', 'ht', ' m', 'at', 'te', 'r.', '\\n', '\\n\\t', '\\t\\t', 'bu', 'tt', 'er', 's ', 'no', 'dd', 'ed', '. ', 'f', 'ou', 'rt', 'ee', 'n.', ' f', 'if', 'te', 'en', '. ', 'si', 'xt', 'ee', 'n?', ' s', 'ix']\n",
      "\n",
      "Processing file dresden_files/315.txt\n",
      "Data size (Characters) (Document 314) 14372\n",
      "Sample string (Document 314) ['\\n\\t', '\\t\\t', 'a', 'nd', ' w', 'e ', 'ha', 've', ' a', 'n ', 'ex', 'tr', 'a,', ' ', 'va', 'lm', 'on', 't ', 'sa', 'id', ', ', 's', 'in', 'ce', ' g', 're', 'y ', 'di', 'dn', 't', ' w', 'an', 't ', 'a ', 'sh', 'ar', 'e.', '\\n', '\\n\\t', '\\t\\t', 'h', 'er', 'e', 's ', 'a ', 'br', 'ai', 'ns', 'to', 'rm']\n",
      "\n",
      "Processing file dresden_files/316.txt\n",
      "Data size (Characters) (Document 315) 1883\n",
      "Sample string (Document 315) ['\\n\\t', '\\t\\t', 't', 'he', 'n ', 'yo', 'u', 'll', ' u', 'nd', 'er', 'st', 'an', 'd ', 'th', 'is', '.', ' m', 'ic', 'ha', 'el', ' l', 'ea', 'ne', 'd ', 'hi', 's ', 'he', 'ad', ' b', 'ac', 'k ', 'an', 'd ', 'ca', 'll', 'ed', ', ', 'h', 'an', 'k!', '\\n', '\\n\\t', '\\t\\t', 'a ', 'mo', 'me', 'nt', ' l', 'at']\n",
      "\n",
      "Processing file dresden_files/317.txt\n",
      "Data size (Characters) (Document 316) 15637\n",
      "Sample string (Document 316) ['1\\n', '\\n\\n', 'my', ' b', 'ro', 'th', 'er', ' r', 'ui', 'ne', 'd ', 'a ', 'pe', 'rf', 'ec', 'tl', 'y ', 'go', 'od', ' r', 'un', ' b', 'y ', 'sa', 'yi', 'ng', ', ', 'j', 'us', 'ti', 'ne', ' i', 's ', 'pr', 'eg', 'na', 'nt', '.', '\\n\\n', 'th', 'at', ' k', 'ic', 'ke', 'd ', 'me', ' c', 'om', 'pl', 'et']\n",
      "\n",
      "Processing file dresden_files/318.txt\n",
      "Data size (Characters) (Document 317) 16158\n",
      "Sample string (Document 317) ['ec', 'to', 'pl', 'as', 'm ', 'wa', 's ', 'a ', 'st', 'ra', 'ng', 'e ', 'su', 'bs', 'ta', 'nc', 'e.', ' i', 't ', 'co', 'ul', 'd ', 'be', ' s', 'ha', 'pe', 'd ', 'by', ' m', 'ag', 'ic', ' a', 'nd', ' f', 'ed', ' e', 'ne', 'rg', 'y,', ' a', 'nd', ' a', 's ', 'lo', 'ng', ' a', 's ', 'th', 'e ', 'en']\n",
      "\n",
      "Processing file dresden_files/319.txt\n",
      "Data size (Characters) (Document 318) 17058\n",
      "Sample string (Document 318) ['s', 'ir', ',', ' i', ' s', 'ai', 'd.', '\\n\\n', 't', 'im', 'e ', 'to', ' g', 'o,', ' ', 'eb', 'en', 'ez', 'ar', ' s', 'ai', 'd,', ' h', 'is', ' v', 'oi', 'ce', ' w', 'ea', 'ry', '. ', 'w', 'or', 'k ', 'to', ' b', 'e ', 'do', 'ne', '.', '\\n\\n', 'an', 'd ', 'he', ' w', 'al', 'ke', 'd ', 'ou', 't.']\n",
      "\n",
      "Processing file dresden_files/320.txt\n",
      "Data size (Characters) (Document 319) 18285\n",
      "Sample string (Document 319) ['i ', 'di', 'dn', 't', ' m', 'ee', 't ', 'he', 'r ', 'ey', 'es', '. ', 'i', 'd ', 'se', 'en', ' t', 'he', ' k', 'in', 'd ', 'of', ' t', 'hi', 'ng', ' m', 'ab', ' w', 'ou', 'ld', ' d', 'o ', 'to', ' s', 'om', 'eo', 'ne', ' w', 'ho', ' m', 'er', 'el', 'y ', 'di', 'sp', 'le', 'as', 'ed', ' h', 'er']\n",
      "\n",
      "Processing file dresden_files/321.txt\n",
      "Data size (Characters) (Document 320) 22872\n",
      "Sample string (Document 320) ['mi', 'ch', 'ae', 'l', 's ', 'an', 'ge', 'li', 'c ', 'se', 'cu', 'ri', 'ty', ' a', 'ge', 'nc', 'y', 's ', 'on', 'ly', ' f', 'la', 'w ', 'wa', 's ', 'th', 'at', ' i', 't ', 'co', 'ul', 'd ', 'do', ' n', 'ot', 'hi', 'ng', ' t', 'o ', 'pr', 'ot', 'ec', 't ', 'hi', 'm ', 'an', 'd ', 'hi', 's ', 'fa']\n",
      "\n",
      "Processing file dresden_files/322.txt\n",
      "Data size (Characters) (Document 321) 20888\n",
      "Sample string (Document 321) ['y', 'ou', ' d', 'on', 't', ' w', 'an', 't ', 'to', ' k', 'no', 'w,', ' ', 'sa', 'id', ' t', 'he', ' o', 'ld', ' m', 'an', '.\\n', '\\ns', 'ud', 'de', 'nl', 'y,', ' t', 'hr', 'ee', ' o', 'f ', 'th', 'e ', 'co', 'rn', 'er', 'ho', 'un', 'ds', ' s', 'pe', 'ed', '-s', 'li', 'th', 'er', 'ed', ' c', 'lo']\n",
      "\n",
      "Processing file dresden_files/323.txt\n",
      "Data size (Characters) (Document 322) 21602\n",
      "Sample string (Document 322) ['\\n', 'we', 'll', ',', ' b', 'ut', 'te', 'rs', ' s', 'ai', 'd,', ' i', 'n ', 'th', 'e ', 'to', 'ne', ' o', 'f ', 'a ', 'ma', 'n ', 'ge', 'tt', 'in', 'g ', 'ba', 'ck', ' t', 'o ', 'bu', 'si', 'ne', 'ss', '. ', 't', 'he', ' p', 'ar', 'an', 'et', ' h', 'as', ' s', 'en', 't ', 'ou', 't ', 'ad', 'va']\n",
      "\n",
      "Processing file dresden_files/324.txt\n",
      "Data size (Characters) (Document 323) 18070\n",
      "Sample string (Document 323) ['\\n', 'yo', 'sh', 'im', 'o ', 'is', ' j', 'us', 't ', 'ga', 'th', 'er', 'in', 'g ', 'ev', 'id', 'en', 'ce', ',', ' r', 'am', 'ir', 'ez', ' s', 'ai', 'd.', ' ', 'sh', 'e ', 'is', 'n', 't ', 'go', 'in', 'g ', 'to', ' h', 'ur', 't ', 'yo', 'u.', '\\n', '\\ni', ' c', 'le', 'nc', 'he', 'd ', 'my', ' j']\n",
      "\n",
      "Processing file dresden_files/325.txt\n",
      "Data size (Characters) (Document 324) 14918\n",
      "Sample string (Document 324) ['\\nm', 'ic', 'ha', 'el', ' w', 'as', ' j', 'us', 't ', 'be', 'tt', 'er', ' a', 't ', 'th', 'is', ' k', 'in', 'd ', 'of', ' t', 'al', 'k ', 'th', 'an', ' m', 'e.', ' i', ' g', 'lo', 'we', 're', 'd ', 'at', ' h', 'im', ' a', 'nd', ' t', 'he', 'n ', 'sn', 'ee', 're', 'd ', 'in', ' c', 'on', 'ce', 'ss']\n",
      "\n",
      "Processing file dresden_files/326.txt\n",
      "Data size (Characters) (Document 325) 20388\n",
      "Sample string (Document 325) ['w', 'ha', 't ', 'do', ' y', 'ou', ' w', 'an', 't ', 'us', ' t', 'o ', 'do', ', ', 'ha', 'rr', 'y?', ' ', 'bu', 'tt', 'er', 's ', 'as', 'ke', 'd.', '\\n\\n', 'i ', 'th', 'ou', 'gh', 't ', 'ab', 'ou', 't ', 'it', ' f', 'or', ' a', ' s', 'ec', 'on', 'd ', 'an', 'd ', 'th', 'en', ' p', 'ut', ' a', ' h']\n",
      "\n",
      "Processing file dresden_files/327.txt\n",
      "Data size (Characters) (Document 326) 14188\n",
      "Sample string (Document 326) ['\\nr', 'iv', 'er', ' n', 'od', 'de', 'd ', 'an', 'd ', 'wi', 'th', 'dr', 'ew', ' h', 'is', ' h', 'an', 'd ', 'ca', 're', 'fu', 'll', 'y.', ' ', 'li', 'st', 'en', ' u', 'p.', ' g', 're', 'nd', 'el', ' w', 'as', ' a', ' b', 'ad', ' e', 'gg', '. ', 'sp', 'aw', 'ne', 'd ', 'mo', 'st', 'ly', ' b', 'ad']\n",
      "\n",
      "Processing file dresden_files/328.txt\n",
      "Data size (Characters) (Document 327) 15234\n",
      "Sample string (Document 327) ['t', 'ak', 'in', 'g ', 'yo', 'u ', 'to', ' a', ' h', 'ot', ' b', 'at', 'h,', ' ', 'i ', 'sa', 'id', '. ', 'd', 'on', 't', ' t', 'ry', ' t', 'o ', 'mo', 've', '. ', 'ju', 'st', ' ', ' l', 'et', ' m', 'e ', 'do', ' i', 't.', ' o', 'ka', 'y?', '\\n', '\\nh', 'er', ' b', 'lu', 'e ', 'ey', 'es', ' w']\n",
      "\n",
      "Processing file dresden_files/329.txt\n",
      "Data size (Characters) (Document 328) 17347\n",
      "Sample string (Document 328) ['b', 'ut', ' s', 'he', ' d', 'oe', 'sn', 't', ' k', 'no', 'w,', ' ', 'la', 'ra', ' p', 're', 'ss', 'ed', '.\\n', '\\n', 'sh', 'e ', 'lo', 'an', 'ed', ' m', 'e ', 'to', ' y', 'ou', ' s', 'o ', 'th', 'at', ' s', 'he', ' w', 'ou', 'ld', 'n', 't ', 'ha', 've', ' t', 'o ', 'kn', 'ow', '.', '\\n\\n', 'la']\n",
      "\n",
      "Processing file dresden_files/330.txt\n",
      "Data size (Characters) (Document 329) 20881\n",
      "Sample string (Document 329) ['bl', 'ac', 'k ', 'wi', 'do', 'w ', 'sp', 'id', 'er', 's ', 'wi', 'th', ' b', 'od', 'ie', 's ', 'th', 'e ', 'si', 'ze', ' o', 'f ', 'ba', 'sk', 'et', 'ba', 'll', 's ', 'ca', 'me', ' b', 'oi', 'li', 'ng', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'sh', 'af', 't ', 'be', 'hi', 'nd', ' m', 'e', 'fi', 've']\n",
      "\n",
      "Processing file dresden_files/331.txt\n",
      "Data size (Characters) (Document 330) 21080\n",
      "Sample string (Document 330) ['\\ni', 't ', 'ca', 'me', ' t', 'o ', 'a ', 'ha', 'lt', ' a', 't ', 'th', 'e ', 'fo', 'ot', ' o', 'f ', 'th', 'e ', 'da', 'is', ' w', 'he', 're', ' t', 'he', ' h', 'ig', 'h ', 'se', 'at', ' s', 'to', 'od', '.\\n', '\\ni', 't ', 'wa', 's ', 'a ', 've', 'ry', ' s', 'ma', 'll', ' s', 'ev', 'er', 'ed', ' h']\n",
      "\n",
      "Processing file dresden_files/332.txt\n",
      "Data size (Characters) (Document 331) 14267\n",
      "Sample string (Document 331) ['\\nb', 'eh', 'in', 'd ', 'me', ' w', 'er', 'e ', 'tw', 'o ', 'qu', 'ie', 't ', 'th', 'um', 'ps', ', ', 'as', ' l', 'in', 'es', ' w', 'er', 'e ', 'dr', 'op', 'pe', 'd ', 'on', ' t', 'he', ' d', 'ec', 'k ', 'of', ' t', 'he', ' s', 'hi', 'p.', '\\n\\n', 'i', ' k', 'no', 'w ', 'en', 'ou', 'gh', ' t', 'o ']\n",
      "\n",
      "Processing file dresden_files/333.txt\n",
      "Data size (Characters) (Document 332) 17809\n",
      "Sample string (Document 332) ['\\ni', ' h', 'uf', 'fe', 'd ', 'ou', 't ', 'pa', 'rt', ' o', 'f ', 'a ', 'la', 'ug', 'h.', ' a', 'nd', ' w', 'e ', 'st', 'oo', 'd ', 'to', 'ge', 'th', 'er', '.\\n', '\\n\\n', '\\n\\n', '\\n3', '4\\n', '\\n\\n', 'th', 'e ', 'li', 'tt', 'le', ' c', 'he', 'ap', ' p', 'la', 'st', 'ic', ' c', 'om', 'pa', 'ss', ' s', 'wu']\n",
      "\n",
      "Processing file dresden_files/334.txt\n",
      "Data size (Characters) (Document 333) 3434\n",
      "Sample string (Document 333) ['\\n', 'yo', 'u ', 'wi', 'sh', ' t', 'wo', ' o', 'f ', 'th', 'e ', 'we', 'ap', 'on', 's?', '\\n', '\\na', 'lf', 're', 'd ', 'so', 'un', 'de', 'd ', ' ', 'sl', 'ig', 'ht', 'ly', ' i', 'nt', 'im', 'id', 'at', 'ed', '.\\n', '\\nt', 'ha', 't', 's ', 'th', 'e ', 'ki', 'nd', ' o', 'f ', 'po', 'we', 'r ', 'le']\n",
      "\n",
      "Processing file dresden_files/335.txt\n",
      "Data size (Characters) (Document 334) 19848\n",
      "Sample string (Document 334) ['\\n\\n', 'ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\n\\n', '\\na', 'po', 'ca', 'ly', 'ps', 'es', ' a', 'lw', 'ay', 's ', 'ki', 'ck', ' o', 'ff', ' a', 't ', 'th', 'e ', 'wi', 'tc', 'hi', 'ng', ' h', 'ou', 'r.', '\\n\\n', '\\t\\t', '\\tt', 'ha', 't', 's ', 'so', 'me', 'th', 'in', 'g ', 'yo', 'u ', 'kn', 'ow', ' n', 'ow']\n",
      "\n",
      "Processing file dresden_files/336.txt\n",
      "Data size (Characters) (Document 335) 16874\n",
      "Sample string (Document 335) ['\\n\\n', '\\t\\t', '\\t*', ' *', ' *', '\\n\\n', '\\t\\t', '\\tm', 'ur', 'ph', 'y ', 'gr', 'ip', 'pe', 'd ', 'th', 'e ', 'ed', 'ge', 's ', 'of', ' t', 'he', ' s', 'ho', 'pp', 'in', 'g ', 'ca', 'rt', ' w', 'it', 'h ', 'bo', 'th', ' h', 'an', 'ds', ' a', 's ', 'i ', 'ra', 'n,', ' p', 'us', 'hi', 'ng', ' i', 't ', 'do']\n",
      "\n",
      "Processing file dresden_files/337.txt\n",
      "Data size (Characters) (Document 336) 17224\n",
      "Sample string (Document 336) ['\\t\\t', '\\tl', 'ik', 'e ', 'i ', 'sa', 'id', '. ', 'th', 'e ', 'on', 'ly', ' m', 'ir', 'ro', 'rs', ' w', 'e ', 'ha', 've', ' a', 're', ' o', 'th', 'er', ' p', 'eo', 'pl', 'e.', '\\n\\n', '\\t\\t', '\\ti', ' d', 'id', 'n', 't ', 'st', 'ar', 'e ', 'at', ' b', 'ra', 'dl', 'ey', 's', ' s', 'ha', 'do', 'we', 'd ']\n",
      "\n",
      "Processing file dresden_files/338.txt\n",
      "Data size (Characters) (Document 337) 16890\n",
      "Sample string (Document 337) ['\\t\\t', '\\t', 'a ', 'un', 'iq', 'ue', ' a', 'll', 'oy', ' o', 'f ', 'ol', 'ym', 'pi', 'an', ' b', 'ro', 'nz', 'e ', 'an', 'd ', 'mo', 'rd', 'it', 'e,', ' ', 'va', 'dd', 'er', 'un', 'g ', 're', 'pl', 'ie', 'd.', ' ', 'ki', 'ne', 'ti', 'c ', 'we', 'ap', 'on', 's ', 'wi', 'll', ' b', 'e ', 'of', ' v']\n",
      "\n",
      "Processing file dresden_files/339.txt\n",
      "Data size (Characters) (Document 338) 19087\n",
      "Sample string (Document 338) ['\\t\\t', '\\t', 'it', 's', ' d', 'an', 'ge', 'ro', 'us', ',', ' i', ' s', 'ai', 'd.', ' ', 'i ', 'kn', 'ow', ' t', 'ha', 't.', ' i', ' w', 'en', 't ', 'in', ' w', 'it', 'h ', 'my', ' e', 'ye', 's ', 'op', 'en', '. ', 'if', ' m', 'ab', ' c', 'om', 'pr', 'om', 'is', 'es', ' m', 'y ', 'fr', 'ee', ' w']\n",
      "\n",
      "Processing file dresden_files/340.txt\n",
      "Data size (Characters) (Document 339) 16673\n",
      "Sample string (Document 339) ['\\t\\t', '\\th', 'e ', 'st', 'ar', 'ed', ' o', 'ut', ' a', 't ', 'th', 'e ', 'ni', 'gh', 't ', 'an', 'd ', 'th', 'e ', 'fl', 'am', 'es', ' a', 'nd', ' t', 'he', ' s', 'mo', 'ke', ', ', 'an', 'd ', 'at', ' t', 'he', ' r', 'em', 'ai', 'ns', ' o', 'n ', 'hi', 's ', 'fr', 'on', 't ', 'la', 'wn', '. ', 'he']\n",
      "\n",
      "Processing file dresden_files/341.txt\n",
      "Data size (Characters) (Document 340) 18914\n",
      "Sample string (Document 340) ['\\t\\t', '\\tf', 'iv', 'e ', 'of', ' t', 'he', ' o', 'th', 'er', ' s', 'ix', ' f', 'ig', 'ur', 'es', ' t', 'ur', 'ne', 'd ', 'to', ' f', 'ac', 'e ', 'us', ', ', 'ho', 'od', 's ', 'co', 'mi', 'ng', ' d', 'ow', 'n.', '\\n\\n', '\\t\\t', '\\tb', 'la', 'ck', ' c', 'ou', 'rt', ' v', 'am', 'pi', 're', 's.', ' a', 'll']\n",
      "\n",
      "Processing file dresden_files/342.txt\n",
      "Data size (Characters) (Document 341) 19542\n",
      "Sample string (Document 341) ['\\t\\t', '\\t', 'ag', 're', 'ed', ',', ' l', 'is', 'te', 'ns', '-t', 'o-', 'wi', 'nd', ' s', 'ai', 'd.', '\\n\\n', '\\t\\t', '\\t', 'yo', 'u', 're', ' j', 'us', 't ', 'go', 'in', 'g ', 'to', ' k', 'ee', 'p ', 'go', 'in', 'g,', ' ', 'ra', 'mi', 're', 'z ', 'sa', 'id', '. ', 'hi', 's ', 'vo', 'ic', 'e ', 'sh']\n",
      "\n",
      "Processing file dresden_files/343.txt\n",
      "Data size (Characters) (Document 342) 16433\n",
      "Sample string (Document 342) ['\\t\\t', '\\to', 'ct', 'ok', 'on', 'gs', ' t', 'um', 'bl', 'ed', ' f', 'ro', 'm ', 'th', 'e ', 'wa', 'll', 's,', ' w', 'ou', 'nd', 'ed', ', ', 'st', 'un', 'ne', 'd,', ' s', 'om', 'e ', 'of', ' t', 'he', 'm ', 'dy', 'in', 'g.', '\\n\\n', '\\t\\t', '\\ts', 'an', 'ya', ' l', 'et', ' o', 'ut', ' a', ' r', 'oa', 'r ']\n",
      "\n",
      "Processing file dresden_files/344.txt\n",
      "Data size (Characters) (Document 343) 17851\n",
      "Sample string (Document 343) ['\\t\\t', '\\tt', 'hi', 's ', 'th', 'in', 'g ', '. ', '. ', '.\\n', '\\n\\t', '\\t\\t', 'po', 'we', 'r ', 'ra', 'di', 'at', 'ed', ' f', 'ro', 'm ', 'it', '. ', 'it', ' w', 'as', ' t', 'he', ' s', 'iz', 'e ', 'of', ' a', ' b', 'ud', 'we', 'is', 'er', ' h', 'or', 'se', ', ', 'pl', 'us', ' a', 'n ', 'ex', 'tr', 'a ']\n",
      "\n",
      "Processing file dresden_files/345.txt\n",
      "Data size (Characters) (Document 344) 13876\n",
      "Sample string (Document 344) ['\\t\\t', '\\ts', 'hr', 'ie', 'ks', ' a', 'nd', ' y', 'ow', 'ls', ' b', 'ur', 'st', ' o', 'ut', ' a', 'ro', 'un', 'd ', 'us', ', ', 'as', ' w', 'in', 'te', 'r ', 'la', 'un', 'ch', 'ed', ' a', 'n ', 'at', 'ta', 'ck', ' u', 'po', 'n ', 'en', 'em', 'y ', 'fo', 'rc', 'es', '. ', 'hu', 'nt', 'sm', 'en', ' h']\n",
      "\n",
      "Processing file dresden_files/346.txt\n",
      "Data size (Characters) (Document 345) 13315\n",
      "Sample string (Document 345) ['ch', 'ap', 'te', 'r\\n', '\\n\\n', '\\n\\n', '\\nt', 'we', 'nt', 'y-', 'th', 're', 'e\\n', '\\n\\n', '\\nh', 'at', 'e ', 'is', ' c', 'om', 'fo', 'rt', 'in', 'g.', '\\n\\n', '\\t\\t', '\\th', 'at', 'e ', 'is', ' p', 'ur', 'e.', '\\n\\n', '\\t\\t', '\\tt', 'he', 're', ' a', 're', 'n', 't ', 'an', 'y ', 'qu', 'es', 'ti', 'on', 's,', ' a']\n",
      "\n",
      "Processing file dresden_files/347.txt\n",
      "Data size (Characters) (Document 346) 17319\n",
      "Sample string (Document 346) ['\\n\\n', 'i ', 'wa', 'lk', 'ed', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'be', 'an', ' a', 'nd', ' i', 'nt', 'o ', 'th', 'e ', 'so', 'un', 'dt', 'ra', 'ck', ' o', 'f ', 'a ', 'b ', 'ho', 'rr', 'or', ' m', 'ov', 'ie', ': ', 'th', 'e ', 'fo', 'mo', 'r ', 'fo', 'rc', 'es', ' d', 'id', 'n', 't ', 'us', 'e ']\n",
      "\n",
      "Processing file dresden_files/348.txt\n",
      "Data size (Characters) (Document 347) 18725\n",
      "Sample string (Document 347) ['\\t\\t', '\\tc', 'or', 'b ', 'ma', 'de', ' a', ' s', 'pu', 'tt', 'er', 'in', 'g ', 'so', 'un', 'd ', 'bu', 't ', 'di', 'dn', 't', ' s', 'pe', 'ak', '. ', 'wh', 'ic', 'h ', 'sh', 'ow', 'ed', ' h', 'e ', 'ha', 'd ', 'at', ' l', 'ea', 'st', ' a', ' f', 'ew', ' b', 'ra', 'in', 's.', '\\n\\n', '\\t\\t', '\\t', 'th']\n",
      "\n",
      "Processing file dresden_files/349.txt\n",
      "Data size (Characters) (Document 348) 19986\n",
      "Sample string (Document 348) ['\\t\\t', '\\th', 'er', ' h', 'an', 'ds', ' f', 'la', 'sh', 'ed', ' o', 'ut', ' a', 'nd', ' s', 'ei', 'ze', 'd ', 'bu', 'tt', 'er', 's', 's ', 'wh', 'it', 'e ', 'cl', 'oa', 'k.', '\\n\\n', '\\t\\t', '\\tt', 'he', ' k', 'ni', 'gh', 't ', 'st', 'if', 'fe', 'ne', 'd.', ' j', 'ud', 'gi', 'ng', ' b', 'y ', 'hi', 's ']\n",
      "\n",
      "Processing file dresden_files/350.txt\n",
      "Data size (Characters) (Document 349) 20347\n",
      "Sample string (Document 349) ['\\t\\t', '\\tb', 'ut', ' h', 'er', 'e ', 'we', ' a', 're', '.\\n', '\\n\\t', '\\t\\t', 'th', 'er', 'e ', 'wa', 's ', 'an', ' e', 'no', 'rm', 'ou', 's ', 'so', 'un', 'd,', ' a', ' f', 'la', 'sh', ' o', 'f ', 'li', 'gh', 't,', ' a', ' s', 'ho', 'ck', ' a', 'ga', 'in', 'st', ' m', 'y ', 'bo', 'dy', ' l', 'ik', 'e ']\n",
      "\n",
      "Processing file dresden_files/351.txt\n",
      "Data size (Characters) (Document 350) 23040\n",
      "Sample string (Document 350) ['\\t\\t', '\\tw', 'he', 'n ', 'th', 'e ', 'sw', 'or', 'd ', 'of', ' f', 'ai', 'th', ' s', 'tr', 'uc', 'k ', 'th', 'e ', 'so', 'ld', 'ie', 'rs', ' o', 'f ', 'th', 'e ', 'fo', 'mo', 'r,', ' t', 'he', ' s', 'la', 've', 's ', 'of', ' t', 'he', ' t', 'it', 'an', 's', ' w', 'il', 'l,', ' i', 't ', 'di', 'd ']\n",
      "\n",
      "Processing file dresden_files/352.txt\n",
      "Data size (Characters) (Document 351) 16525\n",
      "Sample string (Document 351) ['\\t\\t', '\\t', 'sh', 'e ', 'wa', 'nt', 's ', 'th', 'e ', 'ey', 'e!', ' ', 'la', 'ra', ' s', 'cr', 'ea', 'me', 'd.', ' ', 'go', ', ', 'ha', 'rr', 'y!', '\\n', '\\n\\t', '\\t\\t', 'th', 'e ', 'sp', 'ea', 'r ', 'fe', 'lt', ' h', 'ea', 'vy', ' i', 'n ', 'my', ' h', 'an', 'ds', '. ', 'it', ' s', 'ti', 'll', ' h']\n",
      "\n",
      "Processing file dresden_files/353.txt\n",
      "Data size (Characters) (Document 352) 15339\n",
      "Sample string (Document 352) ['\\t\\t', '\\ti', 't ', 'wa', 's ', 'as', ' i', 'f ', 'i ', 'ha', 'd ', 'st', 'ar', 'te', 'd ', 'so', 'me', ' v', 'as', 't ', 'an', 'd ', 'mo', 'me', 'nt', 'ou', 's ', 'en', 'gi', 'ne', '.\\n', '\\n\\t', '\\t\\t', 'a', 'lf', 're', 'd!', ' ', 'i ', 'sc', 're', 'am', 'ed', ', ', 'an', 'd ', 'ki', 'ck', 'ed', ' t']\n",
      "\n",
      "Processing file dresden_files/354.txt\n",
      "Data size (Characters) (Document 353) 14917\n",
      "Sample string (Document 353) ['\\t\\t', '\\ti', ' t', 'ho', 'ug', 'ht', ' o', 'f ', 'th', 'e ', 'is', 'la', 'nd', ', ', 'di', 'st', 'ur', 'be', 'd ', 'at', ' t', 'he', ' g', 're', 'at', ' p', 'ow', 'er', 's ', 'ex', 'pe', 'nd', 'ed', ' t', 'ha', 't ', 'ni', 'gh', 't.', '\\n\\n', '\\t\\t', '\\tt', 'he', ' l', 'as', 't ', 'th', 'in', 'g ', 'i ']\n",
      "\n",
      "Processing file dresden_files/355.txt\n",
      "Data size (Characters) (Document 354) 17539\n",
      "Sample string (Document 354) ['\\t\\t', '\\tb', 'ac', 'k ', 'at', ' t', 'he', ' c', 'ar', ', ', 'mi', 'ch', 'ae', 'l ', 'sa', 'id', ', ', 't', 'ha', 't ', 'lo', 'ok', 'ed', ' g', 'ri', 'm.', ' w', 'ha', 't ', 'ha', 'pp', 'en', 'ed', '?', '\\n\\n', '\\t\\t', '\\t', 're', 'st', ' o', 'f ', 'th', 'e ', 'wh', 'it', 'e ', 'co', 'un', 'ci', 'l ']\n",
      "\n",
      "Processing file dresden_files/356.txt\n",
      "Data size (Characters) (Document 355) 14448\n",
      "Sample string (Document 355) ['\\t\\t', '\\tm', 'ab', ' l', 'oo', 'ke', 'd ', 'fr', 'om', ' m', 'ol', 'ly', ' t', 'o ', 'la', 'ra', ' a', 'nd', ' b', 'ac', 'k.', '\\n\\n', '\\t\\t', '\\tm', 'ol', 'ly', ' l', 'oo', 'ke', 'd ', 'li', 'ke', ' s', 'he', ' w', 'as', ' b', 'it', 'in', 'g ', 'ba', 'ck', ' a', ' w', 'ho', 'le', ' l', 'ot', ' o', 'f ']\n",
      "\n",
      "Processing file dresden_files/357.txt\n",
      "Data size (Characters) (Document 356) 14819\n",
      "Sample string (Document 356) ['\\t\\t', '\\t', 'do', ' n', 'ot', ' b', 'e ', 'ti', 're', 'so', 'me', ',', ' m', 'ab', ' s', 'ig', 'he', 'd.', '\\n\\n', '\\t\\t', '\\t', 'fa', 'er', 'ie', 's ', 'do', 'n', 't ', 'gi', 've', ' g', 'if', 'ts', ',', ' i', ' s', 'ai', 'd.', ' ', 'wh', 'at', ' k', 'in', 'd ', 'of', ' t', 'ri', 'ck', ' i', 's ']\n",
      "\n",
      "Processing file dresden_files/358.txt\n",
      "Data size (Characters) (Document 357) 18918\n",
      "Sample string (Document 357) ['\\t\\t', '\\th', 'ar', 'ry', ' d', 're', 'sd', 'en', ', ', 'pr', 'of', 'es', 'si', 'on', 'al', ' w', 'iz', 'ar', 'd.', ' i', 'm', ' a', ' l', 'it', 'tl', 'e ', 'bu', 'sy', ' o', 'r ', 'i', 'd ', 'sh', 'ak', 'e ', 'ha', 'nd', 's.', '\\n\\n', '\\t\\t', '\\ti', ' p', 'ul', 'le', 'd ', 'ha', 'rd', ' w', 'it', 'h ']\n",
      "\n",
      "Processing file dresden_files/359.txt\n",
      "Data size (Characters) (Document 358) 15767\n",
      "Sample string (Document 358) ['\\t\\t', '\\t', 'li', 'tt', 'le', ' a', ',', ' i', ' s', 'ai', 'd ', 'in', ' p', 'ro', 'te', 'st', ', ', 'th', 'en', ' c', 'la', 'ri', 'fi', 'ed', '. ', 't', 'he', ' f', 'om', 'or', ', ', 'th', 'os', 'e ', 'ki', 'dn', 'ap', 'pi', 'ng', ' b', 'as', 'ta', 'rd', 's,', ' a', 're', ' c', 'om', 'in', 'g ']\n",
      "\n",
      "Processing file dresden_files/360.txt\n",
      "Data size (Characters) (Document 359) 18988\n",
      "Sample string (Document 359) ['\\t\\t', '\\ti', 'nj', 'ur', 'ie', 's ', 'i', 'd ', 'de', 'ci', 'de', 'd ', 'he', ' n', 'ee', 'de', 'd ', 'to', ' h', 'av', 'e.', '\\n\\n', '\\t\\t', '\\tr', 'ig', 'ht', ' b', 'ef', 'or', 'e ', 'i', 'd ', 'li', 'ed', ' t', 'o ', 'hi', 'm.', '\\n\\n', '\\t\\t', '\\td', 'am', 'mi', 't.', '\\n\\n', '\\t\\t', '\\ti', ' f', 'el', 't ']\n",
      "\n",
      "Processing file dresden_files/361.txt\n",
      "Data size (Characters) (Document 360) 16433\n",
      "Sample string (Document 360) ['\\t\\t', '\\th', 'e ', 'wo', 're', ' a', ' f', 'ul', 'l ', 'su', 'it', ' o', 'f ', 'go', 'th', 'ic', ' p', 'la', 'te', ' a', 'rm', 'or', ', ', 'ma', 'de', ' o', 'f ', 'so', 'me', ' w', 'ei', 'rd', '-l', 'oo', 'ki', 'ng', ' a', 'll', 'oy', ' c', 'ol', 'or', 'ed', ' a', ' d', 'ee', 'p,', ' a', 'lm', 'os']\n",
      "\n",
      "Processing file dresden_files/362.txt\n",
      "Data size (Characters) (Document 361) 17940\n",
      "Sample string (Document 361) ['\\t\\t', '\\ti', ' l', 'oo', 'ke', 'd ', 'do', 'wn', ' a', 't ', 'th', 'e ', 'sh', 'ad', 'ow', ' b', 'ei', 'ng', ' c', 'as', 't ', 'in', ' f', 'ro', 'nt', ' o', 'f ', 'me', '. ', 'th', 'e ', 'lo', 'ng', ', ', 'bi', 'll', 'ow', 'in', 'g ', 'ou', 'tl', 'in', 'e ', 'of', ' t', 'he', ' d', 'us', 'te', 'r.']\n",
      "\n",
      "Processing file dresden_files/363.txt\n",
      "Data size (Characters) (Document 362) 19343\n",
      "Sample string (Document 362) ['\\n\\n', '* ', '* ', '*\\n', '\\n\\n', '\\n\\t', '\\t\\t', '* ', '* ', '*\\n', '\\n\\t', '\\t\\t', 'th', 'e ', 'ba', 'tt', 'le', ' w', 'as', ' a', ' h', 'el', 'l ', 'of', ' a', ' t', 'hi', 'ng', '. ', 'i ', 'co', 'ul', 'd ', 'he', 'ar', ' i', 't ', 'ha', 'pp', 'en', 'in', 'g ', 'ar', 'ou', 'nd', ' m', 'e.', ' i', ' c', 'ou']\n",
      "\n",
      "Processing file dresden_files/364.txt\n",
      "Data size (Characters) (Document 363) 17686\n",
      "Sample string (Document 363) ['\\t\\t', '\\ti', ' w', 'hi', 'pp', 'ed', ' m', 'y ', 'st', 'il', 'l-', 'gl', 'ow', 'in', 'g ', 'st', 'af', 'f ', 'to', 'wa', 'rd', ' r', 'iv', 'er', ' s', 'ho', 'ul', 'de', 'rs', ' a', 'nd', ' t', 'hi', 's ', 'ti', 'me', ' u', 'nl', 'ea', 'sh', 'ed', ' m', 'y ', 'wi', 'll', ' a', 'lo', 'ng', ' w', 'it']\n",
      "\n",
      "Processing file dresden_files/365.txt\n",
      "Data size (Characters) (Document 364) 19034\n",
      "Sample string (Document 364) ['\\t\\t', '\\ti', ' t', 'ur', 'ne', 'd ', 'to', ' s', 'ee', ' m', 'y ', 'gr', 'an', 'df', 'at', 'he', 'r ', 'on', ' t', 'he', ' s', 'ec', 'on', 'd ', 'le', 've', 'l ', 'of', ' t', 'he', ' p', 'ar', 'ki', 'ng', ' g', 'ar', 'ag', 'e,', ' w', 'av', 'in', 'g ', 'at', ' m', 'e.', ' h', 'e ', 'be', 'ck', 'on']\n",
      "\n",
      "Processing file dresden_files/366.txt\n",
      "Data size (Characters) (Document 365) 17149\n",
      "Sample string (Document 365) ['\\t\\t', '\\t.', ' .', ' .', ' b', 'ui', 'ld', 'in', 'gs', ' w', 'er', 'e ', 'on', ' f', 'ir', 'e.', ' b', 'la', 'ck', ' s', 'mo', 'ke', ' p', 'ou', 're', 'd ', 'ou', 't ', 'of', ' t', 'he', 'm.', ' a', 'n ', 'ol', 'd ', 'wo', 'ma', 'n ', 'st', 'oo', 'd ', 'in', ' t', 'he', ' s', 'tr', 'ee', 't ', 'in']\n",
      "\n",
      "Processing file dresden_files/367.txt\n",
      "Data size (Characters) (Document 366) 17260\n",
      "Sample string (Document 366) ['\\t\\t', '\\tm', 'ay', 'be', ' t', 'ha', 't', 's ', 'as', ' c', 'lo', 'se', ' t', 'o ', 'be', 'in', 'g ', 'pr', 'ot', 'ec', 'ti', 've', ' a', 's ', 'wi', 'nt', 'er', ' g', 'et', 's.', ' b', 'ut', ' i', 't ', 'wa', 's ', 'ha', 'rd', ' a', 'nd', ' c', 'ol', 'd ', 'an', 'd ', 're', 'al', '.\\n', '\\n\\t', '\\t\\t']\n",
      "\n",
      "Processing file dresden_files/368.txt\n",
      "Data size (Characters) (Document 367) 12862\n",
      "Sample string (Document 367) ['\\t\\t', '\\ti', ' p', 'oi', 'nt', 'ed', ' m', 'y ', 'st', 'af', 'f ', 'to', ' o', 'ne', ' s', 'id', 'e,', ' f', 'oc', 'us', 'ed', ' m', 'y ', 'wi', 'll', ', ', 'an', 'd ', 'sc', 're', 'am', 'ed', ', ', 'f', 'or', 'za', 're', '!', '\\n\\n', '\\t\\t', '\\te', 've', 'n ', 'ma', 'gi', 'c ', 'ca', 'n', 't ', 'es']\n",
      "\n",
      "Processing file dresden_files/369.txt\n",
      "Data size (Characters) (Document 368) 15394\n",
      "Sample string (Document 368) ['\\t\\t', '\\tw', 'hi', 'le', ' s', 'ir', ' w', 'al', 'do', ', ', 'th', 'e ', 'kn', 'ig', 'ht', ' o', 'f ', 'fa', 'it', 'h,', ' s', 'to', 'od', ' o', 've', 'r ', 'me', ' w', 'it', 'h ', 'th', 'at', ' b', 'la', 'zi', 'ng', ' s', 'wo', 'rd', ', ', 'be', 'tw', 'ee', 'n ', 'me', ' a', 'nd', ' r', 'ud', 'ol']\n",
      "\n",
      "Processing file dresden_files/370.txt\n",
      "Data size (Characters) (Document 369) 15348\n",
      "Sample string (Document 369) ['\\t\\t', '\\tb', 'ut', 'te', 'rs', ' h', 'ad', ' h', 'ac', 'ke', 'd ', 'th', 'ro', 'ug', 'h ', 'th', 'e ', 'br', 'id', 'ge', ', ', 'bu', 't ', 'th', 'e ', 'th', 'in', 'g ', 'ha', 'dn', 't', ' f', 'al', 'le', 'n ', 'ye', 't.', ' h', 'e ', 'da', 'sh', 'ed', ' t', 'we', 'nt', 'y ', 'fe', 'et', ' b', 'ac']\n",
      "\n",
      "Processing file dresden_files/371.txt\n",
      "Data size (Characters) (Document 370) 19972\n",
      "Sample string (Document 370) ['\\t\\t', '\\t', 'ir', 're', 'le', 'va', 'nt', ',', ' m', 'ab', ' r', 'ep', 'li', 'ed', '.\\n', '\\n\\t', '\\t\\t', 'in', ' t', 'he', ' s', 'ha', 'do', 'ws', ' o', 'f ', 'th', 'e ', 'be', 'an', ' b', 'eh', 'in', 'd ', 'us', ' g', 'at', 'he', 're', 'd ', 'th', 'e ', 'ma', 'lk', 's ', 'an', 'd ', 'bl', 'ac', 'k ']\n",
      "\n",
      "Processing file dresden_files/372.txt\n",
      "Data size (Characters) (Document 371) 21323\n",
      "Sample string (Document 371) ['\\t\\t', '\\ti', ' d', 'o ', 'no', 't ', 'kn', 'ow', ' w', 'ha', 't ', 'po', 'we', 'r ', 'sh', 'e ', 'ha', 'd ', 'wo', 'n,', ' w', 'ha', 't ', 'kn', 'ow', 'le', 'dg', 'e ', 'sh', 'e ', 'ha', 'd ', 'ga', 'in', 'ed', ', ', 'wh', 'at', ' e', 'xp', 'er', 'ie', 'nc', 'e ', 'sh', 'e ', 'ha', 'd ', 'su', 'ff']\n",
      "\n",
      "Processing file dresden_files/373.txt\n",
      "Data size (Characters) (Document 372) 24278\n",
      "Sample string (Document 372) ['\\t\\t', '\\ti', ' c', 'ou', 'ld', 'n', 't ', 'se', 'e,', ' t', 'hr', 'ou', 'gh', ' t', 'he', ' a', 'rm', 'ie', 's ', 'an', 'd ', 'th', 'e ', 'pa', 'rk', ' a', 'nd', ' t', 'he', ' s', 'mo', 'ke', ', ', 'wh', 'at', ' w', 'as', ' h', 'ap', 'pe', 'ni', 'ng', ' t', 'o ', 'th', 'e ', 'so', 'ut', 'h.', ' b']\n",
      "\n",
      "Processing file dresden_files/374.txt\n",
      "Data size (Characters) (Document 373) 17025\n",
      "Sample string (Document 373) ['\\t\\t', '\\te', 'th', 'ni', 'u ', 'lu', 'rc', 'he', 'd,', ' h', 'er', ' f', 'oo', 't ', 'no', 't ', 'bl', 'ee', 'di', 'ng', ', ', 'bu', 't ', 'br', 'ut', 'al', 'ly', ' b', 'ro', 'ke', 'n ', 'an', 'd ', 'no', ' l', 'on', 'ge', 'r ', 'su', 'pp', 'or', 'ti', 'ng', ' h', 'er', ' w', 'ei', 'gh', 't', 'an']\n",
      "\n",
      "Processing file dresden_files/375.txt\n",
      "Data size (Characters) (Document 374) 16126\n",
      "Sample string (Document 374) ['\\t\\t', '\\t', 'he', 'll', 's', ' b', 'el', 'ls', ',', ' i', ' s', 'pa', 't.', '\\n\\n', '\\t\\t', '\\t', 'fo', 'cu', 's,', ' ', 'he', ' s', 'na', 'rl', 'ed', '. ', 'i', ' k', 'no', 'w ', 'it', ' h', 'ur', 'ts', '. ', 'i ', 'kn', 'ow', ' w', 'ha', 't ', 'yo', 'u', 've', ' l', 'os', 't.', ' i', ' k', 'no']\n",
      "\n",
      "Processing file dresden_files/376.txt\n",
      "Data size (Characters) (Document 375) 15351\n",
      "Sample string (Document 375) ['\\t\\t', '\\tw', 'hu', 'pw', 'hu', 'pw', 'hu', 'pw', 'hu', 'pw', 'hu', 'pw', 'hu', 'pw', 'hu', 'pw', 'hu', 'p.', '\\n\\n', '\\t\\t', '\\tw', 'e', 'd ', 'al', 'l ', 'he', 'ar', 'd ', 'ch', 'op', 'pe', 'rs', ' c', 'om', 'in', 'g ', 'be', 'fo', 're', '. ', 'bu', 't ', 'no', 't ', 'li', 'ke', ' t', 'hi', 's.', ' t']\n",
      "\n",
      "Processing file dresden_files/377.txt\n",
      "Data size (Characters) (Document 376) 16910\n",
      "Sample string (Document 376) ['\\t\\t', '\\tw', 'he', 're', ' s', 'he', ' h', 'ad', ' l', 'ai', 'n,', ' t', 'he', 're', ' w', 'as', ' a', ' s', 'ym', 'bo', 'l ', 'sc', 'or', 'ch', 'ed', ' i', 'nt', 'o ', 'th', 'e ', 'cr', 'at', 'es', ' a', 's ', 'if', ' b', 'y ', 'a ', 'wh', 'it', 'e-', 'ho', 't ', 'st', 'yl', 'us', '. ', 'th', 're']\n",
      "\n",
      "Processing file dresden_files/378.txt\n",
      "Data size (Characters) (Document 377) 14993\n",
      "Sample string (Document 377) ['\\t\\t', '\\tm', 'ab', ' t', 'ur', 'ne', 'd ', 'to', ' m', 'e ', 'an', 'd ', 'no', 'dd', 'ed', ', ', 'ti', 'lt', 'in', 'g ', 'he', 'r ', 'he', 'ad', ' i', 'n ', 'to', 'wa', 'rd', ' t', 'he', ' c', 'en', 'te', 'r ', 'of', ' t', 'he', ' c', 'ir', 'cl', 'e.', '\\n\\n', '\\t\\t', '\\ti', ' s', 'ha', 'mb', 'le', 'd ']\n",
      "\n",
      "Processing file dresden_files/379.txt\n",
      "Data size (Characters) (Document 378) 5337\n",
      "Sample string (Document 378) ['\\t\\t', '\\t', 'yo', 'u ', 'ha', 've', 'n', 't ', 'he', 'ar', 'd ', 'a ', 'wo', 'rd', ' i', ' s', 'ai', 'd,', ' h', 'av', 'e ', 'yo', 'u?', ' ', 'sh', 'e ', 'as', 'ke', 'd ', 'me', ', ', 'a ', 'wh', 'il', 'e ', 'la', 'te', 'r.', '\\n\\n', '\\t\\t', '\\ti', ' b', 'li', 'nk', 'ed', ' a', 'nd', ' t', 'ri', 'ed']\n",
      "\n",
      "Processing file dresden_files/380.txt\n",
      "Data size (Characters) (Document 379) 11017\n",
      "Sample string (Document 379) ['t', 'wa', 's ', 'th', 'e ', 'ni', 'gh', 't ', 'be', 'fo', 're', ' c', 'hr', 'is', 'tm', 'as', ', ', 'an', 'd ', 'al', 'l ', 'th', 'ro', 'ug', 'h ', 'th', 'e ', 'ho', 'us', 'e,', ' n', 'ot', ' a', ' c', 're', 'at', 'ur', 'e ', 'wa', 's ', 'st', 'ir', 'ri', 'ng', ' e', 'xc', 'ep', 't ', 'me', ' a']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the words lower case\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Break the data into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Create a list of lists with bigrams\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6539391 Characters found.\n",
      "Most common words (+UNK) [('e ', 183790), (' t', 160267), ('th', 136248), ('he', 134943), ('d ', 133675)]\n",
      "Least common words (+UNK) [(' ', 1), ('31', 1), ('g', 1), ('r', 1), ('r', 1), ('m', 1), ('jn', 1), ('\\tq', 1), ('.0', 1), ('3r', 1), ('n3', 1), ('\\n=', 1), ('d', 1), (',0', 1), ('b3', 1)]\n",
      "Sample data [30, 105, 212, 50, 28, 35, 701, 223, 14, 54]\n",
      "Sample data [30, 492, 27, 321, 13, 2, 250, 94, 30, 622]\n",
      "Vocabulary:  1008\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tic (116), \tha (27), \tur (86), \t t (2), \tl. (305), \n",
      "\tOutput:\n",
      "\te  (1), \tlf (300), \t e (78), \tha (27), \t h (14), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\te  (1), \tlf (300), \t e (78), \tha (27), \t h (14), \n",
      "\tOutput:\n",
      "\tdo (124), \t a (6), \tar (39), \tn  (21), \te  (1), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tdo (124), \t a (6), \tar (39), \tn  (21), \te  (1), \n",
      "\tOutput:\n",
      "\tor (42), \tn  (21), \tli (69), \tus (101), \tdi (134), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tor (42), \tn  (21), \tli (69), \tus (101), \tdi (134), \n",
      "\tOutput:\n",
      "\t,  (16), \tho (82), \ter (13), \tua (344), \tdn (257), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\t,  (16), \tho (82), \ter (13), \tua (344), \tl. (305), \n",
      "\tOutput:\n",
      "\tha (27), \tur (86), \t t (2), \tl. (305), \t h (14), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the hidden state variables\n",
    "num_nodes = 128\n",
    "\n",
    "# Number of data points in a batch we process\n",
    "batch_size = 64\n",
    "\n",
    "# Number of time steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.0 # We use dropout\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    \n",
    "filename_to_save = 'lstm'+filename_extension+'.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "# Validation data placeholders\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1,vocabulary_size],name='valid_inputs')\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1,vocabulary_size], name = 'valid_labels')\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name = 'test_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate (o_t) - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_hidden')\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_cell')\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_cell')\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_hidden')\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(\n",
    "    valid_inputs, saved_valid_output, saved_valid_state)\n",
    "# Compute the logits\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output),\n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(\n",
    "test_input, saved_test_output, saved_test_state)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                            saved_state.assign(state)]):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "gstep = tf.Variable(0,trainable=False,name='global_step')\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "# Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.05)),\n",
    "    saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.05)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 18:25:04.788454: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-20 18:25:04.788475: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 0\n",
    "steps_per_document = 1\n",
    "valid_summary = 1\n",
    "train_doc_count = 380\n",
    "docs_per_step = 2\n",
    "\n",
    "\n",
    "# Capture the behavior of train perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>2*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==2:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    print('Training (Step: %d)'%step,end=' ')\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress .. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(2):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "        \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})  \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write training and validation perplexities to a csv file\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 5 # number of steps to look ahead\n",
    "beam_neighbors = 5 # number of neighbors to compare to at each step\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "        \n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "\n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] \n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            test_words[p_idx][0,this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iteration\n",
    "        # And update the probabilities for each beam\n",
    "        # In our example these would be [3,4] (indices with maximum value from above vector)\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) #This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            pred_words[b_n_i][0,this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_probs[rand_cand_ids]/np.sum(this_level_probs[rand_cand_ids])\n",
    "    random_id = np.random.choice(rand_cand_ids,p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[random_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 18:26:49.682580: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-20 18:26:49.682604: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(193).(108).(315).(256).(361).(10).(93).(158).(251).(298).\n",
      "Average loss at step 1: 4.865438\n",
      "\tPerplexity at step 1: 129.727707\n",
      "\n",
      "Valid Perplexity: 86.65\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      " se\n",
      "\t\n",
      "\ti the to to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the to to the to the said, in the to the said, in the that the shat the then to the that to that to the that to that to the to to that and the said the then to the that to that to the that to that to the that to that to the to to that and the said.\n",
      "\n",
      "\t\t\t\that the that and the fore the to to the to the said, in the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the to to that and the said.\n",
      "\n",
      "\t\t\t\that the then to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the that to that to the to to the that to that to the that to that to the that to th\n",
      "====================================================================\n",
      "\n",
      "(40).(163).(338).(6).(21).(166).(285).(20).(329).(172).\n",
      "Average loss at step 2: 3.687112\n",
      "\tPerplexity at step 2: 39.929355\n",
      "\n",
      "Valid Perplexity: 49.59\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      " thain, and the said. \"her that i said. \"that the said, \"that thing to that it the that to that it the that to that it the that the said. \"that and to that it the that the said.\n",
      "\n",
      "i said, and they was and said. \"that to the that the said.\n",
      "\n",
      "i said, and they was and said. \"that to the that the said.\n",
      "\n",
      "i saw that to been the said.\n",
      "\n",
      "i said, and they was and said. \"that the said, \"that thing to that it the that to that it the that the said. \"that and the said.\n",
      "\n",
      "i said, and that in the that to that it the that to that it the that to that it the that to that it the that to that it the that the said. \"that and to that it the that to that it the that to that it the that in that the said, \"that thing to that it the that the said.\n",
      "\n",
      "i saw that to been the said.\n",
      "\n",
      "i saw that to that the said. \"that and the said. \"that and to that it the that to that it the that to that it the that the said. \"that and the said. \"that and the said. \"that and the said.\n",
      "\n",
      "i saw that to been the said. \"that and the said. \"that\n",
      "====================================================================\n",
      "\n",
      "(156).(80).(195).(334).(283).(337).(141).(208).(88).(187).\n",
      "Average loss at step 3: 3.387518\n",
      "\tPerplexity at step 3: 29.592407\n",
      "\n",
      "Valid Perplexity: 45.11\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      " the of thad to that they was a said. the started they were they was a said. the started things and think in that in the most of they were that in the starter things to things and think in that in the most of they were they was a said. they could have. i said and that had been things and think in they was a said. the started things and think in that in the most of they were they was a said. the started things to things and think in that in the starter things to things and think in that in the starter than in the starter things to things and said, and said, and think in they was a said. they could have. i said of thing i said.\n",
      "\n",
      "i said, and couldnt said, and thing to things and said, and think in they was a said. the more the said. the started things to things and think in they was a said. the started things and thing to things and said. the started things and thing in that in the starter things to things and said. the started things to things and think in they was a some of the\n",
      "====================================================================\n",
      "\n",
      "(50).(209).(56).(28).(236).(228).(9).(121).(147).(309).\n",
      "Average loss at step 4: 3.243768\n",
      "\tPerplexity at step 4: 25.630124\n",
      "\n",
      "Valid Perplexity: 44.75\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      " ave of thim that here that the said, the speck that his head.\n",
      "\n",
      "\t\t\ti standed to things and though the spections that it things and though the spections that it things and though the spections that it things and though the spections that it things and though the specting thing to things and here of the places of hears of my head. he said. he said. he said. he said. he said. he said. he said. he said. he said. he started of them. why asked to them inside of though the specting thing to things and though the spections that it things and though the spections that it things and though the spections that it things and though the spections that it things and though the specting thing the said, the spected.\n",
      "\n",
      "\t\t\ti said.\n",
      "\n",
      "\t\t\ti hadnt have have and think if that it the spections that it things and though the spections that it things and though the spections that in a looked to being them and there and said quietly.\n",
      "\n",
      "\t\t\ti said.\n",
      "\n",
      "\t\t\ti hadnt been have and though the spections that it th\n",
      "====================================================================\n",
      "\n",
      "(194).(253).(179).(346).(315).(133).(160).(196).(307).(294).\n",
      "Average loss at step 5: 3.066347\n",
      "\tPerplexity at step 5: 21.463345\n",
      "\n",
      "Valid Perplexity: 43.73\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      " o evund. it have and and said, there wasn't to the would been her there and said.\n",
      "\n",
      "\t\t\ti said.\n",
      "\n",
      "\t\t\tthats not three to better there, and that there with, i said there was get to things. the wouldnt believed though to thing that thought. there was a would have got and said. that there was get to things. the wouldnt believed though to thing that thought. there wasnt her there, and that there with, i said there was get to things. the wouldnt belied to that that had there wouldnt hand there anything through the couldnt back to me that to things there, and that there with, she said, and they couldnt, and the said. i wouldnt but your an there was got to better that that there, and that there with, i said there was get to things. that there and said, and she said of things with to here with that there wouldnt behind than that there, and that there without there and said.\n",
      "\n",
      "\t\t\tthings then things. the would had youve get to been thing to things. ther with the worked the right the\n",
      "====================================================================\n",
      "\n",
      "(93).(206).(193).(196).(12).(128).(149).(343).(252).(178).\n",
      "Average loss at step 6: 3.009191\n",
      "\tPerplexity at step 6: 20.270990\n",
      "\n",
      "Valid Perplexity: 40.86\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      " t senside. though he said.\n",
      "\n",
      "there, i said quickly, but that his head.\n",
      "\n",
      "that i said. youre, i said quickly, but that his head. im nodded and said, shoulder had been anything. im not this to the door and thinked at me that i told her though to thing the started at me. i dongted that shoulder had been thought to help in that in the head and thought to make a showled and said. that im show you dont know that youll that, i said quickly, but that thing to this thought the start and then were and thought the place. i said. im there was shoulder had been thought to help in a stafter through the same, and you dont know that you dont know it walled the started the place. i wasnt have, i said question that in a could have his head.\n",
      "\n",
      "i shook his head.\n",
      "\n",
      "i shook his hand you dont know that you dont know it walked to this there. he should have the started at me. i dont the hards and the shoulded him not this to the couldnt his head.\n",
      "\n",
      "she said. his head. there was shoulder \n",
      "====================================================================\n",
      "\n",
      "(225).(286).(182).(177).(159).(166).(196).(110).(98).(136).\n",
      "Average loss at step 7: 2.892718\n",
      "\tPerplexity at step 7: 18.042289\n",
      "\n",
      "Valid Perplexity: 35.43\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      " . i to that it would had to being.\n",
      "\n",
      "i had been the phose the sigher.\n",
      "\n",
      "thomas said quietly. i said.\n",
      "\n",
      "theres good and spell out the spell the stood and against the prettered and the painted to be and the power the hunder things was a second of his the stood to things the spell the spell to things the spell to things and thing the pain.\n",
      "\n",
      "i hadnt have been the shoulded to make the house, and said.\n",
      "\n",
      "i should be into the prettered through to going to me.\n",
      "\n",
      "i that? i said.\n",
      "\n",
      "i had been the phose the sigher.\n",
      "\n",
      "thomas said quietly. i said. that was a second of thing the spell to things the spell the spell the stood and again. i halled the pressed to me. i have had the shoulded to make the house, and said.\n",
      "\n",
      "i should be into the prettered through to going to me.\n",
      "\n",
      "i thanked to this the started the place. i hadnt have been the power the paint to though the preplied to make the house, and said.\n",
      "\n",
      "i should have, and it have the painted the for a secogething in the phone of the preplied.\n",
      "\n",
      "that had \n",
      "====================================================================\n",
      "\n",
      "(141).(39).(67).(153).(377).(198).(34).(317).(185).(52).\n",
      "Average loss at step 8: 2.938272\n",
      "\tPerplexity at step 8: 18.883185\n",
      "\n",
      "Valid Perplexity: 33.07\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      " ldain ching, like this going to down the call of the first the first throat. i shouted at things and through the from the down and through the from the down and started and blazing thing throusled and through the from the down and started at my fact of the started at the back out one of the greet of though the back of that the started and started at my feet of a should hand and started at my fact of the started at the back out out of her through things and through the from the down and started at my fact of the started at the back, and stared and but the clocked the fire of thing out one of the greet of thought the first the first the first threet. then i wanted that that i would haved at the back out out of her through things and though the back out over that thing throusled and through the from the down and through the from the door, and helped at my fact an enough to me and then she said.\n",
      "\n",
      "i shook, but it wouldn't had to the feel, and stared and and then into the stop of the fire of t\n",
      "====================================================================\n",
      "\n",
      "(39).(308).(145).(78).(223).(81).(121).(378).(167).(368).\n",
      "Average loss at step 9: 2.824043\n",
      "\tPerplexity at step 9: 16.844812\n",
      "\n",
      "Valid Perplexity: 34.14\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      " ngp thair.\n",
      "\n",
      "\n",
      "\t\t\tthere one of that would hand that where to this stead aired.\n",
      "\n",
      "\t\t\tand then said.\n",
      "\n",
      "\t\t\ti took the ground on my started out and thinks of through the bridge an enormed a second of the bridge, and the bridged and said, and i said.\n",
      "\n",
      "\t\t\tthey were the kind of they were then said, whatever, i said.\n",
      "\n",
      "\n",
      "\t\t\ti was going to take.\n",
      "\n",
      "\t\t\ti took the ground on my started out and thinks of through the bridge an enormed a second of the bridge, and their back to make the council. it was the first the ground of my shouldered to the enemy one and their backs of it would between and all the bridge and said, and then said, and then made me an enormous that the back and then said. i have the time. it was that would be a come on through the moment, there was through the bridge. then said, whatever, i said.\n",
      "\n",
      "\n",
      "\t\t\ti was something forward. id been thing.\n",
      "\n",
      "\t\t\twhat?\" i shielded and said. whatever they were and they we confusion.\n",
      "\n",
      "\t\t\ti died at the moment, and then said. there they were was that w\n",
      "====================================================================\n",
      "\n",
      "(248).(295).(83).(73).(123).(106).(125).(37).(357).(14).\n",
      "Average loss at step 10: 2.846347\n",
      "\tPerplexity at step 10: 17.224745\n",
      "\n",
      "Valid Perplexity: 30.21\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      " r sentalf things wast that i wouted to my think of time that wouldn't harry,\" i safed. \"i did.\"\n",
      "\n",
      "\"why have you coming,\" i asked.\n",
      "\n",
      "i didn't see that they had the started the street of where in my head. \"they were in the door,\" i said quickly. \"maybe the door. i were that i would have. i said quiet in a cource in him, and his face. he was a slowly as against me that was a places of the streezed that made that i would have. i said. \"you're starting, anything, asked.\n",
      "\n",
      "\"you,\" i said quietly. \"maybe the door. i chance to got and that, and that. i was a courting, shoulder head and i would have. i said. \"you're starting, anything, asked.\n",
      "\n",
      "\"you've got though the sament, and, and than i said. \"and there. i had began to me hand and i could have, and that?\" i said quietly, but i have had been into his hands. i cold have to his hands.\n",
      "\n",
      "\"you know to be though the council. i would have. i said. \"you're starting, anything, asked.\n",
      "\n",
      "\"you,\" i said quietly. \"maybe the door. i started to before the door.\n",
      "\n",
      "i \n",
      "====================================================================\n",
      "\n",
      "(343).(27).(1).(214).(196).(349).(228).(149).(258).(22).\n",
      "Average loss at step 11: 2.728162\n",
      "\tPerplexity at step 11: 15.304736\n",
      "\n",
      "Valid Perplexity: 28.07\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      "  coun,\" i saw through my head. \"there was that was as i was to her the loup-garou down to them. i want is that i could had ground.\n",
      "\n",
      "but i was gone. that they weap there without the light, and started to see the loup-garou down to them. i was that was as i want things the came on things. the sound of through my couldn't happened to the little, it was started to his head. her eyes on the learn and the loup-garou, me the little the rest of time to the power, and, and there was there with things and through the loup-garou, me the ready of time of the loup-garou, me the ready of thing to that would have been through the rest of time to the loup-garou, the lit out the precise, and, and there was there with that i wouldn't have taken to that it was started to her to thought the pit of the eyes.\n",
      "\n",
      "i wanted the little.\n",
      "\n",
      "but it was the little, it was that was a lot of that would heard the loup-garou down to them. i was going to through me into me. i could beet. that i could been him. it want to do \n",
      "====================================================================\n",
      "\n",
      "(163).(233).(372).(29).(146).(225).(153).(88).(162).(213).\n",
      "Average loss at step 12: 2.739780\n",
      "\tPerplexity at step 12: 15.483578\n",
      "\n",
      "Valid Perplexity: 31.67\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      " lewnure, sure, i asked her. ans going taken to this things were in a most thing out and shook his expression of the boats of the water of through the boat and took at himself, but and then we said. and that i cant the think the white court of the council things that have as i asked.\n",
      "\n",
      "i didnt have. it want my eyes. then said. it was a couple of people to defend the court. i wanted the think is had been at him.\n",
      "\n",
      "i told her things that she said. his explassed to this staff. she was a couple of chance and staffing back at himself, and and said, then him that they head as well. theyre, thinking?\n",
      "\n",
      "yeah, i said. he said. youve gotten they head to than they thoughtfully.\n",
      "\n",
      "i stand out the second the last the seconds of head and sure of the council through the think if he staffined at his hand, and shook his eyes. the more said. then his head and shook his head. there, i saidently.\n",
      "\n",
      "i think her head. the more said, yeah. that i want it with things in a most more had been said quietl\n",
      "====================================================================\n",
      "\n",
      "(89).(132).(187).(138).(44).(62).(255).(50).(161).(88).\n",
      "Average loss at step 13: 2.734884\n",
      "\tPerplexity at step 13: 15.407949\n",
      "\n",
      "Valid Perplexity: 30.55\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      " reice. than don't want arturo's new someone. i'm not going to make the building.\"\n",
      "\n",
      "i started frowned. \"they've someone if i don't know that kind of that i'll be a someone,\" i said. \"something was things that the door anything,\" i said. \"i'm starting up,\" i said. her eyes the said, \"how more that's all. i don't things that?\" he said quietly. her eyes. \"things,\" i said. \"something was things, and then anything with thing to be and they've gave me a spell to them in the burneters and then if it is that there while then she was a couple of energy in my hands. \"they've sometimes,\" he said. \"maybe a starting.\"\n",
      "\n",
      "her expretenter. i could have to be and his exprencomenced at the building. \"i think you know that's,\" i said. \"i'm sure?\" i said, and then his eyes narrowed against the building with a spening the building.\n",
      "\n",
      "\"yeah.\"\n",
      "\n",
      "\"not things.\"\n",
      "\n",
      "murphy gave me a least this. they're all the circle.\n",
      "\n",
      "\n",
      "\n",
      "murphy said, then then said, and then through the light of a few more light, the curse to make a som\n",
      "====================================================================\n",
      "\n",
      "(252).(16).(122).(76).(313).(243).(143).(269).(338).(94).\n",
      "Average loss at step 14: 2.685045\n",
      "\tPerplexity at step 14: 14.658865\n",
      "\n",
      "Valid Perplexity: 28.06\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      " t killn,\" ebarry said. \"i don't do you killing your like they killed the power. i knew that mean. i waved to make, i did. and then i can't to kind that it. i wanted to me. her head. and i was here. then she said, i don't killed that my eyes about it.\"\n",
      "\n",
      "she said. it was to me.\n",
      "\n",
      "\t\t\twhat did you know?\" ebenezar shoulded. \"and then i don't know?\" i said at my hand, and her heard her back. \"they know what is yourself and to that the fire of time to my eyes, there,\" i said at my hands. then i didn't wanted to think.\"\n",
      "\n",
      "i shook his head. \"the council.\"\n",
      "\n",
      "ebenezar snapped. \"that was tell me. i told me to me.\"\n",
      "\n",
      "\"i knew that kincaid you,\" i said at my hands. then i could be something i wanted a lot of them. then i didn't walled to think.\"\n",
      "\n",
      "i started at me. \"i don't know when that me?\"\n",
      "\n",
      "\"i'm started that,\" i said at my hands. then i said. \"i knew you knew her hear. then stared at me.\n",
      "\n",
      "\t\t\ti don't knew him.\"\n",
      "\n",
      "\"i knew that kind of things.\"\n",
      "\n",
      "\"i know?\" i couldn't happened to me. \"you got to me.\" i said. \n",
      "====================================================================\n",
      "\n",
      "(261).(270).(197).(12).(258).(327).(2).(240).(373).(250).\n",
      "Average loss at step 15: 2.677945\n",
      "\tPerplexity at step 15: 14.555159\n",
      "\n",
      "Valid Perplexity: 28.13\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      " f.\"\n",
      "\n",
      "i hadnt, butters shook my head and strength.\n",
      "\n",
      "\t\t\ti wasnt, and then, it was one of them. then i said. but there was there was one of that would happened to make the back of that wouldnt, but she said.\n",
      "\n",
      "i thought they would had been a pair of there.\n",
      "\n",
      "i walked the strength, a lot of that would happened to murphy.\n",
      "\n",
      "she was a pair of them. i was stared and then the safe, and stared and then that it was a lot of them. it was stared and from the back on that would happened to murphy.\n",
      "\n",
      "she was a pair of that would happy and shook his hand. they know, i murphy saited. that she was a couple in his force as if shed been a smile.\n",
      "\n",
      "i had been something three they came and that was a moment, and, and then, it was one of them. then i saw that would have got to my eyes, the head and shook his head. i waved to do it would better that had been took her things of those to think of her arm. that would be intentions, and that had been get that when i was stared and then that was a lot more of a\n",
      "====================================================================\n",
      "\n",
      "(299).(66).(342).(171).(195).(11).(4).(199).(249).(226).\n",
      "Average loss at step 16: 2.638678\n",
      "\tPerplexity at step 16: 13.994685\n",
      "\n",
      "Valid Perplexity: 27.97\n",
      "\n",
      "Generated Text after epoch 15 ... \n",
      "======================== New text Segment ==========================\n",
      " ize. yah, murphy said. that is all them that? i shook his head and then she smicked to that the samers were a few ment later than she was a completely. you gearly this place.\n",
      "\n",
      "yeah, i shook his his head. where isnt, i said quietly. it isnt have been started to do that though, i asked. i stared the face the facing to be an internation of here.\n",
      "\n",
      "i started to a little realized through the rest of three their and startling to that it was in a straining and a few ment control there. it walked the sound of them with a pair of them with them with a white council of this.\n",
      "\n",
      "she shook her head.\n",
      "\n",
      "i shrugged. if they were, i askeeped to this the same, and there were they were that wouldnt? i askeeped to there was through the last time that was and through her arms were all through the rest of three the sounds of a couple of course. think then he was an enough the way out out and the same that she was a couple of course.\n",
      "\n",
      "we was an enough the rest of thing the side of the way that \n",
      "====================================================================\n",
      "\n",
      "(364).(174).(372).(251).(291).(244).(338).(303).(341).(205).\n",
      "Average loss at step 17: 2.596477\n",
      "\tPerplexity at step 17: 13.416391\n",
      "\n",
      "Valid Perplexity: 32.36\n",
      "\n",
      "Generated Text after epoch 16 ... \n",
      "======================== New text Segment ==========================\n",
      " or\n",
      "i than them with a woman becare than they had been. there what i kneld that the way out one of her voice.\n",
      "\n",
      "i took a reach of the conventer of her. someone had some kind. theres reactive terrible.\n",
      "\n",
      "that wasnt, and the words of his voice. she wasnts of her voice.\n",
      "\n",
      "i had even i had the staff at me.\n",
      "\n",
      "i start of her.\n",
      "\n",
      "im not anything to get me a good of things.\n",
      "\n",
      "he leaned to my hand at me at things are thought the park of this only, that wasnt, and the words of his voice. she had before. than this hell, it was one of the first to her. then wouldnt several that, and there was a suppot that someone where than this that even if you, i said.\n",
      "\n",
      "i wasnt. then he convented something my head. what?\n",
      "\n",
      "thats not get this only.\n",
      "\n",
      "i stepped up at me, and-looking into the seconds on her.\n",
      "\n",
      "i walked to me, and the door and then that would have to tell they had to helped to her. then he wasnt like a secognize that. i didnt?\n",
      "\n",
      "not what youre, i said.\n",
      "\n",
      "i was a probably known that had been \n",
      "====================================================================\n",
      "\n",
      "(371).(15).(230).(189).(378).(339).(14).(336).(28).(258).\n",
      "Average loss at step 18: 2.541669\n",
      "\tPerplexity at step 18: 12.700848\n",
      "\n",
      "Valid Perplexity: 31.35\n",
      "\n",
      "Generated Text after epoch 17 ... \n",
      "======================== New text Segment ==========================\n",
      " s sikeb and started anywhere and dont have, butters, said. theres notened and it wouldnt? i said quietly. youre got to do it, i said. there way, i said quietly. she was and there will and that had not seen the skull and then he wanted and straight. youve got something all them that would going to have to be a council. that wought have taken they and help his head. what?\n",
      "\n",
      "yeah.\n",
      "\n",
      "youve been company intervive into thing. that wed have take anything. but they wouldnt have take that i cant going to take the forces of those things.\n",
      "\n",
      "well, he said, well have to talk to be a complication.\n",
      "\n",
      "but and said quietly. that wouldnt have to talk to him.\n",
      "\n",
      "bob said.\n",
      "\n",
      "bully, butters shook her head. well serve.\n",
      "\n",
      "butters shook his head and, and said. theres. butters and there and there all theres not to steps the back of them. you knocking it would be a lot of things wouldnt have before. that, butters, said. what?\n",
      "\n",
      "yes, he sapped. i help that hes gone. but it was\n",
      "====================================================================\n",
      "\n",
      "(65).(68).(102).(221).(252).(57).(82).(98).(231).(174).\n",
      "Average loss at step 19: 2.566047\n",
      "\tPerplexity at step 19: 13.014276\n",
      "\n",
      "Valid Perplexity: 30.25\n",
      "\n",
      "Generated Text after epoch 18 ... \n",
      "======================== New text Segment ==========================\n",
      "  emertkers. sament, it wouldnt heard to burned to building to the build, and then it was a couple of his voice. his eyes gruffs was sure of thoughts anymore of thoughts of inside there was a couple of her voice could be better.\n",
      "\n",
      "i grunted to the build, and then her eyes glowed on her eyes against the from the books and the skull, but there are to through the book at there was a couple of one of the coat.\n",
      "\n",
      "\n",
      "\n",
      "chected they were standing at his and there was a couple of anothers, but they work into things and then the streetly.\n",
      "\n",
      "thats there, i sauled and stepped my hand and stepped at me.\n",
      "\n",
      "that had been there, i muttered. thats there, as a couple of books and not that that had better than yourself. it was a walling through the skull, and it was a few of the building that was a foot littomation of the streedings and i had to talk to the coat.\n",
      "\n",
      "\n",
      "\n",
      "UNKUNK\n",
      "\n",
      "\n",
      "\n",
      "chicago.\n",
      "\n",
      "i hadnt beezed them to stay to start that it was a couple of papenter. it wouldnts the buid and starited on that than a \n",
      "====================================================================\n",
      "\n",
      "(104).(166).(211).(327).(155).(14).(358).(89).(360).(51).\n",
      "Average loss at step 20: 2.471998\n",
      "\tPerplexity at step 20: 11.846087\n",
      "\n",
      "Valid Perplexity: 26.19\n",
      "\n",
      "Generated Text after epoch 19 ... \n",
      "======================== New text Segment ==========================\n",
      " a yevelating?\" i asked.\n",
      "\n",
      "\"if you.\" i said. \"but that you've see that you've got think they aren't thing to this this tonight.\"\n",
      "\n",
      "\"why think i'm going to tell you.\" i said, \"yeah,\" mutters said quietly. \"and if i know they around the door,\" she'll said, her voice. \"it isn't, they're getting together.\"\n",
      "\n",
      "i said, \"yeah. that there was she would have been there without my hend. that took me to find out out of the part of there, and i don't see you this wasn't something they're than they were started to get me a good that has godmenter than it, and then he should.\"\n",
      "\n",
      "\"what did you killed me that have think your like in a white court.\"\n",
      "\n",
      "murphy's eyes stared at he watched me and then her eyes shoulders with the store of those cloak and she could have had been some killed and she would have been there. then she had the close to through the back of things and shook my hear. then shook my head. \"no, after they working think i'm not,\" i said something my head, but her eyes and she had the supernable.\n",
      "====================================================================\n",
      "\n",
      "(60).(159).(160).(326).(275).(121).(316).(267).(184).(319).\n",
      "Average loss at step 21: 2.557056\n",
      "\tPerplexity at step 21: 12.897790\n",
      "\n",
      "Valid Perplexity: 26.69\n",
      "\n",
      "Generated Text after epoch 20 ... \n",
      "======================== New text Segment ==========================\n",
      " taw mark mouself and thomas said.\n",
      "\n",
      "id have.\n",
      "\n",
      "i stared at the council an expression. i had even they had been think of that would be.\n",
      "\n",
      "what they will that? i said. i die, i said quietly, there were, and than this was completely, and there was a senter that there was completely, and then that would have before the seconds of me, and then i shook her head against my eyes and said. im trying to know what?\n",
      "\n",
      "when what? i saw asking that she watched her hand and then she shook my head again. im sure that youre, thomas, and that youre alwaved to be matter to here the companion, with them. they werried than everything into the supernatures and said, they want to take, i said quietly, there werring that had been a second an experies one of the room.\n",
      "\n",
      "i didnt see her head and then shook my head and said. they winter the hopes of through the closer of hands. there was a couple of sounds and closed through the spirituaties of the council an expression.\n",
      "\n",
      "im sure youre doing to \n",
      "====================================================================\n",
      "\n",
      "(287).(66).(125).(170).(237).(362).(262).(53).(36).(355).\n",
      "Average loss at step 22: 2.538239\n",
      "\tPerplexity at step 22: 12.657359\n",
      "\n",
      "Valid Perplexity: 27.23\n",
      "\n",
      "Generated Text after epoch 21 ... \n",
      "======================== New text Segment ==========================\n",
      " . h. if that is think i have to do this is what you around my gone.\n",
      "\n",
      "\t\t\tand then she repeed her hand forward an effect thoughtfully and said.\n",
      "\n",
      "\t\t\ti liffed my eyes.\n",
      "\n",
      "\t\t\ti stared at me in the part of the part of through the problems together. then she had her head.\n",
      "\n",
      "\t\t\ti shook her her head. youre goiff to be a forced to trying to try to getting that this is about this and your personable that is think youll take the parts of part of this. if you can do think to do those things, mab sapped.\n",
      "\n",
      "\t\t\twhat? i asked.\n",
      "\n",
      "\t\t\tand then she repeatter that shook his hand.\n",
      "\n",
      "\t\t\tthat is think youll take the nightmares, i said. i should had been shouted. yeah. its this part you?\n",
      "\n",
      "\t\t\twhat? i murmed. if i was gone.\n",
      "\n",
      "\t\t\twhat? i said.\n",
      "\n",
      "\t\t\twhat? i shook her hand.\n",
      "\n",
      "\t\t\twhat? i agreed.\n",
      "\n",
      "\t\t\ti shook his head. youre goiff to be a forced to fighting. he said more than mostly and her head again.\n",
      "\n",
      "\t\t\tand then she repeat me and said, i had heard the doorway. she said, his hand into the part of the\n",
      "====================================================================\n",
      "\n",
      "(137).(300).(10).(92).(360).(82).(216).(277).(23).(379).\n",
      "Average loss at step 23: 2.436275\n",
      "\tPerplexity at step 23: 11.430387\n",
      "\n",
      "Valid Perplexity: 29.39\n",
      "\n",
      "Generated Text after epoch 22 ... \n",
      "======================== New text Segment ==========================\n",
      " f ber, she blinked.\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tand that was a comfortatically.\n",
      "\n",
      "\t\t\tand then she was a few mouse when the friend.\n",
      "\n",
      "\t\t\ti looked back and that was a small, but something something was a sudight was a heavy, anymore.\n",
      "\n",
      "\t\t\tand i had a lot of less that had been surving his face and said, he looked and if it wought youre here.\n",
      "\n",
      "\ttllowed mine.\n",
      "\n",
      "\t\t\ti looked door at the fire.\n",
      "\n",
      "\t\t\twell on through the last time that the front of that was a long togest and then head and throat. youve been you think about the way that was a long time.\n",
      "\n",
      "\t\t\twhich was a little break in a deep, and a little slowly. i could have her head back to think of the back and let out that was a simple of thoughter, when the front of that i had through to myself, and, and the power of something for a most play of me.\n",
      "\n",
      "\t\t\tyoull have been to all, i said and then said, and her eyes again. she looked at his head. sometimen wouldnt go a lot to getting down and took that think and two dend, thought it with the mind of my hands.\n",
      "\n",
      "\t\n",
      "====================================================================\n",
      "\n",
      "(376).(357).(223).(226).(180).(119).(321).(103).(212).(210).\n",
      "Average loss at step 24: 2.515042\n",
      "\tPerplexity at step 24: 12.367127\n",
      "\n",
      "Valid Perplexity: 26.66\n",
      "\n",
      "Generated Text after epoch 23 ... \n",
      "======================== New text Segment ==========================\n",
      " \n",
      "\n",
      "\n",
      "ordererers, and there was a second of touch of my feet, and without specially against the way them under the room. i couldnt have, it was coming and could thats a coups. they happened to problem in the same theyd be someone else it was going to that the same time.\n",
      "\n",
      "she was a council, but he wouched my eyebrows. i should haven here. youre all out.\n",
      "\n",
      "i shook my hound. im some of thought to get out of any of that who wounced the stuck.\n",
      "\n",
      "i cant see they have protection of that have been silents of problem. i said. what?\n",
      "\n",
      "yes, i saiked. yous going to see to getting the call of this.\n",
      "\n",
      "i shotted up against the way them into the wardens and with the room, and theyd be gone in that the way, there was. i could have nodding.\n",
      "\n",
      "they would better that you do it. youll sure your or them.\n",
      "\n",
      "youve going to takes to the council?\n",
      "\n",
      "there was a second as i could seen that way them that was a couple of thing it would have done that things that is the same this chaos to think of th\n",
      "====================================================================\n",
      "\n",
      "(306).(343).(332).(247).(71).(146).(115).(220).(11).(17).\n",
      "Average loss at step 25: 2.385637\n",
      "\tPerplexity at step 25: 10.865980\n",
      "\n",
      "Valid Perplexity: 21.78\n",
      "\n",
      "Generated Text after epoch 24 ... \n",
      "======================== New text Segment ==========================\n",
      " rtk-\"\n",
      "\n",
      "\"sevent,\" i said. \"there's, though of that they can hell, and the could had money, anyway. the counter there, and i was the closer that past the door of them in the roof of there, and through the council, who had been some three thing it would be, and the loup-garou, murphy's, all the hallways are that things that she was a half a hand at the couple of black make my hand around the stairs and starting up at the couple of blood.\n",
      "\n",
      "i had to see her throat. i should, his eyes, and the loup-garou, his eyes, and the loup-garou, his eyes, and the loup-garou, murphy's, and the loup-garou, his eyes, and the loup-garou, murphy's, and the loup-garou, his eyes, and the loup-garou, his eyes, and the loup-garou, his eyes, and the loup-garou, murphy's, and the loup-garou, murphy's, and the loup-garou, murphy had been said, and shook her haid into me, her eyed through my staff away. \"that doesn't sure that i've got that you're all right,\" i said, i could happened, an into the council. what is th\n",
      "====================================================================\n",
      "\n",
      "(110).(32).(7).(344).(284).(349).(13).(225).(52).(311).\n",
      "Average loss at step 26: 2.476877\n",
      "\tPerplexity at step 26: 11.904033\n",
      "\n",
      "Valid Perplexity: 25.15\n",
      "\n",
      "Generated Text after epoch 25 ... \n",
      "======================== New text Segment ==========================\n",
      "  gleg, a salt. i was right, and he didnt stop that had been toward to be able to do it. i know that youre the one of them, and then it wasn't an arched out and movies an enormous, michael around me, and then another time, and the genoskwa would be sure that how much as my head away from times, and it wasnt something had been a predator selfire an enormous spready of its with me, and that wasnt something had been a predator and that there was a prey of the staff around me, and then another thickence, anyway.\n",
      "\n",
      "\t\t\tand that would have all stoppeation of turning and moving, a few mortal, and the genoskwa had been that had better than my shoulder.\n",
      "\n",
      "\t\t\tthe genoskwa glance and right hands on the floor of turner to turned to try the ground.\n",
      "\n",
      "\t\t\tharry, i mumbled.\n",
      "\n",
      "and then shook my heavy that that held me.\n",
      "\n",
      "\t\t\tthe genoskwa glance and but it was, and the genoskwa glance at his hands and then sensation, started toward me another had had been another than a couple that he wouldnt happened at \n",
      "====================================================================\n",
      "\n",
      "(319).(72).(13).(145).(190).(270).(68).(91).(119).(198).\n",
      "Average loss at step 27: 2.328314\n",
      "\tPerplexity at step 27: 10.260626\n",
      "\n",
      "Valid Perplexity: 26.14\n",
      "\n",
      "Generated Text after epoch 26 ... \n",
      "======================== New text Segment ==========================\n",
      " imghusrcycler stand them with themselves. he was a second, and she wasnt. it was that it wasnt an investigational confidence. they started toward the wardens.\n",
      "\n",
      "murphy was a second, and she watched it with a skinwalker, a lot of them, and then she wan a moment, and it was, and it would have to be talk to the my staff, after than into the come into time. it walked out a second, it didnt have to see how much as i had a lot of that there with them, but that will be tale to the white counch, it was stared at him into time. i tooded them.\n",
      "\n",
      "billy and stared at me. i was a second, and she willed in though it. i wanted toward the lights werewolves in a skinwalking arounder to the minute, after the stairs, and started threatening.\n",
      "\n",
      "butters, and then she was a time to this was stronging, a second, a second, i started skinwalker light and kirby hand of his eyes. she walked on that there would have been some here. shed been an apartment. there was a second, but id browned. that i was starting to\n",
      "====================================================================\n",
      "\n",
      "(90).(345).(162).(312).(284).(37).(180).(319).(194).(243).\n",
      "Average loss at step 28: 2.348888\n",
      "\tPerplexity at step 28: 10.473916\n",
      "\n",
      "Valid Perplexity: 27.19\n",
      "\n",
      "Generated Text after epoch 27 ... \n",
      "======================== New text Segment ==========================\n",
      "  right never figured things that wouldnt have the genuine that, and than yourself there werelt seen one of my arms as this six\n",
      "\n",
      "i needs to see them through the hands and point of thomas.\n",
      "\n",
      "i stared to help me, after him against my teeth and sir stuart completed at me. ahhhhhhhh, she said. they were, and that, and the way that they were themselves into them what youll have to get your hands.\n",
      "\n",
      "i nodded.\n",
      "\n",
      "but therey stared the strong, help me against my teeth and sir stuart looked up out of him, and the ground. youve gotter them trying to believed me to you that they happened?\n",
      "\n",
      "youve gone that, murphy said.\n",
      "\n",
      "youve, he said quietly. his head. his head wide at him. theres, i said at all. they were block, and it happened in the power.\n",
      "\n",
      "what? i agreed.\n",
      "\n",
      "i stared at his head, and then said.\n",
      "\n",
      "you cant see there. there was noticed.\n",
      "\n",
      "what? i agreed.\n",
      "\n",
      "i stared at his head, and then said. they dont have.\n",
      "\n",
      "i shouted at that i heard the stronger start of the strong, but h\n",
      "====================================================================\n",
      "\n",
      "(237).(163).(57).(361).(156).(63).(210).(219).(175).(233).\n",
      "Average loss at step 29: 2.440135\n",
      "\tPerplexity at step 29: 11.474593\n",
      "\n",
      "Valid Perplexity: 28.18\n",
      "\n",
      "Generated Text after epoch 28 ... \n",
      "======================== New text Segment ==========================\n",
      "  knly.\n",
      "\n",
      "he smiled at him and saved. why theyll get.\n",
      "\n",
      "he norely said, and took my head an and said, his head up at his head. im sort of thought to you.\n",
      "\n",
      "he nodded and called in his hand and said, yes. he went in the building that there are they realized toot there is a lot one of the rest of this could hands. then a little telling there, and there was a fingers and there are that many of the most of that was a part of the pentacle folk to them around her, and took a low that was and there as well. then he said at him an and there something had been toother than identer things they could have been thinking to seen the chest of time to there was a point that there was being that this way that there are that made a lot more that there was something would be nodding.\n",
      "\n",
      "tilly not sometimes to take a confluence and his his head anything the probassy intention when there was a litter than i had a littating the probably line and trusting that there about the stone of that sometimes in the \n",
      "====================================================================\n",
      "\n",
      "(17).(87).(376).(204).(98).(366).(110).(42).(162).(20).\n",
      "Average loss at step 30: 2.385743\n",
      "\tPerplexity at step 30: 10.867131\n",
      "\n",
      "Valid Perplexity: 24.27\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 29 ... \n",
      "======================== New text Segment ==========================\n",
      " nondovidsteperness.\" i shook my through them. then shook his hand. \"we're going to talk to think to that,\" i sained at the more than anything through them. then showed me, then she stared at thomas, and she was a closed to see the street, and she saw myself. it is all that, all the words, another the same with him, she'd could have. it was one of the words. i couldn't say, there of the same. i saw my head. she'd been away from that, and thought i'd convention to get ten to get them. theres a little crouch to see the warmth of side. it would be and they're going to do anyone who have think if it is a long out the street, and walked in her, and then it wasn't there wouldn't want the warmth of time, and then they went up at her. i couldn't.\n",
      "\n",
      "\"yon,\" i said quietly. it was a lot of the words, another the greater of something something something something than they couldn't. it was a lot of the words. i thought. they went out of the darkness. then it is there, anyway. that they went out of th\n",
      "====================================================================\n",
      "\n",
      "(212).(365).(198).(129).(103).(90).(43).(355).(231).(282).\n",
      "Average loss at step 31: 2.476197\n",
      "\tPerplexity at step 31: 11.895937\n",
      "\n",
      "Valid Perplexity: 23.20\n",
      "\n",
      "Generated Text after epoch 30 ... \n",
      "======================== New text Segment ==========================\n",
      " hesp mentage, and the side of times and turn that why would have been that, and i didnt knee that there was a couple of times of times it wasnt there that was stared and and then well that they were all of the most of times it wants to take, and it cant the next to them into they could happen to that, all that they would have have to be there wouched him and there was a second, and it would have some kind and they didnt want. it was been than you arent everything, and the way out overware. i hadnt happening.\n",
      "\n",
      "mab started, and the sidhe was going to find to them into that it was an extrated. it was realized toward the walls and made a shape of the back of them. thats happened out that i could say, there walked on my fact of those was all of them.\n",
      "\n",
      "why dresden, hare said. why did yif there wouldnt hallowed my head. the one of the sidhe was going to get them. you have talk to the sidhe on yourself, it would help.\n",
      "\n",
      "i know, i said quietly.\n",
      "\n",
      "what happened? mab said, his eyes \n",
      "====================================================================\n",
      "\n",
      "(294).(178).(218).(196).(23).(198).(203).(331).(74).(42).\n",
      "Average loss at step 32: 2.414203\n",
      "\tPerplexity at step 32: 11.180861\n",
      "\n",
      "Valid Perplexity: 21.48\n",
      "\n",
      "Generated Text after epoch 31 ... \n",
      "======================== New text Segment ==========================\n",
      " ngifielded, harry. if that would i have before they had been standing to be things like the way that watched it that way that i am never to think to the true think i'm not that it is there with.\"\n",
      "\n",
      "i shold my head and said skinwalker, and the short of throat, and she was a minutes on.\n",
      "\n",
      "i shood my head with a smasher and threw out over my hand.\n",
      "\n",
      "i shook my head to my hand and said. \"then it happened to.\"\n",
      "\n",
      "i saids appeared, and they had been able to something that i hadnapped into it. it wasn't the door and there was a couple of things and back into it, then into the shotguns of the doors and then he said, \"that's that?\" i asked.\n",
      "\n",
      "\"they would be here.\"\n",
      "\n",
      "\"i have to be willing them?\"\n",
      "\n",
      "\"i know that i'm not to be thinkin,\" i said. \"and you would have, but the desk.\"\n",
      "\n",
      "\"what?\" i agreed the left on them. \"and years, then it isn't going?\"\n",
      "\n",
      "i lifted her, and then she glanced at my hands.\n",
      "\n",
      "i drew on my head. \"i have they aren't the doors to help you are you are. i'm not going to bells.\"\n",
      "\n",
      "i shook my h\n",
      "====================================================================\n",
      "\n",
      "(91).(35).(63).(351).(331).(153).(124).(49).(214).(330).\n",
      "Average loss at step 33: 2.468755\n",
      "\tPerplexity at step 33: 11.807739\n",
      "\n",
      "Valid Perplexity: 22.05\n",
      "\n",
      "Generated Text after epoch 32 ... \n",
      "======================== New text Segment ==========================\n",
      " e leededgently. there, but it walked out off the more, and i wanted the white count. it wasney and they were allowed the skinwalker of the wall, and it came in that there wouldnt seem to be a lot of that was allies of the council. and they was and they have been.\n",
      "\n",
      "theres. but it would have seen the white counting them out of the dark of thought there was a sign of the wardens through the council who would happened that they hadnt been something hard to things. there, and it had been thought that, it was been they wouldnt be, and their was the council, when they wouldnt he to be absolutely back into time to the council, where the white counted that world before. theyre recovered his head. thats anyone things, murph said. you, harry, she said. his voice that had nodded, an eyebrow against my brother.\n",
      "\n",
      "theres a second, mab said. and they wouldnt heard him there was a second of it. it wanted to be an intection of the way that youre allowed in a couple of intections and then \n",
      "====================================================================\n",
      "\n",
      "(143).(289).(275).(131).(194).(90).(336).(190).(207).(305).\n",
      "Average loss at step 34: 2.388741\n",
      "\tPerplexity at step 34: 10.899761\n",
      "\n",
      "Valid Perplexity: 23.54\n",
      "\n",
      "Generated Text after epoch 33 ... \n",
      "======================== New text Segment ==========================\n",
      " rpguse opere.\n",
      "\n",
      "\t\t\tthis was a couple of constructine, i saidnt see myself in time.\n",
      "\n",
      "\t\t\tthat, i slaked at myself into a second light.\n",
      "\n",
      "\t\t\tand there id have have there was a section. that he was a couple of blue glance, and there was a slitter, and, and she said, there is the place that. if you know what do you surely things have been right as if i cant see myself. it was true.\n",
      "\n",
      "\t\t\tand then he said. you had better that yourself, too.\n",
      "\n",
      "\t\t\tyeah, i snatted. youve gotten to the side of that thats not the strength.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "chaptine\n",
      "\n",
      "\n",
      "\t\t\ti started away from thomas and looked at her.\n",
      "\n",
      "\t\t\tthat, i squided. that was a couple of yourself, that is a couple of yourself. she was a couple of yourself. she was a couple of light, and it worked with you. but i wander things and it have but the strength that the side of the time to meant me anything is that the side of the time to me.\n",
      "\n",
      "\t\t\ti white court and then said the strength.\n",
      "\n",
      "he shook her head.\n",
      "\n",
      "it isnt happen that. i said, her voic\n",
      "====================================================================\n",
      "\n",
      "(339).(20).(280).(97).(163).(57).(272).(90).(64).(204).\n",
      "Average loss at step 35: 2.427439\n",
      "\tPerplexity at step 35: 11.329826\n",
      "\n",
      "Valid Perplexity: 23.50\n",
      "\n",
      "Generated Text after epoch 34 ... \n",
      "======================== New text Segment ==========================\n",
      " e lessrway. you know when its horrow the morgans grabbed mine, and there was the table to you?\n",
      "\n",
      "there was a couple of well, i started his back to thing. then she was a lot of there. youre going through the countil i couldnt have to be over to me.\n",
      "\n",
      "molly gotten inflections in the strength of two of the buildings of the strength of magically sense of there was a moment, there werented that holding it in the one for the outer that it would between his back, and his head into the seconds of more than most of things, and wed gotten into the table to reached through the buildings around him.\n",
      "\n",
      "murphy asked.\n",
      "\n",
      "morgan said, then the stretched his hands and said, her voice came of through the buildings around his head into a second of the strength of two of the ground at him down against the table to reaching toward the ground. he was a seclaced in time in the one of them with that was about the othranough to make the one of the one of them without for a momfood, the rest of there was a \n",
      "====================================================================\n",
      "\n",
      "(88).(289).(242).(311).(67).(379).(113).(126).(136).(313).\n",
      "Average loss at step 36: 2.420635\n",
      "\tPerplexity at step 36: 11.252999\n",
      "\n",
      "Valid Perplexity: 23.56\n",
      "\n",
      "Generated Text after epoch 35 ... \n",
      "======================== New text Segment ==========================\n",
      " ivzyh,\" he shrugged. thats a short of themselves. there was here was a professions.\n",
      "\n",
      "\t\t\ti frowned against my head, and her eyes went into time to my staff in time.\n",
      "\n",
      "\t\t\tyoure going to be a lot, she asked.\n",
      "\n",
      "\t\n",
      "\t\t\tmichaels bells, my eyes when her heirs and sat one of those with him.\n",
      "\n",
      "\t\t\tim not a blood of time.\n",
      "\n",
      "\t\t\they, i shrugged. someone elt off the first, i asked.\n",
      "\n",
      "\t\t\tand then he said, that we hallowed me in time, he said.\n",
      "\n",
      "hey, i shrugged. im not a good things, i said after the color of my staff against the floor, an approachinds of the floor, annable and said, hey? i asked me and saved.\n",
      "\n",
      "\t\t\tyouve got some of those time theyll be giving that if yourself, i snapped at his chest, where i could have all that we stared away from time.\n",
      "\n",
      "\t\t\tyeah, i shrugged. someone elt off the first, i muttered. where it would be and theres a short out theres considereds.\n",
      "\n",
      "\t\t\tyoull given me and said your work, i snapped at his chest, when i walked out three hand up and sai\n",
      "====================================================================\n",
      "\n",
      "(374).(78).(379).(218).(336).(149).(99).(88).(351).(261).\n",
      "Average loss at step 37: 2.346634\n",
      "\tPerplexity at step 37: 10.450334\n",
      "\n",
      "Valid Perplexity: 22.87\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 36 ... \n",
      "======================== New text Segment ==========================\n",
      " pon, thomas said.\n",
      "\n",
      "i started to the stairs. it was nothing to get it to the stairs.\n",
      "\n",
      "i wasnt sure theyd been on the stairs. it wanted in there, and there wasnt. it was nothing to get it to the staff back at him.\n",
      "\n",
      "\t\t\ti started to stop the time, and went on through the way of the front of my staff, if you watched.\n",
      "\n",
      "yeah, i shrugged. that was all of the way of the stairs, at least. i didnt hael on the stairs, anymore. it wouldnt happen to stop of them. it wasnrited. it isnt the corpsetaker. it was nothing to the staff back at her. i wasn't sure theyd had there wasnt. it wasnt an instinction of here.\n",
      "\n",
      "i thought of something had been there was that was a weight of his exprettered in my staff, staring in his staff.\n",
      "\n",
      "\t\t\ti wasnt sure that i wasnt that time to get me and then she was going to tell here that was a weight off at her. theres a wizard, she said. and there. youll believe to leave.\n",
      "\n",
      "his staff against his eyes. there was a couple of lights of more like youll be \n",
      "====================================================================\n",
      "\n",
      "(39).(137).(131).(77).(278).(171).(269).(199).(230).(141).\n",
      "Average loss at step 38: 2.489885\n",
      "\tPerplexity at step 38: 12.059890\n",
      "\n",
      "Valid Perplexity: 21.39\n",
      "\n",
      "Generated Text after epoch 37 ... \n",
      "======================== New text Segment ==========================\n",
      "  thirdor to find the back of the world. i think the chairs, anyway. its working inside.\n",
      "\n",
      "what? i snarled. im sorry, it wasnt help.\n",
      "\n",
      "i nodded. im sorry. she said. \"it wasnt, i said against me to me. ive got that there, i said. but it was. it wont, i said quietly. but that happens of his face, but it was.\n",
      "\n",
      "i took.\n",
      "\n",
      "there she repliedled the side of my chest. your little yourself, but its here, i said. if you.\n",
      "\n",
      "i took.\n",
      "\n",
      "thomas and said quiet to thomas. i think that wasnt think of the minute, though the world has going to be, i said. if you can do to do that, i think it wouldnt happen that like the white court, and i would have happened toward the trail. it wouldnt help me thank to the spell to make that though the world had it wasnt helping to do that i thought tell me that, i wasnt?\n",
      "\n",
      "the words of time for things, i shook my heard.\n",
      "\n",
      "thomough the spell, and then she shrugged. that wasnt, i said quietly. but that happened than there isnt thing. it woul\n",
      "====================================================================\n",
      "\n",
      "(247).(321).(40).(378).(66).(2).(342).(374).(25).(285).\n",
      "Average loss at step 39: 2.477559\n",
      "\tPerplexity at step 39: 11.912151\n",
      "\n",
      "Valid Perplexity: 20.72\n",
      "\n",
      "Generated Text after epoch 38 ... \n",
      "======================== New text Segment ==========================\n",
      " . hed in any right at once at that is the one of the hell, and it isnt like that there wasnring them into the world. the world had everything had been on the winding the world, and then she showed me again, the strength out and the red could completely, and it would have anything that is there were a couple of chair, anothers and started silence and right and interestening to them and the riders of the winterence, and his face.\n",
      "\n",
      "\t\t\tand i could havrought the cloud, and then she showed up and startmented and increter into the shoulders at me, and then she swept at that should several tingles of those with my staff, stared, and then she knew that had been one of the closer anymore, and the steady, steading, and i heard her again, and then she swept at that should have been as if the end of the closer of his shoulders and said, there.\n",
      "\n",
      "\t\t\ti grunted. there isnt?\"\n",
      "\n",
      "\"not that,\" i snarled at me, and i looked at him into a moment. i shook his hand off against there, and thought above him. th\n",
      "====================================================================\n",
      "\n",
      "(358).(247).(357).(122).(88).(356).(152).(25).(306).(294).\n",
      "Average loss at step 40: 2.412429\n",
      "\tPerplexity at step 40: 11.161037\n",
      "\n",
      "Valid Perplexity: 22.26\n",
      "\n",
      "Generated Text after epoch 39 ... \n",
      "======================== New text Segment ==========================\n",
      " .\n",
      "\n",
      "thot, and theres nose of the stairs. thought that. it wouldnt help her half a bunch of time to think of the stairs. thoughter, they were all things and if it wasnt there, and this is the stairs. i dont taking it was one of them. there isnt that something than this is that way to get her hands that theres not the protectively, and i dont think that you, harry.\n",
      "\n",
      "\t\t\ti stared at me. and then they were too much and started front of my back to find the blood of thoughtful time. i couldnt haven here than that there was a couple of particular. i could see that when theyd seen that had to be for a moment. then he started back to try to talk to her. though the moment to help me to her the power on them, and then she stared at me. then i complied the door and her eyes with me, and, and she fought of his eyes widened.\n",
      "\n",
      "\n",
      "\n",
      "\t\t\tand thought there was a second that she wasnt. it was nothing. i dont take. it was going to be, and theres not a second of whatever that they couldnt help me the\n",
      "====================================================================\n",
      "\n",
      "(315).(58).(215).(332).(274).(303).(242).(249).(226).(264).\n",
      "Average loss at step 41: 2.427842\n",
      "\tPerplexity at step 41: 11.334397\n",
      "\n",
      "Valid Perplexity: 21.60\n",
      "\n",
      "Generated Text after epoch 40 ... \n",
      "======================== New text Segment ==========================\n",
      " illfo shing around me. theres need to the from the white council. they arent to talk to them. thats not make it with there was a hand on there, and they werented that i would be going to bells. i do not your own.\n",
      "\n",
      "i know, i shrugged. something that we caused to be sure you that you could have have been that wasnt, i said. if you wanted to talk to things. then there were, i said quietly. but i wought about you.\n",
      "\n",
      "you've been there.\n",
      "\n",
      "youve gotten your from that? she said. her voice said in time to the wardens and her eyes, and the skull, and then she said. youll see three of thing.\n",
      "\n",
      "you've been there.\n",
      "\n",
      "youve gotten your from them. it wouldnt, i said. if you wanted to talk to things. then there was something whatever when you call that years. the next that youve got me in a sign of the complete with your own.\n",
      "\n",
      "i know, i shrugged. something that we caused to be sure you that you call that years. the next thing. i dont even they dont know. youve guarded the\n",
      "====================================================================\n",
      "\n",
      "(337).(42).(331).(330).(180).(55).(323).(128).(194).(205).\n",
      "Average loss at step 42: 2.426071\n",
      "\tPerplexity at step 42: 11.314345\n",
      "\n",
      "Valid Perplexity: 22.40\n",
      "\n",
      "Generated Text after epoch 41 ... \n",
      "======================== New text Segment ==========================\n",
      " t portuallow to her.\n",
      "\n",
      "shes going. theres probably wouldnt hear that this isnt everything is that i havent start to kemmer than that wasnt, i said.\n",
      "\n",
      "thats monder to talk to tell you there.\n",
      "\n",
      "where youre going to talk to that, i saited at the front of my staff, but her volver for a moment and-looking and started. whatev i want to do it.\n",
      "\n",
      "i stood up and said, her voice had been into the street into a second into the street into some kind as that i hadnt be able to stop me the way the street into his shoul. i had to help me a little. what is think youve look at me, i said. im sort of thing is things in that i would have been there.\n",
      "\n",
      "i stood up to his head. thanks.\n",
      "\n",
      "i shook myself.\n",
      "\n",
      "i looked at his hand, her voice walked aroke, and then she standed at me to him.\n",
      "\n",
      "im not some kind out of the council. he shook his head. there isnt, i said. im sorcere.\n",
      "\n",
      "i looked at his head. yeah, i muttered. everything, i said. im sorcere.\n",
      "\n",
      "i grunted. theres no open.\n",
      "\n",
      "i\n",
      "====================================================================\n",
      "\n",
      "(92).(284).(352).(340).(108).(5).(141).(248).(319).(371).\n",
      "Average loss at step 43: 2.456364\n",
      "\tPerplexity at step 43: 11.662331\n",
      "\n",
      "Valid Perplexity: 22.40\n",
      "\n",
      "Generated Text after epoch 42 ... \n",
      "======================== New text Segment ==========================\n",
      " heavrdourprises. theyve got to take their world, though they could be all they hadnt been trying to find them where the summer course, and those with your eyes stretched upon as they could belling the energy of his eyes widened.\n",
      "\n",
      "\t\t\tthere would be three thing. she was strong at thomas.\n",
      "\n",
      "\t\t\tshe stared at me an arms of time to seemed to see her throaded that the startination, and her eyes were start and stared at me another than standing up at me, then she stared at michael and she wasntered.\n",
      "\n",
      "\t\t\tit was a couple of black.\n",
      "\n",
      "\t\t\tand then she didnt look at thomas and her eyes went into a titaniatifully, but she was a lot of thing. they shouted to see them that she went to the starting into the stone of things, and then she would have have to be a lot of power of time to see her to that they had been ablock into this time, and then shook his head. theyll get that way to get that years?\n",
      "\n",
      "\t\t\ti stared at me.\n",
      "\n",
      "\t\t\tand then he said. thats they had been coming to consider and that isnt every\n",
      "====================================================================\n",
      "\n",
      "(206).(361).(153).(16).(110).(10).(307).(273).(324).(360).\n",
      "Average loss at step 44: 2.434396\n",
      "\tPerplexity at step 44: 11.408922\n",
      "\n",
      "Valid Perplexity: 22.63\n",
      "\n",
      "\t Reducing learning rate\n",
      "Generated Text after epoch 43 ... \n",
      "======================== New text Segment ==========================\n",
      " urchius, and there wasnt that there was a moment. it wasnt a lot of that would be, and the words were. it was that it worked it.\n",
      "\n",
      "\t\t\tthen i stared to me, but that would heard the street that there was no react that i was too much as if theyd been that i could mean the surface. i shook her shoulder at me. i know that you are you, he said quietly. that i want to be realized it into the world, anyway. that theres no one of time for a few minutes.\n",
      "\n",
      "\t\t\ti should me head and stopped and turned to me.\n",
      "\n",
      "\t\t\twhat?\n",
      "\n",
      "\t\t\ti shook myself, the room beford the sound of the word, the most of that should seemed to stop the shoulder. i stood up, butters went to the way that i could severed it too more through the back of that she was going to do with a couple of dangerous being the council, appeared them in the back of that shoulder. it was started to get it. if they were. it was that there was a long. theres black timed. thats heart thank you, anyway. the while the council, and it wasnesting the\n",
      "====================================================================\n",
      "\n",
      "(174).(115).(101).(333).(267).(149).(313).(291).(143).(325).\n",
      "Average loss at step 45: 2.477263\n",
      "\tPerplexity at step 45: 11.908622\n",
      "\n",
      "Valid Perplexity: 20.47\n",
      "\n",
      "Generated Text after epoch 44 ... \n",
      "======================== New text Segment ==========================\n",
      " teurysius apparently that was a place to help me to help me and said, a lot of that should have been something that wasntered that they were pretty supernatural world. there was a couple of lightning around me that it is there was no remaind in a smark of the streets of his feet, but it happened to do with a long of that she was going to do with this side of his shoul, harry. i wasnt a lot of the world. they were standing the power on deep was as that wouched her hands on a moment. then i said. i shook myself. you'll get things.\n",
      "\n",
      "yeah, i shrugged. something happened through thinking, and when youll see the one of this side, all that this is that she was this is that, if youll seen them in the power.\n",
      "\n",
      "yeah, i shrugged. they were going to be, i said against more and then said, what im nodded.\n",
      "\n",
      "what? i added at me from the back of through the walls of turned to thomas.\n",
      "\n",
      "\t\t\tit was a couple of angels of course, anymore. i would have got to be about that we were all the sam\n",
      "====================================================================\n",
      "\n",
      "(372).(307).(281).(67).(142).(303).(149).(87).(118).(187).\n",
      "Average loss at step 46: 2.464377\n",
      "\tPerplexity at step 46: 11.756157\n",
      "\n",
      "Valid Perplexity: 19.77\n",
      "\n",
      "Generated Text after epoch 45 ... \n",
      "======================== New text Segment ==========================\n",
      " ul\n",
      "**\n",
      "\n",
      "cote move managing that i had the side of my life, and then she would have touched through the dead, and her eyes went in the side of time. then they were all right hand. there's not a couple of more than i could have to be an all of many of them. then i had been anything. though the reason that. i wasntered the way that i could see her threatening to help them that they would be anything that i was all the same, and they were all right the first the first the first the wall of time. then they were all right now. it wasnt an intention of the same thing that she was a moment.\n",
      "\n",
      "\t\t\ti start to keep through the wardens around her face, and her eyes went into time. theres nothing that the reason that, i would see them that it was.\n",
      "\n",
      "\t\t\tit could have had been hit into time. then they were all right now. it wasnt an intention of his feet and then thought the first time.\n",
      "\n",
      "\t\t\ti shook my head, then shook her head. \"i don't know toward that would have been the first their was as well. they\n",
      "====================================================================\n",
      "\n",
      "(77).(125).(219).(143).(377).(333).(271).(160).(196).(242).\n",
      "Average loss at step 47: 2.391239\n",
      "\tPerplexity at step 47: 10.927024\n",
      "\n",
      "Valid Perplexity: 21.83\n",
      "\n",
      "Generated Text after epoch 46 ... \n",
      "======================== New text Segment ==========================\n",
      " s sipped to help me. i didnt happen if youve got to.\n",
      "\n",
      "i stayed at me and said, yeah.\n",
      "\n",
      "yeah, i shrugged. and youll seen them with yourself.\n",
      "\n",
      "i blinked at me and satisfying. there were, i said. if you have to be sure your people, he said.\n",
      "\n",
      "yeah, i snarled.\n",
      "\n",
      "yeah, i muttered. thats there.\n",
      "\n",
      "i grimaced.\n",
      "\n",
      "yeah.\n",
      "\n",
      "yeah, i shrugged. and youll see them out of the first time, i said.\n",
      "\n",
      "thats what youre all there, i said quietly. \"theres something have to be sure you think your process. she was a hand to me, his head, and then i said. if you can.\n",
      "\n",
      "i grinned at me and said, his head against my hand and then she which had takened to his hand and said, something i had too want to be for a moment.\n",
      "\n",
      "i shrugged. youve got out of that would happensive that wasnt, i said. if you were there than the council to help you called them.\n",
      "\n",
      "i grinted at them.\n",
      "\n",
      "i wents at me. then she watched the boat and then she wounded my voice.\n",
      "\n",
      "i thought of his hand at me. i have to be\n",
      "====================================================================\n",
      "\n",
      "(173).(71).(235).(353).(132).(106).(296).(351).(196).(30).\n",
      "Average loss at step 48: 2.420312\n",
      "\tPerplexity at step 48: 11.249366\n",
      "\n",
      "Valid Perplexity: 19.21\n",
      "\n",
      "Generated Text after epoch 47 ... \n",
      "======================== New text Segment ==========================\n",
      " n psychic start stronging, and i didn't happen if i had to take, then there was a couple of strength, but that would have have to be sure that shoulders. that is good than help. the white counder that isn't that yourself, what it wound here, something else, the way to the carry, thought that there was no really think it wouldn't be.\n",
      "\n",
      "\t\t\tthere wasn't realized town to the mortals, whatever too more thing that i'd been to think if it was an employee. when it would have taken a second, if i needed to reached through the council.\n",
      "\n",
      "\t\t\tit was reading in from the council. i had an enter than those times. i was a couple of strength, whatever is that it would be a breath an expressionally, there was a shoulder anymore. i dropped the right handle that times it wants to taking them, and then thought that. it was something else, the white counder that isnt an invisible. i didnt have to be able to see it that times it was, there watched it that it worried.\n",
      "\n",
      "i looked at his hand, stared and said, \"th\n",
      "====================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     feed_dict[train_labels[ui]] \u001b[39m=\u001b[39m lbl\n\u001b[1;32m     65\u001b[0m \u001b[39m# Running the TensorFlow operations\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m _, l, step_perplexity \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun([optimizer, loss, train_perplexity_without_exp], \n\u001b[1;32m     67\u001b[0m                                            feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Update doc_perpelxity variable\u001b[39;00m\n\u001b[1;32m     69\u001b[0m doc_perplexity \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m step_perplexity \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "\n",
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 50\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 380\n",
    "docs_per_step = 10\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>10*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==10:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress .. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # One one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (docs_per_step*steps_per_document*valid_summary)\n",
    "      \n",
    "      # Print loss\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "    \n",
    "      average_loss = 0 # reset loss\n",
    "        \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(10):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      beam_valid_perplexity_ot.append(v_perplexity)\n",
    "      \n",
    "      # Decay learning rate\n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end='')\n",
    "            \n",
    "        print(\"\")\n",
    "        session.run([reset_sample_beam_state])\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "    \n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(beam_train_perplexity_ot)\n",
    "    writer.writerow(beam_valid_perplexity_ot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d2c3aede024c75a749a8ab667e988a1aa49331727bd90cb9374b3ad261b50b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
