{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow as tf\n",
    "tf.disable_v2_behavior()\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  dresden_files\\01.txt\n",
      "File  01.txt  already exists.\n",
      "Downloading file:  dresden_files\\02.txt\n",
      "File  02.txt  already exists.\n",
      "Downloading file:  dresden_files\\03.txt\n",
      "File  03.txt  already exists.\n",
      "Downloading file:  dresden_files\\04.txt\n",
      "File  04.txt  already exists.\n",
      "Downloading file:  dresden_files\\05.txt\n",
      "File  05.txt  already exists.\n",
      "Downloading file:  dresden_files\\06.txt\n",
      "File  06.txt  already exists.\n",
      "Downloading file:  dresden_files\\07.txt\n",
      "File  07.txt  already exists.\n",
      "Downloading file:  dresden_files\\08.txt\n",
      "File  08.txt  already exists.\n",
      "Downloading file:  dresden_files\\09.txt\n",
      "File  09.txt  already exists.\n",
      "Downloading file:  dresden_files\\10.txt\n",
      "File  10.txt  already exists.\n",
      "Downloading file:  dresden_files\\11.txt\n",
      "File  11.txt  already exists.\n",
      "Downloading file:  dresden_files\\12.txt\n",
      "File  12.txt  already exists.\n",
      "Downloading file:  dresden_files\\13.txt\n",
      "File  13.txt  already exists.\n",
      "Downloading file:  dresden_files\\14.txt\n",
      "File  14.txt  already exists.\n",
      "Downloading file:  dresden_files\\15.txt\n",
      "File  15.txt  already exists.\n",
      "Downloading file:  dresden_files\\16.txt\n",
      "File  16.txt  already exists.\n",
      "Downloading file:  dresden_files\\17.txt\n",
      "File  17.txt  already exists.\n",
      "Downloading file:  dresden_files\\18.txt\n",
      "File  18.txt  already exists.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/mscrnt/dwd/master/dresden_files/'\n",
    "\n",
    "# Create a directory if needed\n",
    "dir_name = 'dresden_files'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download a file if not present\"\"\"\n",
    "  print('Downloading file: ', dir_name+ os.sep+filename)\n",
    "    \n",
    "  if not os.path.exists(dir_name+os.sep+filename):\n",
    "    filename, _ = urlretrieve(url + filename, dir_name+os.sep+filename)\n",
    "  else:\n",
    "    print('File ',filename, ' already exists.')\n",
    "  \n",
    "  return filename\n",
    "\n",
    "num_files = 18\n",
    "filenames = [format(i, '02d')+'.txt' for i in range(1,19)]\n",
    "\n",
    "for fn in filenames:\n",
    "    maybe_download(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 files found.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print('%d files found.'%len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file dresden_files\\01.txt\n",
      "Data size (Characters) (Document 0) 235007\n",
      "Sample string (Document 0) ['\\n\\n', 'ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\ni', ' h', 'ea', 'rd', ' t', 'he', ' m', 'ai', 'lm', 'an', ' a', 'pp', 'ro', 'ac', 'h ', 'my', ' o', 'ff', 'ic', 'e ', 'do', 'or', ', ', 'ha', 'lf', ' a', 'n ', 'ho', 'ur', ' e', 'ar', 'li', 'er', ' t', 'ha', 'n ', 'us', 'ua', 'l.', ' h', 'e ', 'di', 'dn']\n",
      "\n",
      "Processing file dresden_files\\02.txt\n",
      "Data size (Characters) (Document 1) 274440\n",
      "Sample string (Document 1) ['ch', 'ap', 'te', 'r ', '1\\n', '\\ni', ' n', 'ev', 'er', ' u', 'se', 'd ', 'to', ' k', 'ee', 'p ', 'cl', 'os', 'e ', 'tr', 'ac', 'k ', 'of', ' t', 'he', ' p', 'ha', 'se', 's ', 'of', ' t', 'he', ' m', 'oo', 'n.', ' s', 'o ', 'i ', 'di', 'dn', \"'t\", ' k', 'no', 'w ', 'th', 'at', ' i', 't ', 'wa', 's ']\n",
      "\n",
      "Processing file dresden_files\\03.txt\n",
      "Data size (Characters) (Document 2) 316689\n",
      "Sample string (Document 2) ['th', 'er', 'e ', 'ar', 'e ', 're', 'as', 'on', 's ', 'i ', 'ha', 'te', ' t', 'o ', 'dr', 'iv', 'e ', 'fa', 'st', '. ', 'fo', 'r ', 'on', 'e,', ' t', 'he', ' b', 'lu', 'e ', 'be', 'et', 'le', ', ', 'th', 'e ', 'mi', 'sm', 'at', 'ch', 'ed', ' v', 'ol', 'ks', 'wa', 'ge', 'n ', 'bu', 'g ', 'th', 'at']\n",
      "\n",
      "Processing file dresden_files\\04.txt\n",
      "Data size (Characters) (Document 3) 306204\n",
      "Sample string (Document 3) ['it', ' r', 'ai', 'ne', 'd ', 'to', 'ad', 's ', 'th', 'e ', 'da', 'y ', 'th', 'e ', 'wh', 'it', 'e ', 'co', 'un', 'ci', 'l ', 'ca', 'me', ' t', 'o ', 'to', 'wn', '.\\n', '\\ni', ' g', 'ot', ' o', 'ut', ' o', 'f ', 'th', 'e ', 'bl', 'ue', ' b', 'ee', 'tl', 'e,', ' m', 'y ', 'be', 'at', '-u', 'p ', 'ol']\n",
      "\n",
      "Processing file dresden_files\\05.txt\n",
      "Data size (Characters) (Document 4) 295602\n",
      "Sample string (Document 4) ['so', 'me', ' t', 'hi', 'ng', 's ', 'ju', 'st', ' a', 're', \"n'\", 't ', 'me', 'an', 't ', 'to', ' g', 'o ', 'to', 'ge', 'th', 'er', '. ', 'th', 'in', 'gs', ' l', 'ik', 'e ', 'oi', 'l ', 'an', 'd ', 'wa', 'te', 'r.', ' o', 'ra', 'ng', 'e ', 'ju', 'ic', 'e ', 'an', 'd ', 'to', 'ot', 'hp', 'as', 'te']\n",
      "\n",
      "Processing file dresden_files\\06.txt\n",
      "Data size (Characters) (Document 5) 332768\n",
      "Sample string (Document 5) ['th', 'e ', 'bu', 'il', 'di', 'ng', ' w', 'as', ' o', 'n ', 'fi', 're', ', ', 'an', 'd ', 'it', ' w', 'as', \"n'\", 't ', 'my', ' f', 'au', 'lt', '.\\n', '\\nm', 'y ', 'bo', 'ot', 's ', 'sl', 'ip', 'pe', 'd ', 'an', 'd ', 'sl', 'id', ' o', 'n ', 'th', 'e ', 'ti', 'le', ' f', 'lo', 'or', ' a', 's ', 'i ']\n",
      "\n",
      "Processing file dresden_files\\07.txt\n",
      "Data size (Characters) (Document 6) 395437\n",
      "Sample string (Document 6) ['\\no', 'n ', 'th', 'e ', 'wh', 'ol', 'e,', ' w', \"e'\", 're', ' a', ' m', 'ur', 'de', 'ro', 'us', ' r', 'ac', 'e.', ' a', 'cc', 'or', 'di', 'ng', ' t', 'o ', 'ge', 'ne', 'si', 's,', ' i', 't ', 'to', 'ok', ' a', 's ', 'fe', 'w ', 'as', ' f', 'ou', 'r ', 'pe', 'op', 'le', ' t', 'o ', 'ma', 'ke', ' t']\n",
      "\n",
      "Processing file dresden_files\\08.txt\n",
      "Data size (Characters) (Document 7) 416316\n",
      "Sample string (Document 7) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\nb', 'lo', 'od', ' l', 'ea', 've', 's ', 'no', ' s', 'ta', 'in', ' o', 'n ', 'a ', 'wa', 'rd', 'en', '’s', ' g', 're', 'y ', 'cl', 'oa', 'k.', ' i', ' d', 'id', 'n’', 't ', 'kn', 'ow', ' t', 'ha', 't ', 'un', 'ti', 'l ', 'th', 'e ', 'da', 'y ', 'i ', 'wa', 'tc']\n",
      "\n",
      "Processing file dresden_files\\09.txt\n",
      "Data size (Characters) (Document 8) 361760\n",
      "Sample string (Document 8) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\n\\n', '\\n\\n', '\\nm', 'an', 'y ', 'th', 'in', 'gs', ' a', 're', ' n', 'ot', ' a', 's ', 'th', 'ey', ' s', 'ee', 'm:', ' t', 'he', ' w', 'or', 'st', ' t', 'hi', 'ng', 's ', 'in', ' l', 'if', 'e ', 'ne', 've', 'r ', 'ar', 'e.', '\\n\\n', 'i ', 'pu', 'll', 'ed', ' m', 'y ']\n",
      "\n",
      "Processing file dresden_files\\10.txt\n",
      "Data size (Characters) (Document 9) 382445\n",
      "Sample string (Document 9) ['\\nc', 'ha', 'pt', 'er', ' o', 'ne', '\\n\\n', '\\nw', ' i', 'nt', 'er', ' c', 'am', 'e ', 'ea', 'rl', 'y ', 'th', 'at', ' y', 'ea', 'r;', ' i', 't ', 'sh', 'ou', 'ld', ' h', 'av', 'e ', 'be', 'en', ' a', ' t', 'ip', '‐o', 'ff', '.\\n', '\\na', ' s', 'no', 'wb', 'al', 'l ', 'so', 'ar', 'ed', ' t', 'hr', 'ou']\n",
      "\n",
      "Processing file dresden_files\\11.txt\n",
      "Data size (Characters) (Document 10) 387128\n",
      "Sample string (Document 10) ['\\nc', 'ha', 'pt', 'er', ' o', 'ne', '\\n\\n', '\\nt', 'he', ' s', 'um', 'me', 'r ', 'su', 'n ', 'wa', 's ', 'bu', 'sy', ' b', 'ro', 'il', 'in', 'g ', 'th', 'e ', 'as', 'ph', 'al', 't ', 'fr', 'om', ' c', 'hi', 'ca', 'go', '’s', ' s', 'tr', 'ee', 'ts', ', ', 'th', 'e ', 'ag', 'on', 'y ', 'in', ' m', 'y ']\n",
      "\n",
      "Processing file dresden_files\\12.txt\n",
      "Data size (Characters) (Document 11) 404594\n",
      "Sample string (Document 11) ['i ', 'an', 'sw', 'er', 'ed', ' t', 'he', ' p', 'ho', 'ne', ', ', 'an', 'd ', 'su', 'sa', 'n ', 'ro', 'dr', 'ig', 'ue', 'z ', 'sa', 'id', ', ', '“t', 'he', 'y’', 've', ' t', 'ak', 'en', ' o', 'ur', ' d', 'au', 'gh', 'te', 'r.', '”\\n', '\\ni', ' s', 'at', ' t', 'he', 're', ' f', 'or', ' a', ' l', 'on']\n",
      "\n",
      "Processing file dresden_files\\13.txt\n",
      "Data size (Characters) (Document 12) 444631\n",
      "Sample string (Document 12) ['ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\nl', 'if', 'e ', 'is', ' h', 'ar', 'd.', '\\n\\n', 'dy', 'in', 'g’', 's ', 'ea', 'sy', '.\\n', '\\ns', 'o ', 'ma', 'ny', ' t', 'hi', 'ng', 's ', 'mu', 'st', ' a', 'li', 'gn', ' i', 'n ', 'or', 'de', 'r ', 'to', ' c', 're', 'at', 'e ', 'li', 'fe', '. ', 'it', ' h', 'as']\n",
      "\n",
      "Processing file dresden_files\\14.txt\n",
      "Data size (Characters) (Document 13) 472438\n",
      "Sample string (Document 13) ['\\n\\n', '\\no', 'ne', '\\n\\n', '\\n\\n', '\\n\\n', 'ma', 'b,', ' t', 'he', ' q', 'ue', 'en', ' o', 'f ', 'ai', 'r ', 'an', 'd ', 'da', 'rk', 'ne', 'ss', ', ', 'mo', 'na', 'rc', 'h ', 'of', ' t', 'he', ' w', 'in', 'te', 'r ', 'co', 'ur', 't ', 'of', ' t', 'he', ' s', 'id', 'he', ', ', 'ha', 's ', 'un', 'iq', 'ue']\n",
      "\n",
      "Processing file dresden_files\\15.txt\n",
      "Data size (Characters) (Document 14) 420052\n",
      "Sample string (Document 14) ['on', 'e\\n', '\\n\\n', '\\t\\t', '\\tt', 'he', 're', ' w', 'as', ' a', ' t', 'ic', 'ki', 'ng', ' t', 'im', 'e ', 'bo', 'mb', ' i', 'ns', 'id', 'e ', 'my', ' h', 'ea', 'd ', 'an', 'd ', 'th', 'e ', 'on', 'e ', 'pe', 'rs', 'on', ' i', ' t', 'ru', 'st', 'ed', ' t', 'o ', 'go', ' i', 'n ', 'an', 'd ', 'ge', 't ']\n",
      "\n",
      "Processing file dresden_files\\16.txt\n",
      "Data size (Characters) (Document 15) 310128\n",
      "Sample string (Document 15) ['1\\n', '\\n\\n', 'my', ' b', 'ro', 'th', 'er', ' r', 'ui', 'ne', 'd ', 'a ', 'pe', 'rf', 'ec', 'tl', 'y ', 'go', 'od', ' r', 'un', ' b', 'y ', 'sa', 'yi', 'ng', ', ', '“j', 'us', 'ti', 'ne', ' i', 's ', 'pr', 'eg', 'na', 'nt', '.”', '\\n\\n', 'th', 'at', ' k', 'ic', 'ke', 'd ', 'me', ' c', 'om', 'pl', 'et']\n",
      "\n",
      "Processing file dresden_files\\17.txt\n",
      "Data size (Characters) (Document 16) 773007\n",
      "Sample string (Document 16) ['<<', '<<', '<<', '< ', 'he', 'ad', '\\n\\n', 'ch', 'ap', 'te', 'r ', 'on', 'e\\n', '\\n\\n', '\\na', 'po', 'ca', 'ly', 'ps', 'es', ' a', 'lw', 'ay', 's ', 'ki', 'ck', ' o', 'ff', ' a', 't ', 'th', 'e ', 'wi', 'tc', 'hi', 'ng', ' h', 'ou', 'r.', '\\n\\n', '\\t\\t', '\\tt', 'ha', 't’', 's ', 'so', 'me', 'th', 'in', 'g ']\n",
      "\n",
      "Processing file dresden_files\\18.txt\n",
      "Data size (Characters) (Document 17) 11017\n",
      "Sample string (Document 17) ['’t', 'wa', 's ', 'th', 'e ', 'ni', 'gh', 't ', 'be', 'fo', 're', ' c', 'hr', 'is', 'tm', 'as', ', ', 'an', 'd ', 'al', 'l ', 'th', 'ro', 'ug', 'h ', 'th', 'e ', 'ho', 'us', 'e,', ' n', 'ot', ' a', ' c', 're', 'at', 'ur', 'e ', 'wa', 's ', 'st', 'ir', 'ri', 'ng', ' e', 'xc', 'ep', 't ', 'me', ' a']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  \n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    # make all the words lower case\n",
    "    data = data.lower()\n",
    "    data = list(data)\n",
    "  return data\n",
    "\n",
    "documents = []\n",
    "global documents\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    chars = read_data(os.path.join(dir_name,filenames[i]))\n",
    "    \n",
    "    # Break the data into bigrams\n",
    "    two_grams = [''.join(chars[ch_i:ch_i+2]) for ch_i in range(0,len(chars)-2,2)]\n",
    "    # Create a list of lists with bigrams\n",
    "    documents.append(two_grams)\n",
    "    print('Data size (Characters) (Document %d) %d' %(i,len(two_grams)))\n",
    "    print('Sample string (Document %d) %s'%(i,two_grams[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6539663 Characters found.\n",
      "Most common words (+UNK) [('e ', 184030), (' t', 160516), ('th', 136291), ('he', 134975), ('d ', 133275)]\n",
      "Least common words (+UNK) [('3-', 1), ('c”', 1), ('??', 1), ('.0', 1), ('3r', 1), ('n3', 1), ('\\n=', 1), ('öd', 1), ('¿q', 1), ('ué', 1), ('’w', 1), ('h”', 1), ('l)', 1), (',0', 1), ('b3', 1)]\n",
      "Sample data [30, 106, 214, 50, 28, 36, 671, 224, 14, 54]\n",
      "Sample data [106, 214, 50, 28, 0, 224, 97, 185, 13, 131]\n",
      "Vocabulary:  1009\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(documents):\n",
    "    chars = []\n",
    "    # This is going to be a list of lists\n",
    "    # Where the outer list denote each document\n",
    "    # and the inner lists denote words in a given document\n",
    "    data_list = []\n",
    "  \n",
    "    for d in documents:\n",
    "        chars.extend(d)\n",
    "    print('%d Characters found.'%len(chars))\n",
    "    count = []\n",
    "    # Get the bigram sorted by their frequency (Highest comes first)\n",
    "    count.extend(collections.Counter(chars).most_common())\n",
    "    \n",
    "    # Create an ID for each bigram by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    # Start with 'UNK' that is assigned to too rare words\n",
    "    dictionary = dict({'UNK':0})\n",
    "    for char, c in count:\n",
    "        # Only add a bigram to dictionary if its frequency is more than 10\n",
    "        if c > 10:\n",
    "            dictionary[char] = len(dictionary)    \n",
    "    \n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have\n",
    "    # to replace each string word with the ID of the word\n",
    "    for d in documents:\n",
    "        data = list()\n",
    "        for char in d:\n",
    "            # If word is in the dictionary use the word ID,\n",
    "            # else use the ID of the special token \"UNK\"\n",
    "            if char in dictionary:\n",
    "                index = dictionary[char]        \n",
    "            else:\n",
    "                index = dictionary['UNK']\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "        \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary,vocabulary_size\n",
    "\n",
    "# Print some statistics about data\n",
    "data_list, count, dictionary, reverse_dictionary = build_dataset(documents)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Least common words (+UNK)', count[-15:])\n",
    "print('Sample data', data_list[0][:10])\n",
    "print('Sample data', data_list[1][:10])\n",
    "print('Vocabulary: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "del documents  # To reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:\n",
      "\tic (116), \tha (27), \tur (86), \t t (2), \tl. (303), \n",
      "\tOutput:\n",
      "\te  (1), \tlf (302), \t e (78), \tha (27), \t h (14), \n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:\n",
      "\te  (1), \tlf (302), \t e (78), \tha (27), \t h (14), \n",
      "\tOutput:\n",
      "\tdo (124), \t a (6), \tar (39), \tn  (21), \te  (1), \n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:\n",
      "\tdo (124), \t a (6), \tar (39), \tn  (21), \te  (1), \n",
      "\tOutput:\n",
      "\tor (42), \tn  (21), \tli (69), \tus (102), \tdi (134), \n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:\n",
      "\tor (42), \tn  (21), \tli (69), \tus (102), \tdi (134), \n",
      "\tOutput:\n",
      "\t,  (16), \tho (80), \ter (13), \tua (345), \tdn (256), \n",
      "\n",
      "Unrolled index 4\n",
      "\tInputs:\n",
      "\t,  (16), \tho (80), \ter (13), \tua (345), \tl. (303), \n",
      "\tOutput:\n",
      "\tha (27), \tur (86), \t t (2), \tl. (303), \t h (14), "
     ]
    }
   ],
   "source": [
    "class DataGeneratorOHE(object):\n",
    "    \n",
    "    def __init__(self,text,batch_size,num_unroll):\n",
    "        # Text where a bigram is denoted by its ID\n",
    "        self._text = text\n",
    "        # Number of bigrams in the text\n",
    "        self._text_size = len(self._text)\n",
    "        # Number of datapoints in a batch of data\n",
    "        self._batch_size = batch_size\n",
    "        # Num unroll is the number of steps we unroll the RNN in a single training step\n",
    "        # This relates to the truncated backpropagation we discuss in Chapter 6 text\n",
    "        self._num_unroll = num_unroll\n",
    "        # We break the text in to several segments and the batch of data is sampled by\n",
    "        # sampling a single item from a single segment\n",
    "        self._segments = self._text_size//self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "    def next_batch(self):\n",
    "        '''\n",
    "        Generates a single batch of data\n",
    "        '''\n",
    "        # Train inputs (one-hot-encoded) and train outputs (one-hot-encoded)\n",
    "        batch_data = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size,vocabulary_size),dtype=np.float32)\n",
    "        \n",
    "        # Fill in the batch datapoint by datapoint\n",
    "        for b in range(self._batch_size):\n",
    "            # If the cursor of a given segment exceeds the segment length\n",
    "            # we reset the cursor back to the beginning of that segment\n",
    "            if self._cursor[b]+1>=self._text_size:\n",
    "                self._cursor[b] = b * self._segments\n",
    "            \n",
    "            # Add the text at the cursor as the input\n",
    "            batch_data[b,self._text[self._cursor[b]]] = 1.0\n",
    "            # Add the preceding bigram as the label to be predicted\n",
    "            batch_labels[b,self._text[self._cursor[b]+1]]= 1.0                       \n",
    "            # Update the cursor\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._text_size\n",
    "                    \n",
    "        return batch_data,batch_labels\n",
    "        \n",
    "    def unroll_batches(self):\n",
    "        '''\n",
    "        This produces a list of num_unroll batches\n",
    "        as required by a single step of training of the RNN\n",
    "        '''\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        for ui in range(self._num_unroll):\n",
    "            data, labels = self.next_batch()            \n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "        \n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "    def reset_indices(self):\n",
    "        '''\n",
    "        Used to reset all the cursors if needed\n",
    "        '''\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "# Running a tiny set to see if things are correct\n",
    "dg = DataGeneratorOHE(data_list[0][25:50],5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "# Iterate through each data batch in the unrolled set of batches\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = np.argmax(dat,axis=1)\n",
    "    lbl_ind = np.argmax(lbl,axis=1)\n",
    "    print('\\tInputs:')\n",
    "    for sing_dat in dat_ind:\n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_dat],sing_dat),end=\", \")\n",
    "    print('\\n\\tOutput:')\n",
    "    for sing_lbl in lbl_ind:        \n",
    "        print('\\t%s (%d)'%(reverse_dictionary[sing_lbl],sing_lbl),end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neurons in the hidden state variables\n",
    "num_nodes = 128\n",
    "\n",
    "# Number of data points in a batch we process\n",
    "batch_size = 64\n",
    "\n",
    "# Number of time steps we unroll for during optimization\n",
    "num_unrollings = 50\n",
    "\n",
    "dropout = 0.0 # We use dropout\n",
    "\n",
    "# Use this in the CSV filename when saving\n",
    "# when using dropout\n",
    "filename_extension = ''\n",
    "if dropout>0.0:\n",
    "    filename_extension = '_dropout'\n",
    "    \n",
    "filename_to_save = 'lstm'+filename_extension+'.csv' # use to save perplexity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Training Input data.\n",
    "train_inputs, train_labels = [],[]\n",
    "\n",
    "# Defining unrolled training inputs\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size],name='train_inputs_%d'%ui))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size], name = 'train_labels_%d'%ui))\n",
    "\n",
    "# Validation data placeholders\n",
    "valid_inputs = tf.placeholder(tf.float32, shape=[1,vocabulary_size],name='valid_inputs')\n",
    "valid_labels = tf.placeholder(tf.float32, shape=[1,vocabulary_size], name = 'valid_labels')\n",
    "# Text generation: batch 1, no unrolling.\n",
    "test_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size], name = 'test_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input gate (i_t) - How much memory to write to cell state\n",
    "# Connects the current input to the input gate\n",
    "ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the input gate\n",
    "im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the input gate\n",
    "ib = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Forget gate (f_t) - How much memory to discard from cell state\n",
    "# Connects the current input to the forget gate\n",
    "fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the forget gate\n",
    "fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the forget gate\n",
    "fb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02, 0.02))\n",
    "\n",
    "# Candidate value (c~_t) - Used to compute the current cell state\n",
    "# Connects the current input to the candidate\n",
    "cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the candidate\n",
    "cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the candidate\n",
    "cb = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "# Output gate (o_t) - How much memory to output from the cell state\n",
    "# Connects the current input to the output gate\n",
    "ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], stddev=0.02))\n",
    "# Connects the previous hidden state to the output gate\n",
    "om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], stddev=0.02))\n",
    "# Bias of the output gate\n",
    "ob = tf.Variable(tf.random_uniform([1, num_nodes],-0.02,0.02))\n",
    "\n",
    "\n",
    "# Softmax Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], stddev=0.02))\n",
    "b = tf.Variable(tf.random_uniform([vocabulary_size],-0.02,0.02))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "# Hidden state\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_hidden')\n",
    "# Cell state\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False, name='train_cell')\n",
    "\n",
    "# Same variables for validation phase\n",
    "saved_valid_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_hidden')\n",
    "saved_valid_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='valid_cell')\n",
    "\n",
    "# Same variables for testing phase\n",
    "saved_test_output = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_hidden')\n",
    "saved_test_state = tf.Variable(tf.zeros([1, num_nodes]),trainable=False, name='test_cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"Create an LSTM cell\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "#Training related inference logic\n",
    "\n",
    "# Keeps the calculated state outputs in all the unrollings\n",
    "# Used to calculate loss\n",
    "outputs = list()\n",
    "\n",
    "# These two python variables are iteratively updated\n",
    "# at each step of unrolling\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "\n",
    "# Compute the hidden state (output) and cell state (state)\n",
    "# recursively for all the steps in unrolling\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    output = tf.nn.dropout(output,keep_prob=1.0-dropout)\n",
    "    # Append each computed output value\n",
    "    outputs.append(output)\n",
    "\n",
    "# calculate the score values\n",
    "logits = tf.matmul(tf.concat(axis=0, values=outputs), w) + b\n",
    "    \n",
    "# Compute predictions.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Compute training perplexity\n",
    "train_perplexity_without_exp = tf.reduce_sum(tf.concat(train_labels,0)*-tf.log(tf.concat(train_prediction,0)+1e-10))/(num_unrollings*batch_size)\n",
    "\n",
    "# ========================================================================\n",
    "# Validation phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for validation data\n",
    "valid_output, valid_state = lstm_cell(\n",
    "    valid_inputs, saved_valid_output, saved_valid_state)\n",
    "# Compute the logits\n",
    "valid_logits = tf.nn.xw_plus_b(valid_output, w, b)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_valid_output.assign(valid_output),\n",
    "                            saved_valid_state.assign(valid_state)]):\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "# Compute validation perplexity\n",
    "valid_perplexity_without_exp = tf.reduce_sum(valid_labels*-tf.log(valid_prediction+1e-10))\n",
    "\n",
    "# ========================================================================\n",
    "# Testing phase related inference logic\n",
    "\n",
    "# Compute the LSTM cell output for testing data\n",
    "test_output, test_state = lstm_cell(\n",
    "test_input, saved_test_output, saved_test_state)\n",
    "\n",
    "# Make sure that the state variables are updated\n",
    "# before moving on to the next iteration of generation\n",
    "with tf.control_dependencies([saved_test_output.assign(test_output),\n",
    "                            saved_test_state.assign(test_state)]):\n",
    "    test_prediction = tf.nn.softmax(tf.nn.xw_plus_b(test_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before calcualting the training loss,\n",
    "# save the hidden state and the cell state to\n",
    "# their respective TensorFlow variables\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                            saved_state.assign(state)]):\n",
    "\n",
    "    # Calculate the training loss by\n",
    "    # concatenating the results from all the unrolled time steps\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=tf.concat(axis=0, values=train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "gstep = tf.Variable(0,trainable=False,name='global_step')\n",
    "\n",
    "# Running this operation will cause the value of gstep\n",
    "# to increase, while in turn reducing the learning rate\n",
    "inc_gstep = tf.assign(gstep, gstep+1)\n",
    "\n",
    "# Decays learning rate everytime the gstep increases\n",
    "tf_learning_rate = tf.train.exponential_decay(0.001,gstep,decay_steps=1, decay_rate=0.5)\n",
    "\n",
    "# Adam Optimizer. And gradient clipping.\n",
    "optimizer = tf.train.AdamOptimizer(tf_learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "# Clipping gradients\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train state\n",
    "reset_train_state = tf.group(tf.assign(saved_state, tf.zeros([batch_size, num_nodes])),\n",
    "                          tf.assign(saved_output, tf.zeros([batch_size, num_nodes])))\n",
    "\n",
    "# Reset valid state\n",
    "reset_valid_state = tf.group(tf.assign(saved_valid_state, tf.zeros([1, num_nodes])),\n",
    "                          tf.assign(saved_valid_output, tf.zeros([1, num_nodes])))\n",
    "\n",
    "# Reset test state\n",
    "reset_test_state = tf.group(\n",
    "    saved_test_output.assign(tf.random_normal([1, num_nodes],stddev=0.05)),\n",
    "    saved_test_state.assign(tf.random_normal([1, num_nodes],stddev=0.05)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(distribution):\n",
    "  '''Greedy Sampling\n",
    "  We pick the three best predictions given by the LSTM and sample\n",
    "  one of them with very high probability of picking the best one'''\n",
    "\n",
    "  best_inds = np.argsort(distribution)[-3:]\n",
    "  best_probs = distribution[best_inds]/np.sum(distribution[best_inds])\n",
    "  best_idx = np.random.choice(best_inds,p=best_probs)\n",
    "  return best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Global Variables \n"
     ]
    }
   ],
   "source": [
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 0\n",
    "steps_per_document = 1\n",
    "valid_summary = 1\n",
    "train_doc_count = 18\n",
    "docs_per_step = 2\n",
    "\n",
    "\n",
    "# Capture the behavior of train perplexity over time\n",
    "train_perplexity_ot = []\n",
    "valid_perplexity_ot = []\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized Global Variables ')\n",
    "\n",
    "average_loss = 0 # Calculates the average loss ever few steps\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>2*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==2:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    print('Training (Step: %d)'%step,end=' ')\n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress .. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "        # resetting hidden state after processing a single document\n",
    "        # It's still questionable if this adds value in terms of learning\n",
    "        # One one hand it's intuitive to reset the state when learning a new document\n",
    "        # On the other hand this approach creates a bias for the state to be zero\n",
    "        # We encourage the reader to investigate further the effect of resetting the state\n",
    "        #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    # Generate new samples\n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (valid_summary*docs_per_step*steps_per_document)\n",
    "      \n",
    "      # Print losses\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      train_perplexity_ot.append(np.exp(average_loss))\n",
    "        \n",
    "      average_loss = 0 # reset loss\n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(2):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      valid_perplexity_ot.append(v_perplexity)\n",
    "          \n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "        \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        \n",
    "        # Start with a random word\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\\t\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        # Generating words within a segment by feeding in the previous prediction\n",
    "        # as the current input in a recursive manner\n",
    "        for _ in range(chars_in_segment):    \n",
    "          sample_pred = session.run(test_prediction, feed_dict = {test_input:test_word})  \n",
    "          next_ind = sample(sample_pred.ravel())\n",
    "          test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "          test_word[0,next_ind] = 1.0\n",
    "          print(reverse_dictionary[next_ind],end='')\n",
    "        print(\"\")\n",
    "        \n",
    "        # Reset train state\n",
    "        session.run(reset_test_state)\n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "\n",
    "# Write training and validation perplexities to a csv file\n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(train_perplexity_ot)\n",
    "    writer.writerow(valid_perplexity_ot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_length = 5 # number of steps to look ahead\n",
    "beam_neighbors = 5 # number of neighbors to compare to at each step\n",
    "\n",
    "# We redefine the sample generation with beam search\n",
    "sample_beam_inputs = [tf.placeholder(tf.float32, shape=[1, vocabulary_size]) for _ in range(beam_neighbors)]\n",
    "\n",
    "best_beam_index = tf.placeholder(shape=None, dtype=tf.int32)\n",
    "best_neighbor_beam_indices = tf.placeholder(shape=[beam_neighbors], dtype=tf.int32)\n",
    "\n",
    "# Maintains output of each beam\n",
    "saved_sample_beam_output = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "# Maintains the state of each beam\n",
    "saved_sample_beam_state = [tf.Variable(tf.zeros([1, num_nodes])) for _ in range(beam_neighbors)]\n",
    "\n",
    "# Resetting the sample beam states (should be done at the beginning of each text snippet generation)\n",
    "reset_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.zeros([1, num_nodes])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We stack them to perform gather operation below\n",
    "stacked_beam_outputs = tf.stack(saved_sample_beam_output)\n",
    "stacked_beam_states = tf.stack(saved_sample_beam_state)\n",
    "\n",
    "# The beam states for each beam (there are beam_neighbor-many beams) needs to be updated at every depth of tree\n",
    "# Consider an example where you have 3 classes where we get the best two neighbors (marked with star)\n",
    "#     a`      b*       c  \n",
    "#   / | \\   / | \\    / | \\\n",
    "#  a  b c  a* b` c  a  b  c\n",
    "# Since both the candidates from level 2 comes from the parent b\n",
    "# We need to update both states/outputs from saved_sample_beam_state/output to have index 1 (corresponding to parent b)\n",
    "update_sample_beam_state = tf.group(\n",
    "    *[saved_sample_beam_output[vi].assign(tf.gather_nd(stacked_beam_outputs,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)],\n",
    "    *[saved_sample_beam_state[vi].assign(tf.gather_nd(stacked_beam_states,[best_neighbor_beam_indices[vi]])) for vi in range(beam_neighbors)]\n",
    ")\n",
    "\n",
    "# We calculate lstm_cell state and output for each beam\n",
    "sample_beam_outputs, sample_beam_states = [],[] \n",
    "for vi in range(beam_neighbors):\n",
    "    tmp_output, tmp_state = lstm_cell(\n",
    "        sample_beam_inputs[vi], saved_sample_beam_output[vi], saved_sample_beam_state[vi]\n",
    "    )\n",
    "    sample_beam_outputs.append(tmp_output)\n",
    "    sample_beam_states.append(tmp_state)\n",
    "\n",
    "# For a given set of beams, outputs a list of prediction vectors of size beam_neighbors\n",
    "# each beam having the predictions for full vocabulary\n",
    "sample_beam_predictions = []\n",
    "for vi in range(beam_neighbors):\n",
    "    with tf.control_dependencies([saved_sample_beam_output[vi].assign(sample_beam_outputs[vi]),\n",
    "                                saved_sample_beam_state[vi].assign(sample_beam_states[vi])]):\n",
    "        sample_beam_predictions.append(tf.nn.softmax(tf.nn.xw_plus_b(sample_beam_outputs[vi], w, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay related\n",
    "# If valid perpelxity does not decrease\n",
    "# continuously for this many epochs\n",
    "# decrease the learning rate\n",
    "decay_threshold = 5\n",
    "# Keep counting perplexity increases\n",
    "decay_count = 0\n",
    "\n",
    "min_perplexity = 1e10\n",
    "\n",
    "# Learning rate decay logic\n",
    "def decay_learning_rate(session, v_perplexity):\n",
    "  global decay_threshold, decay_count, min_perplexity  \n",
    "  # Decay learning rate\n",
    "  if v_perplexity < min_perplexity:\n",
    "    decay_count = 0\n",
    "    min_perplexity= v_perplexity\n",
    "  else:\n",
    "    decay_count += 1\n",
    "\n",
    "  if decay_count >= decay_threshold:\n",
    "    print('\\t Reducing learning rate')\n",
    "    decay_count = 0\n",
    "    session.run(inc_gstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = None\n",
    "\n",
    "def get_beam_prediction(session):\n",
    "    \n",
    "    # Generating words within a segment with Beam Search\n",
    "    # To make some calculations clearer, we use the example as follows\n",
    "    # We have three classes with beam_neighbors=2 (best candidate denoted by *, second best candidate denoted by `)\n",
    "    # For simplicity we assume best candidate always have probability of 0.5 in output prediction\n",
    "    # second best has 0.2 output prediction\n",
    "    #           a`                   b*                   c                <--- root level\n",
    "    #    /     |     \\         /     |     \\        /     |     \\   \n",
    "    #   a      b      c       a*     b`     c      a      b      c         <--- depth 1\n",
    "    # / | \\  / | \\  / | \\   / | \\  / | \\  / | \\  / | \\  / | \\  / | \\\n",
    "    # a b c  a b c  a b c   a*b c  a`b c  a b c  a b c  a b c  a b c       <--- depth 2\n",
    "    # So the best beams at depth 2 would be\n",
    "    # b-a-a and b-b-a\n",
    "        \n",
    "    global test_word\n",
    "    global sample_beam_predictions\n",
    "    global update_sample_beam_state\n",
    "    \n",
    "    # Calculate the candidates at the root level\n",
    "    feed_dict = {}\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        feed_dict.update({sample_beam_inputs[b_n_i]: test_word})\n",
    "\n",
    "    # We calculate sample predictions for all neighbors with the same starting word/character\n",
    "    # This is important to update the state for all instances of beam search\n",
    "    sample_preds_root = session.run(sample_beam_predictions, feed_dict = feed_dict)  \n",
    "    sample_preds_root = sample_preds_root[0]\n",
    "\n",
    "    # indices of top-k candidates\n",
    "    # b and a in our example (root level)\n",
    "    this_level_candidates =  (np.argsort(sample_preds_root,axis=1).ravel()[::-1])[:beam_neighbors].tolist() \n",
    "\n",
    "    # probabilities of top-k candidates\n",
    "    # 0.5 and 0.2\n",
    "    this_level_probs = sample_preds_root[0,this_level_candidates] \n",
    "\n",
    "    # Update test sequence produced by each beam from the root level calculation\n",
    "    # Test sequence looks like for our example (at root)\n",
    "    # [b,a]\n",
    "    test_sequences = ['' for _ in range(beam_neighbors)]\n",
    "    for b_n_i in range(beam_neighbors):\n",
    "        test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "    # Make the calculations for the rest of the depth of the beam search tree\n",
    "    for b_i in range(beam_length-1):\n",
    "        test_words = [] # candidate words for each beam\n",
    "        pred_words = [] # Predicted words of each beam\n",
    "\n",
    "        # computing feed_dict for the beam search (except root)\n",
    "        # feed dict should contain the best words/chars/bigrams found by the previous level of search\n",
    "\n",
    "        # For level 1 in our example this would be\n",
    "        # sample_beam_inputs[0]: b, sample_beam_inputs[1]:a\n",
    "        feed_dict = {}\n",
    "        for p_idx, pred_i in enumerate(this_level_candidates):                    \n",
    "            # Updating the feed_dict for getting next predictions\n",
    "            test_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            test_words[p_idx][0,this_level_candidates[p_idx]] = 1.0\n",
    "\n",
    "            feed_dict.update({sample_beam_inputs[p_idx]:test_words[p_idx]})\n",
    "\n",
    "        # Calculating predictions for all neighbors in beams\n",
    "        # This is a list of vectors where each vector is the prediction vector for a certain beam\n",
    "        # For level 1 in our example, the prediction values for \n",
    "        #      b             a  (previous beam search results)\n",
    "        # [a,  b,  c],  [a,  b,  c] (current level predictions) would be\n",
    "        # [0.1,0.1,0.1],[0.5,0.2,0]\n",
    "        sample_preds_all_neighbors = session.run(sample_beam_predictions, feed_dict=feed_dict)\n",
    "\n",
    "        # Create a single vector with \n",
    "        # Making our example [0.1,0.1,0.1,0.5,0.2,0] \n",
    "        sample_preds_all_neighbors_concat = np.concatenate(sample_preds_all_neighbors,axis=1)\n",
    "\n",
    "        # Update this_level_candidates to be used for the next iteration\n",
    "        # And update the probabilities for each beam\n",
    "        # In our example these would be [3,4] (indices with maximum value from above vector)\n",
    "        this_level_candidates = np.argsort(sample_preds_all_neighbors_concat.ravel())[::-1][:beam_neighbors]\n",
    "\n",
    "        # In the example this would be [1,1]\n",
    "        parent_beam_indices = this_level_candidates//vocabulary_size\n",
    "\n",
    "        # normalize this_level_candidates to fall between [0,vocabulary_size]\n",
    "        # In this example this would be [0,1]\n",
    "        this_level_candidates = (this_level_candidates%vocabulary_size).tolist()\n",
    "\n",
    "        # Here we update the final state of each beam to be\n",
    "        # the state that was at the index 1. Because for both the candidates at this level the parent is \n",
    "        # at index 1 (that is b from root level)\n",
    "        session.run(update_sample_beam_state, feed_dict={best_neighbor_beam_indices: parent_beam_indices})\n",
    "\n",
    "        # Here we update the joint probabilities of each beam and add the newly found candidates to the sequence\n",
    "        tmp_this_level_probs = np.asarray(this_level_probs) #This is currently [0.5,0.2]\n",
    "        tmp_test_sequences = list(test_sequences) # This is currently [b,a]\n",
    "\n",
    "        for b_n_i in range(beam_neighbors):\n",
    "            # We make the b_n_i element of this_level_probs to be the probability of parents\n",
    "            # In the example the parent indices are [1,1]\n",
    "            # So this_level_probs become [0.5,0.5]\n",
    "            this_level_probs[b_n_i] = tmp_this_level_probs[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Next we multipyle these by the probabilities of the best candidates from current level \n",
    "            # [0.5*0.5, 0.5*0.2] = [0.25,0.1]\n",
    "            this_level_probs[b_n_i] *= sample_preds_all_neighbors[parent_beam_indices[b_n_i]][0,this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Make the b_n_i element of test_sequences to be the correct parent of the current best candidates\n",
    "            # In the example this becomes [b, b]\n",
    "            test_sequences[b_n_i] = tmp_test_sequences[parent_beam_indices[b_n_i]]\n",
    "\n",
    "            # Now we append the current best candidates\n",
    "            # In this example this becomes [ba,bb]\n",
    "            test_sequences[b_n_i] += reverse_dictionary[this_level_candidates[b_n_i]]\n",
    "\n",
    "            # Create one-hot-encoded representation for each candidate\n",
    "            pred_words.append(np.zeros((1,vocabulary_size),dtype=np.float32))\n",
    "            pred_words[b_n_i][0,this_level_candidates[b_n_i]] = 1.0\n",
    "\n",
    "    # Calculate best beam id based on the highest beam probability\n",
    "    # Using the highest beam probability always lead to very monotonic text\n",
    "    # Let us sample one randomly where one being sampled is decided by the likelihood of that beam\n",
    "    rand_cand_ids = np.argsort(this_level_probs)[-3:]\n",
    "    rand_cand_probs = this_level_probs[rand_cand_ids]/np.sum(this_level_probs[rand_cand_ids])\n",
    "    random_id = np.random.choice(rand_cand_ids,p=rand_cand_probs)\n",
    "\n",
    "    best_beam_id = parent_beam_indices[random_id]\n",
    "\n",
    "    # Update state and output variables for test prediction\n",
    "    session.run(update_sample_beam_state,feed_dict={best_neighbor_beam_indices:[best_beam_id for _ in range(beam_neighbors)]})\n",
    "\n",
    "    # Make the last word/character/bigram from the best beam\n",
    "    test_word = pred_words[best_beam_id]\n",
    "    \n",
    "    return test_sequences[best_beam_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(0).(7).\n",
      "Average loss at step 1: 5.531789\n",
      "\tPerplexity at step 1: 252.595414\n",
      "\n",
      "Valid Perplexity: 3.23\n",
      "\n",
      "Generated Text after epoch 0 ... \n",
      "======================== New text Segment ==========================\n",
      " ithethth t tth t te e  te e e e  te e e e the e e e  te e e e the e e e hee e e e hee e e e  te e e e the e e e the e e e  te e e e  te e e e hee e e e  te e e e the e e e hee e e e hee e e e hee e e e hee e e e hee e e e hee e e e  te e e e  te e e e hee e e e  te e e e hee e e e hee e e e hee e e e the e e e  te e e e  te e e e the e e e hee e e e the e e e the e e e the e e e  te e e e hee e e e  te e e e the e e e the e e e  te e e e the e e e the e e e hee e e e hee e e e  te e e e the e e e  te e e e  te e e e the e e e hee e e e hee e e e hee e e e  te e e e  te e e e the e e e hee e e e the e e e hee e e e hee e e e  te e e e  te e e e  te e e e hee e e e hee e e e hee e e e hee e e e  te e e e hee e e e the e e e the e e e hee e e e the e e e hee e e e the e e e  te e e e hee e e e hee e e e  te e e e hee e e e hee e e e  te e e e hee e e e  te e e e the e e e hee e e e  te e e e the e e e  te e e e  te e e e hee e e e hee e e e  te e e e the e e e  te e e e hee e e e the e e e \n",
      "====================================================================\n",
      "\n",
      "(4).(14).\n",
      "Average loss at step 2: 5.363668\n",
      "\tPerplexity at step 2: 213.506758\n",
      "\n",
      "Valid Perplexity: 3.15\n",
      "\n",
      "Generated Text after epoch 1 ... \n",
      "======================== New text Segment ==========================\n",
      " rlt th the ad th thee  the the t the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee  the thee \n",
      "====================================================================\n",
      "\n",
      "(1).(15).\n",
      "Average loss at step 3: 4.995605\n",
      "\tPerplexity at step 3: 147.762271\n",
      "\n",
      "Valid Perplexity: 2.88\n",
      "\n",
      "Generated Text after epoch 2 ... \n",
      "======================== New text Segment ==========================\n",
      " evked ther that the that to the to to the to to the to to the to to the to to the to to the to the to to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to the to to to the to to the to to the of the the to the that to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to the to to to the to to the to to the to to the to to the to to the to to the to the to of the the the the that the the the the to to the to to the to to the of the the to the that to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to the to to to the to to the to to the of the the to the that to the to to the to to the to to the to to the to to the to to the to to the to to the to to the to the to to to the to to the to to the to the to to to the \n",
      "====================================================================\n",
      "\n",
      "(7).(10).\n",
      "Average loss at step 4: 4.482178\n",
      "\tPerplexity at step 4: 88.427088\n",
      "\n",
      "Valid Perplexity: 2.70\n",
      "\n",
      "Generated Text after epoch 3 ... \n",
      "======================== New text Segment ==========================\n",
      "  yked and that and to that ther that the shad the that and then the the was the that and then the the that ther that the shad the that and then the the that the shad the that and then the the that the shad the that and then the the that the shad the that and then the the was the that and the to there of to ther and ther that the shad the that and then the the that the shad the that and then the the was the that and the to there of to ther and to that and then the the was the that and then the the that the shad the that and then the the was the that and then the the that the shad the that and then the the that ther that to that and then the the was the that and then the the that ther that the shad the that to the that ther that the shad the that and then the the was the ther to the was the that and then the the that the shat the that and then the the was the that and then the the that ther that the shad the that and then the the that the shad the that and then the the that the shad the th\n",
      "====================================================================\n",
      "\n",
      "(1).(2).\n",
      "Average loss at step 5: 4.155977\n",
      "\tPerplexity at step 5: 63.814284\n",
      "\n",
      "Valid Perplexity: 2.52\n",
      "\n",
      "Generated Text after epoch 4 ... \n",
      "======================== New text Segment ==========================\n",
      " idother the ther ther of to the saided thand the cound and to hat the that and the could the could the could the with the saided at to the she said.\n",
      "\n",
      "i saided thand the cound and to hat the that and the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the with the saided thand the said, and the could the with the saided thand the said, and the could the with the saided thand the said, and the could the could the could the could the could the could the could the with the saided thand to the saided thand the cound and to hat the that and the could the could the could the with the saided thand the said, and the with the saided thand the cound and to hat the that and the could the with the saided at to the she said.\n",
      "\n",
      "i saided at to the she said.\n",
      "\n",
      "i saided thand the cound and the with the saided thand the cound and to hat the that and to hat the that and the could the could the with the saided at \n",
      "====================================================================\n",
      "\n",
      "(10).(15).\n",
      "Average loss at step 6: 3.966791\n",
      "\tPerplexity at step 6: 52.814771\n",
      "\n",
      "Valid Perplexity: 2.48\n",
      "\n",
      "Generated Text after epoch 5 ... \n",
      "======================== New text Segment ==========================\n",
      " lc\n",
      "“i that the said.”\n",
      "he said.\n",
      "\n",
      "he said. “i said. that in the said to then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to said to then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said. “that thing the said.”\n",
      "\n",
      "“then the said, and to said to then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to said to then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to said to then the said, and to said to then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said. “that and the could and thing the said.”\n",
      "\n",
      "“then the said, and to that the said.”\n",
      "\n",
      "“then the said, and to that the said. “that and the could and said.”\n",
      "\n",
      "“that to that and the said.”\n",
      "\n",
      "“then the said, and to said the said.”\n",
      "\n",
      "“th\n",
      "====================================================================\n",
      "\n",
      "(9).(13).\n",
      "Average loss at step 7: 3.804013\n",
      "\tPerplexity at step 7: 44.880951\n",
      "\n",
      "Valid Perplexity: 2.44\n",
      "\n",
      "Generated Text after epoch 6 ... \n",
      "======================== New text Segment ==========================\n",
      " omned that thing and the said. “her that in that to here the said.”\n",
      "\n",
      "“that the said, and said. “i said.\n",
      "\n",
      "i said.\n",
      "\n",
      "i said, i said. then to the said. i saw that in to said.\n",
      "\n",
      "“whe said. “that the said.\n",
      "\n",
      "i hered the that to that the said. “i said and the said.”\n",
      "\n",
      "“that the said. “i said.\n",
      "\n",
      "i said. the said. “i said, i said, and.”\n",
      "\n",
      "“that and to then to the said. i said. i said.\n",
      "\n",
      "i said.\n",
      "\n",
      "i said, that the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the would the w\n",
      "====================================================================\n",
      "\n",
      "(5).(17).\n",
      "Average loss at step 8: 3.696299\n",
      "\tPerplexity at step 8: 40.297885\n",
      "\n",
      "Valid Perplexity: 2.47\n",
      "\n",
      "Generated Text after epoch 7 ... \n",
      "======================== New text Segment ==========================\n",
      " thked that a said,” i ward and thing then my said, “that the said. “that and and thing then my said, “that and thing then my said, “that and thing to ther and the said, “that i said.” i said. he said, “that and thing to ther and the said, “that in the that the said.\n",
      "\n",
      "\t\t\t\ti said. \t\t\tand said of and the said, “that in the that the fire of the did. \t\t\t\ti said. \t\t\tand said of and the did. \t\t\t\ti said. \t\t\tand said of and the said, “that to the that the face of to said. \t\t\t\the said.\n",
      "\n",
      "i said.\n",
      "\n",
      "\t\n",
      "\n",
      "i said. he said, “that and thing then my said, “that and thing to that in the that the said.\n",
      "\n",
      "\t\t\t\ti said. \t\t\tand said of and the said, “that in the that the said.\n",
      "\n",
      "\t\t\t\ti said. \t\t\tand said of and the did. \t\t\t\ti said. \t\t\tand said of and the said, “that to the that the said. “the said, and my his and and said.” i said. he said. “that he said.\n",
      "\n",
      "\t\t\t\t\the said.\n",
      "\n",
      "i said. \t\t\tand said of and the said, “that in the that the said.\n",
      "\n",
      "\t\t\t\ti said. \t\t\tand said of and the did. \t\t\t\ti said. \t\t\tand said of and the said, “th\n",
      "====================================================================\n",
      "\n",
      "(4).(3).\n",
      "Average loss at step 9: 3.664833\n",
      "\tPerplexity at step 9: 39.049624\n",
      "\n",
      "Valid Perplexity: 2.31\n",
      "\n",
      "Generated Text after epoch 8 ... \n",
      "======================== New text Segment ==========================\n",
      "  aund that thround to that the said, \"that in that i said.\"\n",
      "\n",
      "i hear the that the show that thinked the said.\"\n",
      "\n",
      "i hear the that the show that thinked the said.\"\n",
      "\n",
      "i hear the that the said, \"that in that i said.\"\n",
      "\n",
      "i hear the that the said, \"that in that i said.\"\n",
      "\n",
      "i hear the couldn't said.\n",
      "\n",
      "i saw that the cound the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the could the from the that to that the couse to the couldn't had and to been the show that thinked the said.\"\n",
      "\n",
      "i hear the that the said. \"that her the said, \"that in that i said.\"\n",
      "\n",
      "i hear the couldn't said.\n",
      "\n",
      "i saw that the could the could the could the from the ward the show that thinked the said.\n",
      "\n",
      "i said, and have the show that thinked the said.\"\n",
      "\n",
      "i hear the couldn't said.\n",
      "\n",
      "i saw that the could the could the could the could the could the could the could the could the could the\n",
      "====================================================================\n",
      "\n",
      "(16).(5).\n",
      "Average loss at step 10: 3.597350\n",
      "\tPerplexity at step 10: 36.501367\n",
      "\n",
      "Valid Perplexity: 2.27\n",
      "\n",
      "Generated Text after epoch 9 ... \n",
      "======================== New text Segment ==========================\n",
      " hem it wout and that the said. \"i said. \"i said.\n",
      "\n",
      "i said, and i said. \"i said, \"that in the couldn't think to that the was had the started and her the said, \"the said.\n",
      "\n",
      "\"that in the couldn't thing the said, \"the said.\n",
      "\n",
      "i started the said. \"i down and show that had and the said. \"that in the couldn't thing the started and her to that it the to the sain that to been the said, \"the said.\n",
      "\n",
      "\"that in the couldn't think to that to the said.\n",
      "\n",
      "\"that in the said, and i said. \"i with the said, and i said. \"i said, \"that in the couldn't think to that to the said.\n",
      "\n",
      "i started the said.\n",
      "\n",
      "i said, and i said. \"i said.\n",
      "\n",
      "i said, and that the started and her to that it the to the sain that to been the said, \"the couldn't started the said.\n",
      "\n",
      "i said, and i said. \"i said.\n",
      "\n",
      "i said, and that the started and her the started and her had a said. \"that in the said, and that the said, \"the said.\n",
      "\n",
      "\"that in the said, and i said. \"i said.\n",
      "\n",
      "i said, and i said. \"i said, \"that in the said, and that the started and her the s\n",
      "====================================================================\n",
      "\n",
      "(4).(11).\n",
      "Average loss at step 11: 3.545092\n",
      "\tPerplexity at step 11: 34.642884\n",
      "\n",
      "Valid Perplexity: 2.29\n",
      "\n",
      "Generated Text after epoch 10 ... \n",
      "======================== New text Segment ==========================\n",
      " pew that thing the said, “that me. it the said. i said. “i said, “that me. it the said. i said. “i said, “that had and thand to the said. i said. “i said, “that had and thand to the said. i said. “i said, “that me. it the said. i said. “i said, “that me. it the said. i said. “i said, “that had and thand to the said. i said. “i said, “that had and thand to the said, and her that think to that it the said. “i started the said. “that the said, “that me. it the said. i said. “i said, “that me. it the said. i said. “i said, “that me.\n",
      "\n",
      "i said the said, and said, and that have the started the said.\n",
      "\n",
      "i down and said.\n",
      "\n",
      "i said. “i said, “that me. it the said. i said. “i said, “that me. it the said. i said. “i said, “that me. it the said. i said. “i said, “that me.\n",
      "\n",
      "i said.\n",
      "\n",
      "i said. i said.\n",
      "\n",
      "i saw the said.\n",
      "\n",
      "“that it the said. “i started the said.\n",
      "\n",
      "i couldn’t something thinked and her to that it the couldn’t said.\n",
      "\n",
      "i said. “i said, “that me. it the could that her to that to the said. i said. “i sai\n",
      "====================================================================\n",
      "\n",
      "(7).(10).\n",
      "Average loss at step 12: 3.471088\n",
      "\tPerplexity at step 12: 32.171741\n",
      "\n",
      "Valid Perplexity: 2.26\n",
      "\n",
      "Generated Text after epoch 11 ... \n",
      "======================== New text Segment ==========================\n",
      " , she that thing the started the said, “that the said, “that i said. “i said. “whand the was a said.\n",
      "\n",
      "“i said. “when the started the said.\n",
      "\n",
      "i said. “whand the was a said.\n",
      "\n",
      "he said. “when the started the said.\n",
      "\n",
      "i said. “whand the was a said.\n",
      "\n",
      "he said. “when the started the said.\n",
      "\n",
      "i said. “whand the was a said.\n",
      "\n",
      "“i said. “when the started the said.\n",
      "\n",
      "i said. “whand the was a said.\n",
      "\n",
      "“i said. “what to things and said.\n",
      "\n",
      "i said. “i said, “that think to things to things to things and said.\n",
      "\n",
      "i said. “i said, “that think to things and said. “i saw that the said. “i said, and shound to down to things and said.\n",
      "\n",
      "i said. “i said, “that i said.\n",
      "\n",
      "i said. “whand the was a said.\n",
      "\n",
      "he said. “what to things and said.\n",
      "\n",
      "i said. “i said, “that i said. “i said. “whand the was a said.\n",
      "\n",
      "he said. “when the started the said.\n",
      "\n",
      "i said. “whand the said, “that me. i said.\n",
      "\n",
      "i said. “the said.\n",
      "\n",
      "i said. “the said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.\n",
      "\n",
      "i said. “the said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.”\n",
      "\n",
      "i said.\n",
      "====================================================================\n",
      "\n",
      "(0).(10).\n",
      "Average loss at step 13: 3.449225\n",
      "\tPerplexity at step 13: 31.475990\n",
      "\n",
      "Valid Perplexity: 2.24\n",
      "\n",
      "Generated Text after epoch 12 ... \n",
      "======================== New text Segment ==========================\n",
      " sip that was had the started the would her been that her to that it the said, and you that the said. “when she said. “the said. “whe said, “that it the said, and shound to down the said, “the that the said. “the said. “whe said. “i said. “what?” i said.\n",
      "\n",
      "i said. the said. the said.”\n",
      "\n",
      "i said. the said.”\n",
      "\n",
      "i said. i said. “when the said. “the said. “whe said, “that it the that to that it the that to thing that the said. “when she said. “the said. “whe said, “that it the that to that it the said. “if i said. “whe said. “when the said. “the that the said. “the that the said. “the said. “whe said, “that it the that to that it the that to that it the that to that it the that to that it the that to that it the said. “if i said. “whe said, “what the started the said.\n",
      "\n",
      "i said. “what?” i said the said, and said. i said. “when she said. “the said. “whe said. “i said. “what?” i said.\n",
      "\n",
      "i said. i said. “when the said. “the said. “whe said, “that it the said. “if i said. “whe said. “when she said, “the \n",
      "====================================================================\n",
      "\n",
      "(1).(8).\n",
      "Average loss at step 14: 3.426659\n",
      "\tPerplexity at step 14: 30.773667\n",
      "\n",
      "Valid Perplexity: 2.20\n",
      "\n",
      "Generated Text after epoch 13 ... \n",
      "======================== New text Segment ==========================\n",
      "  down that would the would the sound to that the sainst that me, and the said.\n",
      "\n",
      "i don't the couldn't started and and said, and said,\" i said, that the would the sound to things to things to things to that the sainst that me, and the said.\n",
      "\n",
      "i don't the couldn't started and and said, and that have the sound to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to that the sainst that i said.\n",
      "\n",
      "i said. \"thand to the couldn't started the said. \"then in the couldn't think to things to things to things to things to things to things to things to things to things to things to things to things to things to things to things to that the sainst that i said. \"i couldn't started and and said.\n",
      "\n",
      "i said. i said.\n",
      "\n",
      "thing to things to things to things to things to things to things to things to things to th\n",
      "====================================================================\n",
      "\n",
      "(1).(12).\n",
      "Average loss at step 15: 3.387792\n",
      "\tPerplexity at step 15: 29.600515\n",
      "\n",
      "Valid Perplexity: 2.22\n",
      "\n",
      "Generated Text after epoch 14 ... \n",
      "======================== New text Segment ==========================\n",
      " thugh,” i ward and that,” i said the said. the said, “that the said, “things the started and into the said. “if i said. “i said. “things the said. “the said. “than they was the said, “that the said, “things that had been thinked the said.\n",
      "\n",
      "i didn’t sainst that i said.\n",
      "\n",
      "i couldn’t started and into the said. “if i said. “i said. “things the started and into the said, and your had but in the couldn’t started and into the said. “if i said. “i said. “things and thinked the said, “things and thinked and into the said. “if i said, an and said. the said, “that the said.\n",
      "\n",
      "i didn’t sainst that i said.\n",
      "\n",
      "i couldn’t started and into the said. “if i said. “i said. “things that had been thinked the could have,” i said.\n",
      "\n",
      "i said.\n",
      "\n",
      "i said. “there was had back that had but in the couldn’t started and into the said. “if i said. “i said, “things the said. “the said. “than they was the said, “that the said, “things the started the said, “things the said. “the wards of thinked the said, “things that have and t\n",
      "====================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [89], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     feed_dict[train_labels[ui]] \u001b[39m=\u001b[39m lbl\n\u001b[0;32m     65\u001b[0m \u001b[39m# Running the TensorFlow operations\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m _, l, step_perplexity \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrun([optimizer, loss, train_perplexity_without_exp], \n\u001b[0;32m     67\u001b[0m                                            feed_dict\u001b[39m=\u001b[39;49mfeed_dict)\n\u001b[0;32m     68\u001b[0m \u001b[39m# Update doc_perpelxity variable\u001b[39;00m\n\u001b[0;32m     69\u001b[0m doc_perplexity \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m step_perplexity \n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[0;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m   1372\u001b[0m                        run_metadata)\n\u001b[0;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mc:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[0;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[0;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename_to_save = 'lstm_beam_search_dropout'\n",
    "\n",
    "# Some hyperparameters needed for the training process\n",
    "\n",
    "num_steps = 34\n",
    "steps_per_document = 100\n",
    "valid_summary = 1\n",
    "train_doc_count = 18\n",
    "docs_per_step = 2\n",
    "\n",
    "\n",
    "beam_nodes = []\n",
    "\n",
    "beam_train_perplexity_ot = []\n",
    "beam_valid_perplexity_ot = []\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# We use the first 10 documents that has \n",
    "# more than 10*steps_per_document bigrams for creating the validation dataset\n",
    "\n",
    "# Identify the first 10 documents following the above condition\n",
    "long_doc_ids = []\n",
    "for di in range(num_files):\n",
    "  if len(data_list[di])>2*steps_per_document:\n",
    "    long_doc_ids.append(di)\n",
    "  if len(long_doc_ids)==2:\n",
    "    break\n",
    "    \n",
    "# Generating validation data\n",
    "data_gens = []\n",
    "valid_gens = []\n",
    "for fi in range(num_files):\n",
    "  # Get all the bigrams if the document id is not in the validation document ids\n",
    "  if fi not in long_doc_ids:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi],batch_size,num_unrollings))\n",
    "  # if the document is in the validation doc ids, only get up to the \n",
    "  # last steps_per_document bigrams and use the last steps_per_document bigrams as validation data\n",
    "  else:\n",
    "    data_gens.append(DataGeneratorOHE(data_list[fi][:-steps_per_document],batch_size,num_unrollings))\n",
    "    # Defining the validation data generator\n",
    "    valid_gens.append(DataGeneratorOHE(data_list[fi][-steps_per_document:],1,1))\n",
    "\n",
    "\n",
    "feed_dict = {}\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    for di in np.random.permutation(train_doc_count)[:docs_per_step]:            \n",
    "        doc_perplexity = 0\n",
    "        for doc_step_id in range(steps_per_document):\n",
    "            \n",
    "            # Get a set of unrolled batches\n",
    "            u_data, u_labels = data_gens[di].unroll_batches()\n",
    "            \n",
    "            # Populate the feed dict by using each of the data batches\n",
    "            # present in the unrolled data\n",
    "            for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "                feed_dict[train_inputs[ui]] = dat\n",
    "                feed_dict[train_labels[ui]] = lbl\n",
    "            \n",
    "            # Running the TensorFlow operations\n",
    "            _, l, step_perplexity = session.run([optimizer, loss, train_perplexity_without_exp], \n",
    "                                                       feed_dict=feed_dict)\n",
    "            # Update doc_perpelxity variable\n",
    "            doc_perplexity += step_perplexity \n",
    "            \n",
    "            # Update the average_loss variable\n",
    "            average_loss += step_perplexity \n",
    "            \n",
    "        # Show the printing progress .. ...\n",
    "        print('(%d).'%di,end='') \n",
    "    \n",
    "    # resetting hidden state after processing a single document\n",
    "    # It's still questionable if this adds value in terms of learning\n",
    "    # One one hand it's intuitive to reset the state when learning a new document\n",
    "    # On the other hand this approach creates a bias for the state to be zero\n",
    "    # We encourage the reader to investigate further the effect of resetting the state\n",
    "    #session.run(reset_train_state) # resetting hidden state for each document\n",
    "    print('')\n",
    "    \n",
    "    if (step+1) % valid_summary == 0:\n",
    "      \n",
    "      # Compute average loss\n",
    "      average_loss = average_loss / (docs_per_step*steps_per_document*valid_summary)\n",
    "      \n",
    "      # Print loss\n",
    "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "      print('\\tPerplexity at step %d: %f' %(step+1, np.exp(average_loss)))\n",
    "      beam_train_perplexity_ot.append(np.exp(average_loss))\n",
    "    \n",
    "      average_loss = 0 # reset loss\n",
    "        \n",
    "      valid_loss = 0 # reset loss\n",
    "        \n",
    "      # calculate valid perplexity\n",
    "      for v_doc_id in range(2):\n",
    "          # Remember we process things as bigrams\n",
    "          # So need to divide by 2\n",
    "          for v_step in range(steps_per_document//2):\n",
    "            uvalid_data,uvalid_labels = valid_gens[v_doc_id].unroll_batches()        \n",
    "\n",
    "            # Run validation phase related TensorFlow operations       \n",
    "            v_perp = session.run(\n",
    "                valid_perplexity_without_exp,\n",
    "                feed_dict = {valid_inputs:uvalid_data[0],valid_labels: uvalid_labels[0]}\n",
    "            )\n",
    "\n",
    "            valid_loss += v_perp\n",
    "            \n",
    "          session.run(reset_valid_state)\n",
    "      \n",
    "          # Reset validation data generator cursor\n",
    "          valid_gens[v_doc_id].reset_indices()      \n",
    "    \n",
    "      print()\n",
    "      v_perplexity = np.exp(valid_loss/(steps_per_document*10.0//2))\n",
    "      print(\"Valid Perplexity: %.2f\\n\"%v_perplexity)\n",
    "      beam_valid_perplexity_ot.append(v_perplexity)\n",
    "      \n",
    "      # Decay learning rate\n",
    "      decay_learning_rate(session, v_perplexity)\n",
    "    \n",
    "      # Generating new text ...\n",
    "      # We will be generating one segment having 500 bigrams\n",
    "      # Feel free to generate several segments by changing\n",
    "      # the value of segments_to_generate\n",
    "      print('Generated Text after epoch %d ... '%step)  \n",
    "      segments_to_generate = 1\n",
    "      chars_in_segment = 500//beam_length\n",
    "    \n",
    "      for _ in range(segments_to_generate):\n",
    "        print('======================== New text Segment ==========================')\n",
    "        # first word randomly generated\n",
    "        test_word = np.zeros((1,vocabulary_size),dtype=np.float32)\n",
    "        rand_doc = data_list[np.random.randint(0,num_files)]\n",
    "        test_word[0,rand_doc[np.random.randint(0,len(rand_doc))]] = 1.0\n",
    "        print(\"\",reverse_dictionary[np.argmax(test_word[0])],end='')\n",
    "        \n",
    "        for _ in range(chars_in_segment):\n",
    "            \n",
    "            test_sequence = get_beam_prediction(session)\n",
    "            print(test_sequence,end='')\n",
    "            \n",
    "        print(\"\")\n",
    "        session.run([reset_sample_beam_state])\n",
    "        \n",
    "        print('====================================================================')\n",
    "      print(\"\")\n",
    "\n",
    "session.close()\n",
    "    \n",
    "with open(filename_to_save, 'wt') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(beam_train_perplexity_ot)\n",
    "    writer.writerow(beam_valid_perplexity_ot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b8910ab33c7afb18a70a6c11b82b733b296c5c529ccd380964595a3a81d01b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
